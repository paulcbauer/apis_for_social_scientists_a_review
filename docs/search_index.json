[["index.html", "Preface", " APIs for social scientists:A collaborative review Current editors:Paul C. Bauer, Camille Landesvatter, Lion BehrensAuthors &amp; contributors:Paul C. Bauer, Jan Behnert, Lion Behrens, Chung-hong Chan, Jacopo Gambato, Noam Himmelrath, Bernhard Clemm von Hohenberg, Johanna Hölzl, Lukas Isermann, Philipp Kadel, Melike N. Kaplan, Jana Klein, Markus Konrad, Barbara K. Kreis, Dean Lajic, Camille Landesvatter, Madleen Meier-Barthold, Grace Olzinski, Nina Osenbrügge, Ondřej Pekáček, Felix Rusche, Pirmin Stöckle, Marie-Lou Sohnius, Malte Söhren, Domantas Undzėnas First public version: 29 November, 2021This version: 17 September, 2024 Preface Permanent link: https://paulcbauer.github.io/apis_for_social_scientists_a_review/ The present online book provides a review of APIs that may be useful for social scientists. Please start by reading the Introduction. The material was/is being developed by various contributors, which you can find above and in the contributor section of the corresponding github repository. If you are interested in contributing, please check out the Section How to contribute in the github README. The material is licensed under a Apache License 2.0 license. Where we draw on other authors’ material, other licenses may apply. We are extremely grateful for feedback and if you find errors, please let us know. This document was generated with R, RMarkdown and Bookdown. "],["introduction.html", "Chapter 1 Introduction 1.1 Prerequisites: Authentication 1.2 Prerequisites: Software &amp; packages 1.3 Replication", " Chapter 1 Introduction This project APIs for Social Scientists: A collaborative Review is an outcome of the seminar Computational Social Science (CSS) taught at the University of Mannheim in 2021. While teaching the seminar, we had trouble finding short reviews of APIs with quick R-code examples. Fortunately, almost everyone participating in the seminar was motivated enough to write a quick API review. Hopefully, our resource will help future students to start diving into different APIs. Below we review different data- and service-APIs that may be useful to social scientists. The chapters always include a simple R code example as well as references to social science research that has relied on them. The idea is to provide short reviews of max. 10 pages for the corresponding API with code to get you started. Each chapter follows a systematic set of questions: What data/service is provided by the API? (+ who provides it?) What are the prerequisites to access the API (e.g., authentication)? What does a simple API call look like? How can we access the API from R (httr + other packages)? Are there social science research examples using the API? 1.1 Prerequisites: Authentication A lot of the APIs require that you authenticate with the API provider. The underlying script of this review is written in such a way that it contains R chunks for authentication, however they will not be visible in the examples below (we only show placeholders for you to recognize at which step you will need to authenticate). These chunks in most cases make use of so-called keys in JSON format (e.g., service account key for Google APIs). However cloning the corresponding repository of this review will not result in giving you the keys, hence in order to replicate our API calls, you will have to generate and use your own individual keys. 1.2 Prerequisites: Software &amp; packages The code examples rely on R and different packages thereof. It is probably easiest if you install all of them in one go using the code below. The p_load() function (pacman package) checks whether packages are installed. If not, they are installed and loaded. ## # install.packages(&#39;pacman&#39;) ## library(pacman) ## p_load(&#39;checkpoint&#39;, &#39;httr&#39;, ## &#39;memoise&#39;, &#39;googleway&#39;, &#39;httr&#39;, &#39;tidyverse&#39;, &#39;ckanr&#39;, &#39;jsonlite&#39;, &#39;readxl&#39;, ## &#39;curl&#39;, &#39;httr&#39;, &#39;devtools&#39;, &#39;RCrowdTangle&#39;, &#39;dplyr&#39;, &#39;jsonlite&#39;, &#39;httr&#39;, ## &#39;remotes&#39;, &#39;dplyr&#39;, &#39;ggplot2&#39;, &#39;tidyr&#39;, &#39;Radlibrary&#39;, &#39;dplyr&#39;, &#39;tidyr&#39;, &#39;DT&#39;, ## &#39;DemografixeR&#39;, &#39;jsonlite&#39;, &#39;httr&#39;, &#39;httr&#39;, &#39;dplyr&#39;, &#39;httr&#39;, &#39;googleLanguageR&#39;, ## &#39;tidyverse&#39;, &#39;tm&#39;, &#39;ggwordcloud&#39;, &#39;httr&#39;, &#39;googleway&#39;, &#39;ggplot2&#39;, &#39;tidyverse&#39;, ## &#39;mapsapi&#39;, &#39;stars&#39;, &#39;tidyverse&#39;, &#39;googleLanguageR&#39;, &#39;googleLanguageR&#39;, &#39;httr&#39;, ## &#39;gtrendsR&#39;, &#39;ggplot2&#39;, &#39;dplyr&#39;, &#39;httr&#39;, &#39;httr&#39;, &#39;httr&#39;, &#39;jsonlite&#39;, &#39;tibble&#39;, ## &#39;archiveRetriever&#39;, &#39;httr&#39;, &#39;jsonlite&#39;, &#39;stringr&#39;, &#39;openai&#39;, &#39;httr&#39;, &#39;stringr&#39;, ## &#39;mediacloud&#39;, &#39;tidytext&#39;, &#39;quanteda&#39;, &#39;quanteda&#39;, &#39;tidyverse&#39;, &#39;osmdata&#39;, &#39;sf&#39;, ## &#39;ggmap&#39;, &#39;osmdata&#39;, &#39;osmdata&#39;, &#39;osmdata&#39;, &#39;ggmap&#39;, &#39;kableExtra&#39;, &#39;spotifyr&#39;, ## &#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;httr&#39;, &#39;academictwitteR&#39;, &#39;tidyverse&#39;, &#39;lubridate&#39;, ## &#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;dplyr&#39;, &#39;rtweet&#39;, &#39;WikipediR&#39;, &#39;rvest&#39;, &#39;xml2&#39;, ## &#39;httr&#39;, &#39;jsonlite&#39;, &#39;here&#39;, &#39;dplyr&#39;, &#39;ggplot2&#39;, &#39;tuber&#39;, &#39;tidyverse&#39;, &#39;purrr&#39;) p_load_gh(&quot;quanteda/quanteda.corpora&quot;) p_load_gh(&quot;cbpuschmann/RCrowdTangle&quot;) p_load_gh(&quot;joon-e/mediacloud&quot;) p_load_gh(&quot;facebookresearch/Radlibrary&quot;) 1.3 Replication A lot of the R packages that you have installed and loaded above regularly get updated, which often comes with slight changes in functionality. Also, new versions of the R programming environment itself are constantly developed. While we rely on an automatized way to regularly check whether all code that is presented in the subsequent chapters replicates without errors, it might be that you are executing code just before we find out about conflicts that come with newer versions of R or used packages. If you would like to ensure that all of the presented code runs smoothly, you can execute the following commands before proceeding. p_load(&#39;checkpoint&#39;) checkpoint(&quot;2022-08-03&quot;) The checkpoint package from the Reproducible R Toolkit (RRT) makes use of the daily snapshots of CRAN which are mirrored by the RRT-team on a separate server. Once you execute the command above, R will install all packages that you have loaded with the p_load() function in the same version as we used them on the date at which we last worked on the code. You may choose to only take this route if you would like to replicate the code presented in a particular chapter but run into an error message. To reset your session to the state it was before using checkpoint, call uncheckpoint() or simply restart R. "],["best-practices.html", "Chapter 2 Best Practices 2.1 Read the Developer Agreement, Policy and API Documentation 2.2 Don’t Hardcode Authentication Information into your R Code 2.3 Memoise your API Calls", " Chapter 2 Best Practices Chung-hong Chan When working with 3rd party APIs, please make sure you follow the best practices. 2.1 Read the Developer Agreement, Policy and API Documentation You must read the Developer Agreement, Policy and API documentation. It is still true even though you are going to use the R packages providing the wrapper functions (e.g. RCrowdTangle, tuber, academictwitteR etc.) First, the Developer Agreement and Policy provide information on what you can and cannot do with the data obtained through the API. It is important for both Open Science practices (e.g. sharing data publicly) and sharing data between individuals within the research group. Please make sure you understand the data redistribution policy. The API provided by Twitter, for example, forbids the redistribution of Twitter Content to third parties. However, academic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs for peer review purposes. The API provided by CrowdTangle basically forbids any data redistribution. This point is of paramount importance for social scientists because the Cambridge Analytica data scandal is a case of API data abuse by an academic researcher. Second, the API documentation provides information on what are the expected API responses and rate limits. Knowing the information is important because you know what to expect. Also, you won’t offset the problems related to the API to the R package developers. 2.2 Don’t Hardcode Authentication Information into your R Code You should not hardcode your authentication information (authentication keys, secrets, tokens) into your R code. But what do I mean by that? For example, the following is an example of hardcoding. require(tuber) ## fake, taken from tuber&#39;s vignette client_id &lt;- &quot;998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com&quot; client_secret &lt;- &quot;MbOSt6cQhhFkwETXKur-L9rN&quot; yt_oauth(app_id = client_id, app_secret = client_secret, token = &#39;&#39;) This is not a good practice for two reasons. First, your client_id and client_secret are directly visible in your R code. It is super easy to accidentally leak these supposedly secret information while sharing your code. Even supposedly professional programmers do that quite often. Also unlike a typical password system which renders these secret information as asterisks, it enables the so-called “shoulder surfing attack”: a malicious actor can obtain these information by simply looking (or videotaping through a tele lens) at your computer screen over your shoulder. Second, when you run your code, your client_id and client_secret are burnt into your command history. A malicious person can browse through your command history to obtain these information. You might wonder, well, they are just two pieces of string. No big deal. But please bare in mind these information is not simply for collecting data from YouTube. It could also be the credential for your Google Cloud access. Frequent access to some API endpoints that require money can incur financial loss. Other APIs such as Twitter v1 APIs allow deletion of data, posting data or reading all your direct messages on your behalf, simply with your API authentication information. So please: PROTECT YOUR AUTHENTICATION INFORMATION LIKE YOUR PASSWORDS! 2.2.1 Alternative: Use Environment Variables So, what is the alternative? A securer solution is to store your authentication information as environment variables (envvars) instead. Specifically, you should store your authentication information as your user-level envvars. For a quick experimentation, run this: usethis::edit_r_environ(scope = &quot;user&quot;) If you are doing this in RStudio, a new file is now opened. That is your user-level .Renviron file. It is a hidden file (indicated by the “.”) in your user directory 1. For most people, it should be a blank file. For some, the file might already have something in it. Now, in the file, put this line into it: MYSECRET=ROMANCE This line says: I want to create a user-level envvar called “MYSECRET” with the value being “ROMANCE”. Please note that this is not R and you must use the equal sign. As instructed, save the file and then restart R. In the fresh R session, you can now retrieve the value of the environment variable “MYSECRET”. If you did that correctly, running this should give you the value of “MYSECRET”, i.e. “ROMANCE”. ## You should get &quot;ROMANCE&quot; Sys.getenv(&quot;MYSECRET&quot;) The practice is to setup envvars of your authentication information. For example, in your .Renviron set up the following envvars. YT_CLIENTID=998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com YT_CLIENTSECRET=MbOSt6cQhhFkwETXKur-L9rN And then in your R code, it should be: require(tuber) yt_oauth(app_id = Sys.getenv(&quot;YT_CLIENTID&quot;), app_secret = Sys.getenv(&quot;YT_CLIENTSECRET&quot;), token = &#39;&#39;) As long as your hidden .Renviron file is not leaked, you are safe. For reproducibility purposes, you should document all these envvars (the definitions, not the values) in the README file 2. This method is not perfect, of course. For example, your .Renviron is a plain text file and it can still be leaked. If you want to know other alternatives, see the “Managing secrets” vignette of httr. 2.3 Memoise your API Calls Many API documentation will tell you to cache. Caching is to store a local copy of the response from the API. If you submit the exact same API request again, instead of making another API request to the server, the result is retrieved from the local copy. The technique is also called memoisation. This method is more useful for API responses that are not dynamically changed, e.g. Google Natural Language API, Google Places API, or CKAN API. It is less useful for social media APIs because information such as number of likes changes frequently. If you are only interested in retrieving the content, you can also cache those social media APIs. Caching is good because it reduces unnecessary API requests. It is also helpful to prevent exceeding rate limit. 2.3.1 Implementation of memoisation in R As a quick experiment, we use an extremely simple API: restful catAPI. library(httr) content(GET(&quot;https://thatcopy.pw/catapi/rest/&quot;)) If you run the above code many times, you should get a different cat photo every time. However, you can create a memoised version of the httr::GET function called mGET. library(memoise) mGET &lt;- memoise(GET) Similarly, you can create a memoised version of any API function, e.g. library(googleway) mgoogle_places &lt;- memoise(google_places) Back to the restful catAPI example: If you run it the mGET instead of GET many times, you will get the same result over and over again. content(mGET(&quot;https://thatcopy.pw/catapi/rest/&quot;)) It is because the response from the API is cached locally. All subsequent identical requests will not be made online. Instead they get fetched from the local cache. If you don’t need the local cache anymore, delete it using the forget function. forget(mGET) For more information about memoisation, please refer to the official website of memoise. For the Unix users (Mac OSX, Linux, FreeBSD, Solaris, HPUX, etc.) reading this, you can also define envvars in your hidden .*rc file (e.g. .bashrc or .zshrc, depending on your shell). The method is to set that up using export YT_CLIENTSECRET=\"MbOSt6cQhhFkwETXKur-L9rN\". The envvars defined in your .*rc file can also be retrieved by Sys.getenv. The section on Google Natural Language API actually contains an example. If you have an habit of publishing your dotfiles, you should store these envvars in your .localrc instead. Well, if you know what dotfiles are, your Unix Wizardry should be able to tell you how to do that.↩︎ For the people who need to use Github Actions to run or test your code, you can also store those envvars in your R code as Github Encrypted Secrets.↩︎ "],["ckan-api.html", "Chapter 3 CKAN API 3.1 Provided services/data 3.2 Prerequisites 3.3 Simple API call 3.4 API access in R 3.5 Social science examples", " Chapter 3 CKAN API Barbara K. Kreis The CKAN API is an API offered by the open-source data management system (DMS) CKAN (Open Knowledge Foundation). Currently, CKAN is used as a DMS by many different users, governmental institutions and corporations alike. This API review will focus on the use of the CKAN API to access and work with open government data. As the CKAN DMS is used by various governments to offer open datasets, it is a helpful tool for researchers to access this treasure of publicly open information. CKAN hosts free datasets from various governments, such as from Germany, Canada, Australia, the Switzerland and many more. You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;tidyverse&#39;, &#39;ckanr&#39;, &#39;jsonlite&#39;, &#39;readxl&#39;, &#39;curl&#39;) If the code presented in this chapter renders an error when you execute it, you can re-run the code using the day-specific snapshot of the CRAN server when we were compiling this chapter. Some of the R packages that you have installed and loaded above might regularly get updated, which often comes with slight changes in functionality. R will install all packages that you have loaded with the p_load() function above in the same version as we used them on the date at which we last worked on the code. To ensure replicability, simply run # install.packages(&#39;checkpoint&#39;) library(checkpoint) checkpoint(&#39;2024-06-21&#39;) 3.1 Provided services/data What data/service is provided by the API? All of CKAN’s core features can be accessed via the CKAN API (Open Knowledge Foundation) With the CKAN API, you can Get a JSON-formatted list of a site’s objects, datasets or groups. Get a full JSON representation of an object, e.g. a dataset. Search for any packages (datasets) or resources that match a query. Get an activity stream of recently changed datasets on a site. Please see the following link for more information on the services provided by the CKAN API and some specific examples. When it comes to the specific datasets on the government sites, there are two types that can be accessed: specific datasets and meta data sets. For example, the German and the US Government have a website each, where you can get access to metadata that include descriptions and URLs about the specific open datasets that can be accessed. These meta datasets can be a starting point for research on a specific topic. The specific datasets include a variety of different contents from public administration, such as election results, data on schools, maps and many more. The German data portal govdata.de for example serves as a collection point for all those data from various institutions. Those specific administrative institutions are the ones that actually provide the data. Therefore, not every institution provides the same data on the same topic. 3.2 Prerequisites What are the prerequisites to access the API (authentication)? There are no prerequisites to access the CKAN API. Furthermore, there seem to be no prerequisites to access the open data from the various governmental institutions using CKAN. 3.3 Simple API call What does a simple API call look like? When a user wants to make an API call, two use cases have to be distinguished: Calling meta-data and calling specific datasets. Meta-datasets When calling the meta data, the DCAT catalog has to be queried. DCAT-AP.de is a German metadata model to exchange open government data. For more information and information on the meta data structure, see this website. The API call for the DCAT catalog can deliver three formats: RDF, Turtle and JSON-LD. The type of format can be specified at the end of the request (e.g. “format=jsonld”). The following API call is an example for the search term “Kinder”. https://ckan.govdata.de/api/3/action/dcat_catalog_search?q=kinder&amp;format=jsonld Specific datasets from GovData To look for specific datasets, not the meta data, only little has to be changed in the URL. In the case of querying specific datasets, the response format is JSON. The following API call is an example when looking for the first 5 packages (datasets) that contain the search term “Kinder” (=children). https://www.govdata.de/ckan/api/3/action/resource_show?q=kinder 3.4 API access in R How can we access the API from R (httr + other packages)? The CKAN API can be accessed from R with the httr package or the ckanr package. Please note that as a scientist you can only use GET requests. All kinds of POST requests are restricted to government employees that work at the institutions which provide the data sets. # CKAN API # # Option 1: Use the httr package to access the API library(httr) # required to work with the API # With the following query we get the same information as described in the paragraph above base_url &lt;- &quot;https://www.govdata.de/ckan/api/3/action/resource_show&quot; berlin &lt;- GET(base_url, query=list(q=&quot;kinder&quot;,rows=5)) # Option 2: Use the ckanr package to access the API # load relevant packages library(tidyverse) library(ckanr) library(jsonlite) library(readxl) library(curl) #connect to the website url_site &lt;- &quot;https://www.govdata.de/ckan&quot; ckanr_setup(url = url_site) # first, let&#39;s see which groups are on this site group_list(as = &quot;table&quot;) #you can see there are different groups #now we want to look at them in more detail group_list(limit = 2) # now you can look for the specific packages package_list(as = &quot;table&quot;) # now, let&#39;s look at a specific package more closely, to get some more information package_show(&quot;100-jahre-stadtgrun-stadtpark-und-volkspark&quot;) # now, let&#39;s do a more specific search for specific resources (we look at Kinder = kids/ children) x &lt;- resource_search(q = &quot;name:Kinder&quot;, limit = 3) x$results # here you get the name, the Description (not always filled out) and the data format # now we want to have a closer look at the second resource (day care for children) # we need to get the url, by using the resource number url&lt;-resource_show(id =&quot;a8413550-bf4d-40f3-921a-941da3fce132&quot;) url$url # with the url, we can now import the data url &lt;- (&quot;https://geo.sv.rostock.de/download/opendata/kindertagespflegeeinrichtungen/kindertagespflegeeinrichtungen.csv&quot;) destfile &lt;- (&quot;kindertagespflegeeinrichtungen.csv&quot;) curl::curl_download(url, destfile) kindertagespflegeeinrichtungen &lt;- read_csv(destfile) View(kindertagespflegeeinrichtungen) # in this file, you can now for example look at the opening hours of the day cares in Rostock (a German city) 3.5 Social science examples Are there social science research examples using the API? When looking for social science research that used the CKAN API and Open Government data (OGD), it seems that there is more papers and research on the usage of those data, than on the data themselves (Bedini (2014), Corrêa (2015)). In a recent paper that examines the use of OGD (Quarati and De Martino (2019)), the authors come to the conclusion, that on the one hand many OGD portals lack information about data usage, and on the other hand, where those information can be found, it becomes obvious that the data are only rarely used. For example, regarding the German OGD portal “GovData.de”, I did not find any social science papers that specifically used data from GovData.de. However, there are a few papers available that describe the German open data initiative (Liu (2018)) and the metadata (Marienfeld (2013)) that can be found on GovData.de. References Bedini, Farazi, I. 2014. “Open Government Data: Fostering Innovation. EJournal of EDemocracy and Open Government.” EJournal of EDemocracy and Open Government. Corrêa, Corrêa, A. S. 2015. “A Collaborative-Oriented Middleware for Structuring Information to Open Government Data.” Proceedings of the 16th Annual International Conference on Digital Government Research. Liu, Liu, C. 2018. “Open Government Data: The German Government Is Moving.” Asia-Pacific Social Science and Modern Education Conference (SSME 2018). Marienfeld, Schieferdecker, F. 2013. “Metadata Aggregation at GovData.de: An Experience Report.” Proceedings of the 9th International Symposium on Open Collaboration. Quarati, Alfonso, and Monica De Martino. 2019. “Open Government Data Usage: A Brief Overview.” In IDEAS 2019, 23rd International Database Engineering &amp; Applications Symposium. unknown. "],["crowdtangle-api.html", "Chapter 4 CrowdTangle API 4.1 Provided services/data 4.2 Prerequisites 4.3 Simple API call 4.4 API access in R 4.5 Social science examples", " Chapter 4 CrowdTangle API Lion Behrens and Pirmin Stöckle CrowdTangle is a public insights tool, whose main intent was to monitor what content overperformed in terms of interactions (likes, shares, etc.) on Facebook and other social media platforms. In 2016, CrowdTangle was acquired by Facebook that now provides the service. You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;devtools&#39;, &#39;RCrowdTangle&#39;, &#39;dplyr&#39;, &#39;jsonlite&#39;) 4.1 Provided services/data What data/service is provided by the API? CrowdTangle allows users to systematically follow and analyze what is happening with public content on the social media platforms of Facebook, Twitter, Instagram and Reddit. The data that can be assessed through the CrowdTangle API consists of any post that was made by a public page, group or verified public person who has ever acquired more than 110,000 likes since the year 2014 or has ever been added to the list of tracked public accounts by any active API user. If a new public page or group is added, data is pulled back from day one. Data that is tracked: Content (the content of a post, including text, included links, links to included images or videos) Interactions (count of likes, shares, comments, emoji-reactions) Page Followers Facebook Video Views Benchmark scores of all metrics from the middle 50% of posts in the same category (text, video) from the respective account Data that is not tracked: Comments (while the number of comments is included, the content of the comments is not) Demographical data Page reach, traffic and clicks Private posts and profiles Ads only appear in the ad library (which is public), boosted content cannot differentiated from organic content CrowdTangle’s database is updated once every fifteen minutes and comes as time-series data which merges the content of a post on one of the included platforms (a text post, video, or image) alongside aggregate information on the post’s views, likes and interactions. When connecting to the user interface via the CrowdTangle website, the user can either manually set up a list of pages of interest whose data should be acquired. Alternatively, one can choose from an extensive number of pre-prepared lists covering a variety of topics, regions, or socially and politically relevant events such as inaugurations and elections. Data can be downloaded from the user interface as csv files or as json files via the API. 4.2 Prerequisites What are the prerequisites to access the API (authentication)? Full access to the CrowdTangle API is only given to Facebook partners who are in the business of publishing original content or fact-checkers as part of Facebook’s Third-Party Fact-Checking program. From 2019, the CrowdTangle API and user interface is also available for academics and researchers in specific fields. Currently, this prioritization includes research on one of the following fields: misinformation, elections, COVID-19 racial justice, well-being. To get access to CrowdTangle, a formal request has to be filed via an online form, asking for a short description of the research project and intended use of the data. As a further restriction, CrowdTangle currently only allows academic staff, faculty and registered PhD students permission to obtain a CrowdTangle account. This does not include individuals enrolled as students at a university unless they are employed as research assistants. Also, certain access policies differ between academics and the private sector. Usage of CrowdTangle for research purposes does currently not provide access to any content posted on Reddit given that data is retrieved via the Application Programming Interface. Content from Reddit is open to every registered user only when navigating through the company’s dynamic user interface that does not imply usage of any scripting language. Finally, the CrowdTangle API requires researchers to log in using an existing Facebook account. Overall, access to the API is quite restrictive, both because of the prioritization of certain research areas, and because the access request will be decided individually so that an immediate access is not possible. If access is granted, CrowdTangle provides quite extensive onboarding and training resources to use the API. Replicability Access to CrowdTangle is gated and Facebook does not allow data from CrowdTangle to be published. So researchers can publish aggregate results from analyses on the data, but not the original data, which might be problematic for the replicability of research conducted with the API. A possible workaround is that you can pull ID numbers of posts in your dataset, which can then be used by anyone with a CrowdTangle API access to recreate your dataset. CrowdTangle also provides some publicly available features such as a Link Checker Chrome Extension, allowing users to see how often a specific link has been shared on social media, and a curated public hub of Live displays, giving insight about specific topics on Facebook, Instagram and Reddit. 4.3 Simple API call What does a simple API call look like? All requests to the CrowdTangle API are made via GET to https://api.crowdtangle.com/. In order to access data, users log in on the website with their Facebook account and acquire a personalized token. The CrowdTangle API expects the API token to be included in each query. With one of these available endpoints, each of which comes with a set of specific parameters: GET /posts Retrieve a set of posts for the given parameters. GET /post Retrieves a specific post. GET /posts/search Retrieve a set of posts for the given parameters and search terms. GET /leaderboard Retrieves leaderboard data for a certain list or set of accounts. GET /links Retrieve a set of posts matching a certain link. GET /lists Retrieve the lists, saved searches and saved post lists of the dashboard associated with the token sent in. A simple example: Which party or parties posted the 10 most successful Facebook posts this year? On the user interface, I created a list of the pages of all parties currently in the German Bundestag. We want to find out which party or parties posted the 10 most successful posts (i.e. the posts with the most interactions) this year. The respective API call looks like that: https://api.crowdtangle.com/posts?token=token&amp;listIds=listIDs&amp;sortBy=total_interactions&amp;startDate=2021-01-01&amp;count=10, where token is the personal API key, and listIDs is the ID of the list created with the user interface. Here, we sortBy total interactions with the startDate at the beginning of this year and the output restricted to count 10 posts. 4.4 API access in R How can we access the API from R (httr + other packages)? Instead of typing the API request into our browser, we can use the httr package’s GET function to access the API from R. # Option 1: Accessing the API with base &quot;httr&quot; commands library(httr) ct_posts_resp &lt;- GET(&quot;https://api.crowdtangle.com/posts&quot;, query=list(token = Sys.getenv(&quot;Crowdtangle_token&quot;), # API key has to be included in every query listIds = listIds, # ID of the created list of pages or groups sortBy = &quot;total_interactions&quot;, startDate = &quot;2021-01-01&quot;, count = 10)) ct_posts_list &lt;- content(ct_posts_resp) class(ct_posts_list) # verify that the output is a list # List content str(ct_posts_list, max.level = 3) # show structure &amp; limit levels # with some list operations we can get a dataframe with the account name and post date of the 10 posts with the most interactions in 2021 among the pages in the list list_part &lt;- rlist::list.select(ct_posts_list$result$posts, account$name, date) rlist::list.stack(list_part) Alternatively, we can use a wrapper function for R, which is provided by the RCrowdTangle package available on github. The package provides wrapper functions for the /posts, /posts/search, and /links endpoints. Conveniently, the wrapper function directly produces a dataframe as output, which is typically what we want to work with. As the example below shows, the wrapper function may not include the specific information we are looking for, however, as the example also shows, it is relatively straightforward to adapt the function on our own depending on the specific question at hand. To download the package from github, we need to load the devtools package, and to use the wrapper function, we need dplyr and jsonlite. # Option 2: There is a wrapper function for R, which can be downloaded from github library(devtools) # to download from github install_github(&quot;cbpuschmann/RCrowdTangle&quot;) library(RCrowdTangle) # The R wrapper relies on jsonlite and dplyr library(dplyr) library(jsonlite) ct_posts_df &lt;- ct_get_posts(listIds, startDate = &quot;2021-01-01&quot;, token = token) #conveniently, the wrapper function directly produces a dataframe class(ct_posts_df) # to sort by total interactions we have to compute that figure because it is not part of the dataframe ct_posts_df %&gt;% mutate(total_interactions = statistics.actual.likeCount+statistics.actual.shareCount+ statistics.actual.commentCount+ statistics.actual.loveCount+ statistics.actual.wowCount+ statistics.actual.hahaCount+ statistics.actual.sadCount+ statistics.actual.angryCount+ statistics.actual.thankfulCount+ statistics.actual.careCount) %&gt;% arrange(desc(total_interactions)) %&gt;% select(account.name, date) %&gt;% head(n=10) # alternatively, we can adapt the wrapper function by ourselves to include the option to sort by total interactions ct_get_posts &lt;- function(x = &quot;&quot;, searchTerm = &quot;&quot;, language = &quot;&quot;, types= &quot;&quot;, minInteractions = 0, sortBy = &quot;&quot;, count = 100, startDate = &quot;&quot;, endDate = &quot;&quot;, token = &quot;&quot;) { endpoint.posts &lt;- &quot;https://api.crowdtangle.com/posts&quot; query.string &lt;- paste0(endpoint.posts, &quot;?listIds=&quot;, x, &quot;&amp;searchTerm=&quot;, searchTerm, &quot;&amp;language=&quot;, language, &quot;&amp;types=&quot;, types, &quot;&amp;minInteractions=&quot;, minInteractions, &quot;&amp;sortBy=&quot;, sortBy, &quot;&amp;count=&quot;, count, &quot;&amp;startDate=&quot;, startDate, &quot;&amp;endDate=&quot;, endDate, &quot;&amp;token=&quot;, token) response.json &lt;- try(fromJSON(query.string), silent = TRUE) status &lt;- response.json$status nextpage &lt;- response.json$result$pagination$nextPage posts &lt;- response.json$result$posts %&gt;% select(-expandedLinks, -media) %&gt;% flatten() return(posts) } ct_posts_df &lt;- ct_get_posts(listIds, sortBy = &quot;total_interactions&quot;, startDate = &quot;2021-01-01&quot;, token = token) ct_posts_df %&gt;% select(account.name, date) %&gt;% head(n=10) 4.5 Social science examples Are there social science research examples using the API? A common use case is to track the spread of specific links containing misinformation, e.g. conspiracy around the connection of COVID-19 and 5G (Bruns, Harrington, and Hurcombe (2020)). Berriche and Altay (2020) provide an in-depth analysis of a specific page involved in online health misinformation and investigate factors driving interactions with the respective posts. They find that users mainly interact to foster social relations, not to spread misinformation. CrowdTangle has also been used to study changes in the framing of vaccine refusal by analyzing content of posts by pages opposing vaccinations over time (Broniatowski et al. (2020)). Another approach is to monitor political communication of political actors, specifically in the run-up to elections. Larsson (2020) investigates a one-month period before the 2018 Swedish election and finds that right-wing political actors are more successful than mainstream actors in engaging their Facebook followers, often using sensational rhetoric and hate-mongering. References Berriche, Manon, and Sacha Altay. 2020. “Internet Users Engage More with Phatic Posts Than with Health Misinformation on Facebook.” Palgrave Communications 6 (1): 1–9. Broniatowski, David A, Amelia M Jamison, Neil F Johnson, Nicolás Velasquez, Rhys Leahy, Nicholas Johnson Restrepo, Mark Dredze, and Sandra C Quinn. 2020. “Facebook Pages, the ‘Disneyland’ Measles Outbreak, and Promotion of Vaccine Refusal as a Civil Right, 2009-2019.” Am. J. Public Health 110 (S3): S312–18. Bruns, Axel, Stephen Harrington, and Edward Hurcombe. 2020. “‘Corona? 5G? Or Both?’: The Dynamics of COVID-19/5G Conspiracy Theories on Facebook.” Media International Australia 177 (1): 12–29. Larsson, Anders Olof. 2020. “Right-Wingers on the Rise Online: Insights from the 2018 Swedish Elections.” New Media &amp; Society 22 (12): 2108–27. "],["facebook-ad-library-api.html", "Chapter 5 Facebook Ad Library API 5.1 Provided services/data 5.2 Prerequisites 5.3 Simple API call 5.4 API access in R 5.5 Social science examples", " Chapter 5 Facebook Ad Library API Ondřej Pekáček You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;remotes&#39;, &#39;dplyr&#39;, &#39;ggplot2&#39;, &#39;tidyr&#39;, &#39;Radlibrary&#39;, &#39;dplyr&#39;, &#39;tidyr&#39;, &#39;DT&#39;) 5.1 Provided services/data What data/service is provided by the API? The Faceboook Ad Library API is provided by Facebook and forms an extended part of its Graph API ecosystem. As of December 2021, it provides limited access to the Facebook Ad Library. While the web version enables the user to search multiple types of paid advertising on Facebook platforms (including Instagram and WhatsApp), the API offers programmatic access to political/social issues ads. Facebook currently imposes a storage limit of 7 years for paid advertising on its platforms. Facebook also offers summarized information (on a per-country basis) about paid political ads in its Ads Library Report. However, this website only offers information on selected countries and displays information regarding the sum spent by the advertisers, neglecting the information on demographic and regional targeting. The API enables its users to get information on every paid political ad, regardless of whether it is still active. Researchers are currently able to obtain the following data (among others) for each individual paid ad: Full text, date, unique link, and other metadata Information on financial spending Reach and impressions metrics Gender and age category demographics Country/region targeting information Please note that some key figures are not precise, such as spending, impressions, and reach. Instead, they are provided as min-max range estimates (such as spend_lower and spend_upper variables). Also, we should keep in mind that the API enforces a rate limit of 200 calls per hour. 5.2 Prerequisites What are the prerequisites to access the API (authentication)? As a first step, you need to confirm their identity and location, which should take up to 48 hours. Secondly, proceed to the Facebook Developers and create an account there. Finally, you also need to create a Facebook App to use the API. You might also want to take note of the App ID and App Secret, which are found in the Settings-&gt;Basic section of the App. Unlike some other APIs, such as Twitter, Facebook does not provide us with a “permanent” access token. After the login, we get a short-lived (max. 2 hours) access token. It can be retrieved from the Graph API Explorer. Fortunately, we could easily extend the token to about two months with the Access Token Debugger tool. We only need to paste the token, click Debug, and click the Extend Access Token button below the information panel and enter our Facebook password into the prompt. In the future, this process can also be done programmatically by calling the API with the short-lived access token, App ID, and App Secret (more information here. As recommended in Chapter 2, be careful not to include the token explicitly in your script for security purposes. In this chapter, we will set the token as the environment variable FB_TOKEN using .Renviron file and restart R for the changes to take effect. Running the Sys.getenv(\"FB_TOKEN\") should provide us with a correct token. 5.3 Simple API call What does a simple API call look like? Firstly, it might be helpful to get familiar with the API parameters that we can request by reading the official documentation. We will follow the sample example on the API documentation page and replicate the simple call. However, we will use R’s interface instead of cURL in a command line in our case. To this end, we need to first load the required packages in this script. #loading packages library(httr) library(remotes) library(dplyr) library(ggplot2) library(tidyr) We are using the httr package to make the API call - it has already been loaded in the previous step. # We will be using the ads_archive endpoint of the 12th version of the Graph. endpoint_url &lt;- &quot;https://graph.facebook.com/v12.0/ads_archive&quot; # Specify the query parameters to mirror the one in official documentation. my_query &lt;- list( search_terms = &quot;california&quot;, ad_type = &quot;POLITICAL_AND_ISSUE_ADS&quot;, ad_reached_countries = &quot;US&quot;, access_token = Sys.getenv(&quot;FB_TOKEN&quot;) ) # Using the URL endpoint and the list of our queries, we make the API call. raw_response &lt;- httr::GET( url = endpoint_url, # Using this header is not necessary here, but it is usually a good practice # to specify we want a JSON back (some APIs might send us XML by default). httr::add_headers(Accept = &quot;application/json&quot;), query = my_query ) # Check the status of the response. If everything is OK, it should be 200. cat(&quot;Our status message is: \\n&quot;) httr::http_status(raw_response) # Finally, we inspect the content of the response. R sees list of lists, # which could be converted to other formats, such as a data frame. # We will select the second sub-list named &quot;data&quot;. parsed_response &lt;- httr::content(raw_response, as = &quot;parsed&quot;)[[&quot;data&quot;]] cat(&quot;The first ad in our response is: \\n&quot;) parsed_response[[1]] We could use additional parameters under the query list specified in the documentation. However, this approach can be relatively more complex for more focused use. For instance, each API response also returns a link to the next page (pagination), which means one would need to write quite a long script that takes this into account. Another complication is that part of the URL in the response, which links to the following page or to the individual ad itself, contains your full access token. You must thus make sure to remove it when sharing the data further. Fortunately, we do not need to deal with these issues directly for most use cases. Instead, we will use the Radlibrary package for R, which significantly simplifies the API calls by providing an easy-to-understand functions. This package can handle the pagination automatically and reliably removes the access token from any downloaded data. 5.4 API access in R How can we access the API from R (httr + other packages)? Aside from writing our API functions using the httr package, we could use the Radlibrary, an open-source package written for R. As of December 2021, Radlibrary is not yet available at R’s primary CRAN repository. Hence its installation is (slightly) more complicated since it needs to be installed directly from its GitHub repository instead. For this process, we will use the install_github() function, for which you either need to have devtools or remotes (a more lightweight package used here) installed. remotes::install_github(&quot;facebookresearch/Radlibrary&quot;) library(Radlibrary) Radlibrary can also simplify the long-term access token retrieval discussed above. Run the function following functions. If you already have an FB_TOKEN environment variable set up from the previous step, you can skip this part. However, most APIs will not be doing the same for us, so it is a valuable skill to do this manually. # User-friendly setup that asks you for app ID and secret. adlib_setup() # This exchanges the short term token for the long term one. adlib_set_longterm_token() # You can verify that the token has been set correctly. token_get() Our first query with Radlibrary Once the package is installed, we can construct a more complicated query with just a few lines. We will focus on the issue of housing in the UK in November 2021. detailed_query &lt;- adlib_build_query( # Let&#39;s select only United Kingdom. ad_reached_countries = &quot;GB&quot;, # We want to get both active and inactive ads. ad_active_status = &quot;ALL&quot;, search_terms = &quot;Housing&quot;, # Because of the amount of ads, we will extract only one week. ad_delivery_date_min = &quot;2021-11-01&quot;, ad_delivery_date_max = &quot;2021-11-07&quot;, # We can only access Political/Social Issues ads using the API. ad_type = &quot;POLITICAL_AND_ISSUE_ADS&quot;, # We want the adds for these platforms owned by Facebook/Meta. publisher_platform = c( &quot;FACEBOOK&quot;, &quot;INSTAGRAM&quot;, &quot;MESSENGER&quot;, &quot;WHATSAPP&quot; ), # This is the default limit for the API response. # Since we use pagination, we can keep it as it is. limit = 1000, # What will be included in the returned data? Can also be # &quot;demographic_data&quot; or &quot;region_data&quot;, among others. fields = &quot;ad_data&quot; ) The query is “lazy.” Our API call will not be executed unless we specifically ask for it. # The function adlib_get_paginated is a version of adlib_get, suitable for # larger requests. If you got token using the adlib_setup() function, you do # not have to specify this argument. However, we will be using the # environment variable set in the previous part of the chapter. ads_list &lt;- adlib_get_paginated(detailed_query, token = Sys.getenv(&quot;FB_TOKEN&quot;)) We can convert the list to a standard dataset using the as_tibble() function, because the ads_list is a particular type of class called paginated_adlib_data_response. This means we can specify other arguments to the as_tibble() function, such as the type of the table we require and whether we wish to censor our access token from the data. # The &quot;type&quot; argument must correspond to the &quot;fields&quot; argument in the # adlib_build_query like this: # &quot;ad_data&quot; = &quot;ad&quot;, &quot;region_data&quot; = &quot;region&quot;, &quot;demographic_data&quot; = &quot;demographic&quot;. ads_df &lt;- as_tibble(ads_list, type = &quot;ad&quot;, censor_access_token = TRUE) Practical case study: Housing in the UK through the prism of political advertising on Facebook’s platforms and its audience # First, save all of the data types that we will ask the API to extract. fields_vector &lt;- c(&quot;ad_data&quot;, &quot;region_data&quot;, &quot;demographic_data&quot;) # Correspondingly, save all of the table types. table_type_vector &lt;- c(&quot;ad&quot;, &quot;region&quot;, &quot;demographic&quot;) # Initiate an empty list to which we will append the extracted API data. # The list could be initiated simply by using list(); however, especially for # larger data sets, specifying the length of a list in R in advance speeds up # the processing. The length of the list equals our 3 data types. fb_ad_list &lt;- vector(mode = &quot;list&quot;, length = length(fields_vector)) # We will also name its three items with values from table_type_vector so we can # refer to them further names(fb_ad_list) &lt;- table_type_vector We are using a for loop this time, where the API call in each iteration is the same, with the difference in the asked data type. Unlike in the first example, we are interested in the ads themselves and their audience. for (i in seq_along(fields_vector)) { print(paste(&quot;Extracting the&quot;, fields_vector[i])) query &lt;- adlib_build_query( ad_reached_countries = &quot;GB&quot;, ad_active_status = &quot;ALL&quot;, search_terms = &quot;Housing&quot;, ad_delivery_date_min = &quot;2021-11-05&quot;, ad_delivery_date_max = &quot;2021-11-07&quot;, ad_type = &quot;POLITICAL_AND_ISSUE_ADS&quot;, publisher_platform = c(&quot;FACEBOOK&quot;, &quot;INSTAGRAM&quot;), fields = fields_vector[i] ) # The call is limited to 1000 results but pagination of overcomes it. # We pipe the output of the paginated call to the as_tibble function. fb_ad_list[[table_type_vector[i]]] &lt;- adlib_get_paginated(query, token = Sys.getenv(&quot;FB_TOKEN&quot;) ) %&gt;% as_tibble( type = table_type_vector[i], censor_access_token = TRUE ) } After extraction using the for loop, we have three data frames in one list. However, these datasets are in a different format and with a different number of rows. The only information that unites them is the unique ID of each ad, which we will use when merging them. # The demographic &amp; region datasets are in the &quot;long&quot; format (multiple # rows of information for each ad), and we need a transformation to a &quot;wide&quot; # format (single row per ad) of the ad dataset using the tidyr package. fb_ad_list[[&quot;demographic&quot;]] &lt;- pivot_wider(fb_ad_list[[&quot;demographic&quot;]], id_cols = adlib_id, names_from = c(&quot;gender&quot;, &quot;age&quot;), names_sort = TRUE, values_from = percentage ) fb_ad_list[[&quot;region&quot;]] &lt;- pivot_wider(fb_ad_list[[&quot;region&quot;]], id_cols = adlib_id, names_from = region, names_sort = TRUE, values_from = percentage ) # Performing a left join on the common id column across the 3 datasets, remove # full duplicates and arrange by date. merged_dataset &lt;- fb_ad_list[[&quot;ad&quot;]] %&gt;% left_join(fb_ad_list[[&quot;demographic&quot;]], by = &quot;adlib_id&quot;) %&gt;% left_join(fb_ad_list[[&quot;region&quot;]], by = &quot;adlib_id&quot;) %&gt;% distinct() %&gt;% arrange(desc(ad_creation_time)) We end up with a “tidy” dataset, in which each row is one observation (ads) and columns are variables such as spending, reach, age group and region, making it amenable to quick summarisation and exploratory visualizations. Please note that you only need one ad that displays internationally in your dataset and you will end up with many extra region columns that are NAs for most ads. For instance, in our case, we get UK regions columns and all of the US states together with some other EU regions as well! In reality, only two ads targeted both UK and other international regions in our small sample. As a result, it means that the extraction of the region data could take quite a bit longer than that of the other data. Practically, we would probably need to consider careful data cleaning after closely inspecting the dataset. As a final part of this exploration, let’s create some summary statistics on UK housing ads from the first week of November 2021, using a few selected variables in our sample. # Using the dataset containing combined ads, demographic and region data, we # select only ads from the first week of November 2021, group by Facebook pages, # which paid for more than one add during this period. For these observations, # we create summary statistics on selected variables. merged_dataset %&gt;% filter(ad_delivery_start_time &gt;= &quot;2021-11-01&quot; &amp; ad_delivery_start_time &lt;= &quot;2021-11-07&quot;) %&gt;% group_by(page_name) %&gt;% summarise( nr_ads = n(), spend_upper_avg = mean(spend_upper, na.rm = TRUE), impressions_upper_avg = mean(impressions_upper, na.rm = TRUE), avg_prop_England = mean(England, na.rm = TRUE), avg_prop_female_25_34 = mean(`female_25-34`, na.rm = TRUE), avg_prop_male_25_34 = mean(`male_25-34`, na.rm = TRUE), avg_prop_female_65_plus = mean(`female_65+`, na.rm = TRUE), avg_prop_male_65_plus = mean(`male_65+`, na.rm = TRUE) ) %&gt;% filter(nr_ads &gt; 1) %&gt;% arrange(desc(nr_ads)) %&gt;% # To visualize the information, we use DataTables package, which allows for # interactivity (such as sorting and horizontal scrolling). datatable( extensions = &quot;FixedColumns&quot;, options = list( scrollX = TRUE, fixedColumns = TRUE, dom = &quot;t&quot;, # DataTables does not display NAs, however, we can use a small JavaScript # snippet to fill in the missing values in the table (optional). rowCallback = JS(c( &quot;function(row, data){&quot;, &quot; for(var i=0; i&lt;data.length; i++){&quot;, &quot; if(data[i] === null){&quot;, &quot; $(&#39;td:eq(&#39;+i+&#39;)&#39;, row).html(&#39;NA&#39;)&quot;, &quot; .css({&#39;color&#39;: &#39;rgb(151,151,151)&#39;, &#39;font-style&#39;: &#39;italic&#39;});&quot;, &quot; }&quot;, &quot; }&quot;, &quot;}&quot; )) ) ) %&gt;% # DataTables enable us to format the data directly in the visual table, we do # not necessarily need to make these changes to the original dataset. formatCurrency(3, &quot;\\U00A3&quot;) %&gt;% formatPercentage(5:9, 2) %&gt;% formatRound(4, 0) 5.5 Social science examples Are there social science research examples using the API? Since its launch in late 2018, the Facebook Ads library has attracted interdisciplinary research attention. Among the peer-reviewed research findings, we find the theoretical overview of the situation of the scholarly social media API access in the aftermath of Cambridge Analytica by Bruns (2019), mentioning the newly opened, “bespoke” Facebook Ad Library, but pointing out its lack of “comprehensive search functionality” and “limited international coverage.” Focusing on the US context during the 2018 mid-term elections, Fowler et al. (2021) compared TV advertising with Facebook advertising, concluding that the latter tends to occur earlier in the campaign, is less hostile and issue-focused. A study by Schmøkel and Bossetta (2021) presents a quantitative analysis of images from the Ad Library using clustering and visual emotion classification during the 2020 primary elections. Perhaps surprisingly, their results indicate that most of the images communicated happiness and calm, but the repertoire of the “core” images was minimal (434 out of over 80,000 images across eight candidates); hence most of the photos were merely repeated with a different text overlay. A recent example of a cross-country study is Bene and Kruschinski (2021), whose chapter uses the API to analyze the political advertisement of parties in 12 EU countries in the run-up to the 2019 European Parliament elections, finding that advertisements seem to form the majority of parties’ activities on Facebook but, also, a considerable divergence in cross-country usage patterns. There is literature focused on the weaknesses of the data provided by the Facebook Ad library. For instance, in the case of Spain, Cano-Orón et al. (2021) focuses on the issue of disinformation during the 2019 Spanish general elections in the corpus of Facebook political ads, and Calvo, Cano-Orón, and Baviera (2021) uses the same corpus and period to conduct an exploratory analysis of the political communication on Facebook. However, the authors note that due to the limitations for 2019 data using the API, they had to resort to web crawling of the Ad Library instead. It is also important to note that some NGOs, such as Transparency International, are involved in the advocacy for a more transparent and comprehensive reporting in the Facebook Ad Library. Finally, there is a body of preliminary research using the Facebook Ad Library API - publications currently in the form of pre-prints or conference proceedings. For example, Edelson, Lauinger, and McCoy (2020) investigates the security weaknesses of the Facebook advertising ecosystem, enabling malicious advertisers to conduct undeclared coordinated activities and construct inauthentic communities. The Italian political discourse regarding migration is at the center of exploration by Capozzi et al. (2021). To conclude, there are still significant gaps in the academic knowledge regarding the Facebook paid political advertising. More research attention paid to cross-country comparison, visual information, or ethical considerations would be beneficial to understand better the functioning of this ecosystem and its societal implications. Given the recently expanded transparency of Google regarding political advertisement on its platform, upcoming research should also consider using the Google Political Ads dataset in tandem with Facebook’s Ad Library API. References Bene, Márton, and Simon Kruschinski. 2021. “Political Advertising on Facebook.” In Campaigning on Facebook in the 2019 European Parliament Election: Informing, Interacting with, and Mobilising Voters, edited by Jörg Haßler, Melanie Magin, Uta Russmann, and Vicente Fenoll, 283–99. Political Campaigning and Communication. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73851-8_18. Bruns, Axel. 2019. “After the APIcalypse: Social Media Platforms and Their Fight Against Critical Scholarly Research.” Information, Communication &amp; Society 22 (11): 1544–66. https://doi.org/10.1080/1369118X.2019.1637447. Calvo, Dafne, Lorena Cano-Orón, and Tomás Baviera. 2021. “Global Spaces for Local Politics: An Exploratory Analysis of Facebook Ads in Spanish Election Campaigns.” Social Sciences 10 (7): 271. https://doi.org/10.3390/socsci10070271. Cano-Orón, Lorena, Dafne Calvo, Guillermo López García, and Tomás Baviera. 2021. “Disinformation in Facebook Ads in the 2019 Spanish General Election Campaigns.” Media and Communication 9 (1): 217–28. https://doi.org/10.17645/mac.v9i1.3335. Capozzi, Arthur, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti, André Panisson, and Daniela Paolotti. 2021. “Clandestino or Rifugiato? Anti-immigration Facebook Ad Targeting in Italy.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15. Yokohama Japan: ACM. https://doi.org/10.1145/3411764.3445082. Edelson, Laura, Tobias Lauinger, and Damon McCoy. 2020. “A Security Analysis of the Facebook Ad Library.” In 2020 IEEE Symposium on Security and Privacy (SP), 661–78. San Francisco, CA, USA: IEEE. https://doi.org/10.1109/SP40000.2020.00084. Fowler, Erika Franklin, Michael M. Franz, Gregory J. Martin, Zachary Peskowitz, and Travis N. Ridout. 2021. “Political Advertising Online and Offline.” American Political Science Review 115 (1): 130–49. https://doi.org/10.1017/S0003055420000696. Schmøkel, Rasmus, and Michael Bossetta. 2021. “FBAdLibrarian and Pykognition: Open Science Tools for the Collection and Emotion Detection of Images in Facebook Political Ads with Computer Vision.” Journal of Information Technology &amp; Politics 0 (0): 1–11. https://doi.org/10.1080/19331681.2021.1928579. "],["genderize.io-api.html", "Chapter 6 Genderize.io API 6.1 Provided services/data 6.2 Prerequisites 6.3 Simple API call 6.4 API access in R 6.5 Social science examples", " Chapter 6 Genderize.io API Markus Konrad (WZB) You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;DemografixeR&#39;) 6.1 Provided services/data What data/service is provided by the API? The Genderize.io API provides a service for predicting the likely gender of a person given their name. The API is provided by Danish company Demografix ApS that also provides APIs for predicting age (Agify.io) and nationality (Nationalize.io) for a given name (more on that later).3 The service is useful to augment a dataset of individuals with their likely gender, when at least the individuals’ given name is known. The results provided by the API should be taken with care. As with many commercial APIs, the exact data sources and methods for the genderize.io API are not disclosed. The dedicated “Our Data” page only states that “[o]ur data is collected from all over the web” and provides a list with the amount of data that was collected for many countries (a total of over 114M entries at time of writing). Wais (2016) claims that scraped public social media profiles are used as data source. Another problem is that there’s only a binary categorization of gender. The categorization comes with a prediction probability estimate which in turn depends on the popularity of the name and of course the name itself (since there are many unisex names). Furthermore, gender prediction for a name may depend on country and year of birth (the Genderize.io API allows for country-specific results). One also has to keep in mind different name orders in different cultures. E.g., in North and South America as well as most of Europe it is common that the given name, from which a gender may be predicted, comes first before the family name, whereas in East Asia the given name often comes last. So in general, predicting an individual’s gender from their name is not an easy task. See Wais (2016) for an overview on modern gender prediction methods. You should only consider using the methods in this chapter when there’s no other way to obtain the gender information. In case you use the API, you should transparently report the limits of the name-based approach, use country-specific results whenever possible (see below), report the accuracy of the predictions, and use a threshold for the minimum accuracy and/or incorporate the prediction accuracy into your models. For gender prediction, there are also alternatives to using this API: the gender package provides gender prediction using historical data from the US and a few European countries (see Blevins and Mullen (2015)) NameCensus.com provides lists of most common female and male first names in the US WikiPedia provides categories for given names by gender 6.2 Prerequisites At the time of writing, the API can be queried with up to 1000 names per day for free. There’s not even an API key required for the free tier. However, if you require more than 1000 API requests per day, you need to obtain an API key from store.genderize.io – see this page for pricing. 6.3 Simple API call What does a simple API call look like? The API is very simple and basically accepts two parameters for an HTTP GET request: name as the given name for which gender prediction is performed; an array of up to 10 names per request can be send country_id as optional localization parameter (given as ISO 3166-1 alpha-2 country code) When no country_id is given, the gender prediction is performed using a database of given names for all countries with a notable bias towards Western countries (see the numbers on the “Our Data” page). If country information is known, you should provide it in the API request, as this gives more accurate, context-aware results. Especially if you’re working with names outside the Western cultural sphere, you should be aware of the Western bias in the datasets used for the predictions and make use of the localization feature. We can perform a sample request using the curl command in a terminal or by simply visiting the URL in a browser: curl &#39;https://api.genderize.io?name=sasha&#39; The result is an HTTP response with JSON formatted data which contains the predicted gender, the prediction probability estimate and the count of entries which informed the prediction. For the example requests above, the API responds with: { &quot;name&quot;: &quot;sasha&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;probability&quot;: 0.51, &quot;count&quot;: 13219 } This tells us that for the requested name “sasha”4, the gender was predicted as male, but only with probability 0.51. This makes sense since this name is considered a unisex name in many countries. The prediction is based on 13219 samples in the database, which seems very solid. Now to show the influence of localization, we try the German variant of this name, “Sascha”, and append the country_id parameter for Germany: curl &#39;https://api.genderize.io?name=sascha&amp;country_id=DE&#39; { &quot;name&quot;: &quot;sascha&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;probability&quot;: 0.99, &quot;count&quot;: 22408, &quot;country_id&quot;: &quot;DE&quot; } We can see that the localized request for Germany predicts “sascha” as male with 99% probability, based on 22408 database entries. Interestingly, only the Latinized forms of Sasha seem to be available in the database. Neither the Cyrillic form Саша, nor other non-Latin forms like Saša return results. However, experiments with other names in non-Latin alphabets show a pattern: The German name “Jürgen” has about 700 entries in the genderize database, while “Jurgen” has almost 4000 entries. The Turkish name “Gül” exists only 36 times but “Gul” gives almost 5000 entries. A similar pattern is seen when using accents: “André” exists three times, but “Andre” more than 64,000 times. So in general, it seems you should convert all non-Latin (or non-ASCII) characters in a name to Latin counterparts in order to get better results. You can send up to ten names per request, by concatenating several name[]=... parameters: curl &#39;https://api.genderize.io?name[]=sasha&amp;name[]=alex&amp;name[]=alexandra&#39; The predictions are then listed for each supplied name: [ {&quot;name&quot;: &quot;sasha&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;probability&quot;: 0.51, &quot;count&quot;: 13219}, {&quot;name&quot;: &quot;alex&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;probability&quot;: 0.9, &quot;count&quot;: 411319}, {&quot;name&quot;: &quot;alexandra&quot;, &quot;gender&quot;: &quot;female&quot;, &quot;probability&quot;: 0.98, &quot;count&quot;: 122985} ] 6.4 API access in R How can we access the API from R? There are several packages for R that provide convenient functions for communicating with the genderize.io API: DemografixeR (CRAN) – Package website GenderGuesser genderizeR5 Since DemografixeR is the only package available on CRAN at time of writing, I will use this package for further demonstration. You can install the package via install.packages('DemografixeR'). 6.4.1 Load package Once installed, the package can be loaded with the following command: library(DemografixeR) 6.4.2 The genderize function and its arguments The main function to use is the genderize() function. The first argument is the one or more names (as character string vector) for which you want to predict the gender. So to replicate the first API call from the previous section in R, we could write: genderize(&#39;sasha&#39;) [1] &quot;female&quot; Note that the output only consists of the gender prediction as character string vector. This is a dangerous default behavior, as it omits important information about the prediction probability and the size of the data pool used for the prediction. We need to set the simplify argument to FALSE in order to get that information in the form of a dataframe: genderize(&#39;sasha&#39;, simplify = FALSE) name type count gender probability 1 sasha gender 14512 female 0.52 Again, we can localize the request by using the country_id parameter: genderize(&#39;sascha&#39;, country_id = &#39;DE&#39;, simplify = FALSE) name type count gender country_id probability 1 sascha gender 22624 male DE 0.99 Supplying a character string vector will predict the gender of all these names. Note that with the genderize() function, you’re not limited to ten names as when using the API directly. Here, we predict the gender of six names in their original and Latinized variant each. This also shows the higher counts when using only Latin characters in the query: genderize(c(&#39;gül&#39;, &#39;gul&#39;, &#39;jürgen&#39;, &#39;jurgen&#39;, &#39;andré&#39;, &#39;andre&#39;, &#39;gökçe&#39;, &#39;gokce&#39;, &#39;jörg&#39;, &#39;jorg&#39;, &#39;rené&#39;, &#39;rene&#39;), simplify = FALSE) name type count gender probability 6 gül gender NA &lt;NA&gt; NA 5 gul gender NA &lt;NA&gt; NA 10 jürgen gender NA &lt;NA&gt; NA 9 jurgen gender NA &lt;NA&gt; NA 2 andré gender NA &lt;NA&gt; NA 1 andre gender NA &lt;NA&gt; NA 4 gökçe gender NA &lt;NA&gt; NA 3 gokce gender NA &lt;NA&gt; NA 8 jörg gender NA &lt;NA&gt; NA 7 jorg gender 818 male 0.98 12 rené gender NA &lt;NA&gt; NA 11 rene gender 37687 male 0.90 You can also provide a different country_id for each name in the request: genderize(c(&#39;sasha&#39;, &#39;sascha&#39;), country_id = c(&#39;RU&#39;, &#39;DE&#39;), simplify = FALSE) name type count gender country_id probability 2 sasha gender 2690 male RU 0.59 1 sascha gender 22624 male DE 0.99 This is especially helpful together with expand.grid(), which generates all combinations of values in the two vectors: names &lt;- c(&#39;sasha&#39;, &#39;sascha&#39;) countries &lt;- c(&#39;RU&#39;, &#39;DE&#39;) (names_cntrs &lt;- expand.grid(names = names, countries = countries, stringsAsFactors = FALSE)) names countries 1 sasha RU 2 sascha RU 3 sasha DE 4 sascha DE genderize(names_cntrs$names, country_id = names_cntrs$countries, simplify = FALSE) name type count gender country_id probability 4 sasha gender NA &lt;NA&gt; &lt;NA&gt; NA 3 sascha gender 39 male RU 0.82 2 sasha gender NA &lt;NA&gt; &lt;NA&gt; NA 1 sascha gender 22624 male DE 0.99 Lastly, you can set the parameter meta to TRUE. This will add additional columns to the result with your rate limit (maximum daily number of requests), the remaining number of requests, the seconds until rate limit reset and the time of the request: genderize(&#39;judy&#39;, simplify = FALSE, meta = TRUE) name type count gender probability api_rate_limit api_rate_remaining api_rate_reset api_request_timestamp 1 judy gender 225977 female 1 100 80 NA 2024-06-21 16:08:17 6.4.3 Provide API key If you bought an API key, you can provide it using the apikey parameter. It’s however recommended to use the save_api_key() function to safely store such an API key. It will then automatically be used for each request. Please be careful when dealing with API keys and never publish them. 6.4.4 Functions for access to other APIs The package also provides access to the APIs for predicting age (Agify.io) and nationality (Nationalize.io) from a name. Examples on how to do that are given on the package’s website. However, such predictions are even more problematic than the gender predictions and should never be trusted. Even the examples given on the API’s respective websites showcase foolish predictions: A “Michael” is predicted as being 70 years old, living either in the US (9% probability), in Australia (6%) or New Zealand (5%). Pinpointing an age using only a given name is nonsense and the country predictions simply won’t help you much for many names, given how many names are internationally used. 6.5 Social science examples Are there social science research examples using the API? There seem to be several bibliometric studies that focus on the gender publication gap, which use the genderize.io API to estimate the gender of journal paper authors. Two notable examples are Holman, Stuart-Fox, and Hauser (2018) and Shen et al. (2018). Hipp and Konrad (2021) used the genderize.io API to predict the gender from names of GitHub users (for those that provided a valid given name). This was then used to analyze the different impact of the COVID-19 pandemic on the productivity of female and male software developers. Other examples (also outside academia) are listed on the “Use cases” page. References Blevins, Cameron, and Lincoln Mullen. 2015. “Jane, John... Leslie? A Historical Method for Algorithmic Gender Prediction.” DHQ: Digital Humanities Quarterly 9 (3). Hipp, Lena, and Markus Konrad. 2021. “Has Covid-19 Increased Gender Inequalities in Professional Advancement? Cross-Country Evidence on Productivity Differences Between Male and Female Software Developers.” Journal of Family Research, September. https://doi.org/10.20377/jfr-697. Holman, Luke, Devi Stuart-Fox, and Cindy E. Hauser. 2018. “The Gender Gap in Science: How Long Until Women Are Equally Represented?” PLOS Biology 16 (4): e2004956. https://doi.org/10.1371/journal.pbio.2004956. Shen, Yiqin Alicia, Jason M. Webster, Yuichi Shoda, and Ione Fine. 2018. “Persistent Underrepresentation of Women’s Science in High Profile Journals.” https://doi.org/10.1101/275362. Wais, Kamil. 2016. “Gender Prediction Methods Based on First Names with genderizeR.” The R Journal 8 (1): 17. https://doi.org/10.32614/RJ-2016-002. The author of this chapter is in no way affiliated with Demografix ApS.↩︎ Experiments showed that the API is not case-sensitive, i.e. it doesn’t matter if you query the name “sasha”, “Sasha” or “SASHA”.↩︎ At time of writing, the package was no longer maintained and not available on CRAN anymore.↩︎ "],["github.com-api.html", "Chapter 7 GitHub.com API 7.1 Provided services/data 7.2 Prerequisites 7.3 Simple API call 7.4 API access in R 7.5 Social science examples", " Chapter 7 GitHub.com API Markus Konrad (WZB) You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;jsonlite&#39;, &#39;httr&#39;) 7.1 Provided services/data What data/service is provided by the API? The GitHub.com API provides a service for interacting with the social coding platform GitHub. A social coding platform is a website that allows users to work on software projects collaboratively. Users can share their work, engage in discussions and track activities of other users and projects. GitHub is currently the largest such platform with more than 50M user accounts (as of January 2022). GitHub’s API allows for retrieving user-generated data from its platform, which is probably of main interest for social scientists. It also provides tools for controlling your account, organizations and projects on the platform in order to automate workflows. This is more of interest for professional software development. From a social scientist’s perspective, the API may be of interest when studying online communities, working methods, organizational structures, communication and discussions, etc. with a focus on (open-source) software development. Many projects that are hosted on GitHub are open-source projects with a transparent development process and communications. For private projects, which can also be hosted on GitHub, there’s understandably only a few aggregate data available. When collecting data on GitHub, you should follow GitHub’s terms of service, especially the API terms. GitHub users agree that, when using the service, anything they post publicly may be viewed and used by other users (see ToS section D5). Still, you should be cautious when collecting and analyzing such user-generated data, as the user was not informed about contributing to a research project. If research data should later be published for replication, only aggregated and/or anonymized data should be made public. Depending on the data you collect, an ethics review should also be considered. Special care should be taken about sampling techniques and replicability when working with this API. The sheer amount of data shared on GitHub requires sampling in most cases, but this is often hard to implement with the API, since results are usually sorted in a way that introduces bias when you can only fetch a certain number of top results (e.g. searching for users gives you the “most popular” users matching your criteria). Cosentino, Izquierdo, and Cabot (2016) show that many studies that use the GitHub API fail to implement proper sampling. There are several competing platforms to GitHub. Many of them also provide an API which allow you to retrieve data. To list a few: BitBucket – API documentation SourceForge – API documentation Launchpad – API documentation 7.2 Prerequisites The API can be used for free and you can send up to 60 requests per hour if you’re not authenticated (i.e. if you don’t provide an API key). For serious data collection, this is not much, so it is recommended to sign up on GitHub and generate a personal access token that acts as API key. This token can then be used to authenticate your API requests. Your quota is then 5000 requests per hour. 7.3 Simple API call What does a simple API call look like? First of all, it’s important to note that GitHub actually provides two APIs: A REST API and a GraphQL API. They differ mainly in how requests are performed and how the responses are formatted, but also in what can be requested. The REST API is the default interface and provides a way of access akin to most other APIs in this book. The GraphQL API provides a quite different interface which requires you to more specifically describe what kind of information you want to retrieve. Though some very detailed information can only be retrieved via the GraphQL API, we will stick to the REST API for this chapter due to the complexity of the GraphQL API. We can perform a request using the curl command in a terminal or by simply visiting the URL in a browser. We will start by retrieving public profile data from the GitHub account of software developer and hacker Lilith Wittmann:6 curl https://api.github.com/users/LilithWittmann The result is an HTTP response with JSON formatted data which contains user profile information that can also be found on the respective public profile page: { &quot;login&quot;: &quot;LilithWittmann&quot;, &quot;name&quot;: &quot;Lilith Wittmann&quot;, &quot;location&quot;: &quot;Berlin, Germany&quot;, &quot;bio&quot;: &quot;freelance developer &amp; consultant&quot;, &quot;twitter_username&quot;: &quot;LilithWittmann&quot;, &quot;public_repos&quot;: 37, &quot;public_gists&quot;: 6, &quot;followers&quot;: 440, &quot;following&quot;: 52, // [ more data ... ] } The GitHub API offers extensive search capabilities. You can search for users, repositories, discussions and more by constructing a search query. Here, we search for users that use R and report being located in Berlin: curl &#39;https://api.github.com/search/users?q=language:r+location:berlin&#39; { &quot;total_count&quot;: 541, &quot;incomplete_results&quot;: false, &quot;items&quot;: [ { &quot;login&quot;: &quot;IndrajeetPatil&quot;, &quot;id&quot;: 11330453, // [...] }, { &quot;login&quot;: &quot;christophergandrud&quot;, &quot;id&quot;: 1285805, // [...] }, { &quot;login&quot;: &quot;RobertTLange&quot;, &quot;id&quot;: 20374662, // [...] }, // [ more data ... ] } We could then continue fetching profile information for each account in the result set. Note that by default, the results are ordered by “best match”. This can be changed with an additional “sort” parameter. The GitHub API doesn’t provide a way of sampling search results – they are always sorted in some way and hence may introduce bias when you only fetch a limited number of results (which is often necessary since the result set is too large). You may consider to narrow the criteria for your search requests, e.g. by sampling geographical locations, so that you can always obtain the whole search results for a place. Another example would be retrieving data about a project repository. We will use the GitHub repository for the popular R package dplyr as an example: curl https://api.github.com/repos/tidyverse/dplyr { &quot;id&quot;: 6427813, &quot;name&quot;: &quot;dplyr&quot;, &quot;full_name&quot;: &quot;tidyverse/dplyr&quot;, &quot;description&quot;: &quot;dplyr: A grammar of data manipulation&quot;, &quot;homepage&quot;: &quot;https://dplyr.tidyverse.org&quot;, &quot;size&quot;: 50767, &quot;stargazers_count&quot;: 3959, &quot;watchers_count&quot;: 3959, &quot;language&quot;: &quot;R&quot;, // [ more data ... ] } Finally, let’s fetch data about an issue in the dplyr repository. On GitHub, issues are code or documentation problems as well as development tasks that users and collaborators bring up and discuss. Here, we collect all data on dplyr issue #5958: curl https://api.github.com/repos/tidyverse/dplyr/issues/5958 { &quot;url&quot;: &quot;https://api.github.com/repos/tidyverse/dplyr/issues/5958&quot;, &quot;number&quot;: 5958, &quot;title&quot;: &quot;Inaccurate documentation for `name` argument of `count()`&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;sfirke&quot;, &quot;id&quot;: 7569808, // [ more data ... ] }, &quot;comments&quot;: 13, &quot;body&quot;: &quot;The documentation for `count()` says of the [...]&quot;, // truncated // [ more data ... ] } As you can see in the example above, a response may contain nested data (the “user” entry shows information about the author of the issue as a nested structure). 7.4 API access in R How can we access the API from R? There are packages for many programming languages that provide convenient access for communicating with the GitHub API. Unfortunately, there are no such packages for R at the time of writing.7 This means we can only access the API directly, e.g. by using the jsonlite package to fetch the data and convert it to an R list or dataframe. We start with translating the first API call from the previous section to R: library(jsonlite) profile_data &lt;- fromJSON(&#39;https://api.github.com/users/LilithWittmann&#39;) # this gives a list head(profile_data, 3) ## $login ## [1] &quot;LilithWittmann&quot; ## ## $id ## [1] 891030 ## ## $node_id ## [1] &quot;MDQ6VXNlcjg5MTAzMA==&quot; The JSON response from the GitHub API was directly converted to an R list object. When a JSON result only consists of arrays, fromJSON() automatically converts the result to a dataframe by default. We can observe that when fetching the repositories of an user: profile_repos &lt;- fromJSON(&#39;https://api.github.com/users/LilithWittmann/repos&#39;) # this gives a dataframe head(profile_repos[,1:3]) # selecting only the first 3 columns ## id node_id name ## 1 42730077 MDEwOlJlcG9zaXRvcnk0MjczMDA3Nw== airflow ## 2 481880572 R_kgDOHLjp_A ant-design ## 3 410145768 R_kgDOGHJT6A aries-staticagent-python ## 4 319779724 MDEwOlJlcG9zaXRvcnkzMTk3Nzk3MjQ= awesome-behoerden-floss ## 5 817707951 R_kgDOML07rw bund-id-documentation-backup ## 6 811975324 R_kgDOMGXCnA bundid-scam Let’s also repeat the search query from the previous section. We want to obtain accounts that use R and entered “Berlin” in their profile’s location field. This time, the result is a list that reports the number of search results as search_results$total_count and the search results details as a dataframe in search_results$items: search_results &lt;- fromJSON(&#39;https://api.github.com/search/users?q=language:r+location:berlin&#39;) # this gives a list with a dataframe in &quot;items&quot; str(search_results, list.len = 3, vec.len = 3) ## List of 3 ## $ total_count : int 694 ## $ incomplete_results: logi FALSE ## $ items :&#39;data.frame&#39;: 30 obs. of 19 variables: ## ..$ login : chr [1:30] &quot;RobertTLange&quot; &quot;christophergandrud&quot; &quot;lisacharlotterost&quot; ... ## ..$ id : int [1:30] 20374662 1285805 9379282 1569647 638225 18618077 1563902 42495969 ... ## ..$ node_id : chr [1:30] &quot;MDQ6VXNlcjIwMzc0NjYy&quot; &quot;MDQ6VXNlcjEyODU4MDU=&quot; &quot;MDQ6VXNlcjkzNzkyODI=&quot; ... ## .. [list output truncated] Let’s suppose we want to collect profile data for each account in the result set. First, let’s investigate the search results in search_results$items: head(search_results$items[,1:3]) ## login id node_id ## 1 RobertTLange 20374662 MDQ6VXNlcjIwMzc0NjYy ## 2 christophergandrud 1285805 MDQ6VXNlcjEyODU4MDU= ## 3 lisacharlotterost 9379282 MDQ6VXNlcjkzNzkyODI= ## 4 dirkschumacher 1569647 MDQ6VXNlcjE1Njk2NDc= ## 5 al2na 638225 MDQ6VXNlcjYzODIyNQ== ## 6 kozodoi 18618077 MDQ6VXNlcjE4NjE4MDc3 To retrieve the profile details of each user, we only need the account name which we can then feed into the https://api.github.com/users/&lt;account&gt; API query. For demonstration purposes, let’s only select the first ten users in the result set: r_users_berlin &lt;- search_results$items$login[1:10] # gives a character vector of 10 account names head(r_users_berlin) ## [1] &quot;RobertTLange&quot; &quot;christophergandrud&quot; &quot;lisacharlotterost&quot; &quot;dirkschumacher&quot; &quot;al2na&quot; &quot;kozodoi&quot; We can now build the API queries to fetch the profile information for each user: users_query &lt;- paste0(&#39;https://api.github.com/users/&#39;, r_users_berlin) # gives a character vector of 10 API queries head(users_query) ## [1] &quot;https://api.github.com/users/RobertTLange&quot; &quot;https://api.github.com/users/christophergandrud&quot; &quot;https://api.github.com/users/lisacharlotterost&quot; &quot;https://api.github.com/users/dirkschumacher&quot; &quot;https://api.github.com/users/al2na&quot; ## [6] &quot;https://api.github.com/users/kozodoi&quot; The fromJSON() function only accepts a single character string as query, but we have a vector of ten queries for the ten users. Hence, we resort to lapply() which passes each query to fromJSON() and stores the result as a list of ten lists (i.e. a nested structure – a list of lists). Each item in users_details is then a list that contains the profile information for the respective user. users_details &lt;- lapply(users_query, fromJSON) # gives a list of 10 lists (one for each user) str(users_details, list.len = 3) ## List of 10 ## $ :List of 32 ## ..$ login : chr &quot;RobertTLange&quot; ## ..$ id : int 20374662 ## ..$ node_id : chr &quot;MDQ6VXNlcjIwMzc0NjYy&quot; ## .. [list output truncated] ## $ :List of 32 ## ..$ login : chr &quot;christophergandrud&quot; ## ..$ id : int 1285805 ## ..$ node_id : chr &quot;MDQ6VXNlcjEyODU4MDU=&quot; ## .. [list output truncated] ## $ :List of 32 ## ..$ login : chr &quot;lisacharlotterost&quot; ## ..$ id : int 9379282 ## ..$ node_id : chr &quot;MDQ6VXNlcjkzNzkyODI=&quot; ## .. [list output truncated] ## [list output truncated] A nested list is hard to work with, so we convert the result to a dataframe. On the way, we select only the information that we actually want to work with. For demonstration purposes, we only select the account name, the real name, the location, the number of public repositories and the number of followers of each user. users_rows &lt;- lapply(users_details, function(d) { # select information that we need and generate a dataframe with a single row data.frame(login = d$login, name = d$name, loc = d$location, repos = d$public_repos, followers = d$followers, stringsAsFactors = FALSE) }) # combine all dataframe rows to form the final dataframe users_df &lt;- do.call(rbind, users_rows) # gives a dataframe with login, name, location, number of repos, number of followers users_df ## login name loc repos followers ## 1 RobertTLange Robert Tjarko Lange Tokyo, Berlin, Barcelona, London 36 630 ## 2 christophergandrud Christopher Gandrud Berlin 147 443 ## 3 lisacharlotterost Lisa Charlotte Muth Berlin, Germany 11 368 ## 4 dirkschumacher Dirk Schumacher Berlin 157 329 ## 5 al2na Altuna Akalin Berlin 73 266 ## 6 kozodoi Nikita Kozodoi Berlin, Germany 26 166 ## 7 tholor Malte Pietsch Berlin, Germany 19 156 ## 8 MathiasHarrer Mathias Harrer Munich &amp; Berlin 30 153 ## 9 joachim-gassen Joachim Gassen Berlin, Germany 34 150 ## 10 saraswatmks Manish Saraswat Berlin, Germany 62 149 As we can see, working with the results from the API requires some effort in R, since we have to deal with the often complex structure of the provided data. There’s no “convenience wrapper” package for the GitHub API in R so far. If you’re familiar with other programming languages, you may consider using one of the recommended packages.8 7.4.1 Provide API key Authenticating with the GitHub API via an API key allows you to send much more requests to the API (5000 requests per hour instead of 60 at the time of writing). API access keys for the GitHub API are called personal access tokens (PAT) and the documentation explains how to generate a PAT once you’ve logged into your GitHub account. Please be careful with your PATs and never publish them. When you want to use authenticated requests, you need to pass along authentication information (your GitHub user account and your PAT) with every request. This is not possible with fromJSON() – you have to send HTTP requests directly, e.g. via GET() from the httr package and then need to handle the HTTP response. The following example shows this. Here, we have the secret access token PAT stored as environment variable and fetch it via Sys.getenv(). Next, we use GET() to make an authenticated HTTP response to the GitHub API. We use the /user API endpoint, which simply reports information about the currently authenticated user (which is your own account). If we were to use this API endpoint in an unauthenticated manner, we’d receive the error message “Requires authentication”.9 We then need to get the content of the response as text (content(response, as = 'text')) and pass this to fromJSON() in order to get the result as list object. library(httr) PAT &lt;- Sys.getenv(&quot;GitHub_token&quot;) response &lt;- GET(&#39;https://api.github.com/user&#39;, authenticate(&#39;&lt;account name&gt;&#39;, PAT)) account_details &lt;- fromJSON(httr::content(response, as = &#39;text&#39;)) account_details You can use that code the same way as for the other API requests, only that when you use authenticated requests the request quotas are much higher. 7.5 Social science examples Are there social science research examples using the API? When the GitHub API is used in a research context, this is mainly done in the fields of computer science, (IT) business economics and social media studies. Lima, Rossi, and Musolesi (2014) analyze social ties, collaboration patterns and the geographical distribution of users on the platform. Chatziasimidis and Stamelos (2015) try to find “success rules” for software projects by analyzing GitHub data and relating it to software download numbers. Importantly, Cosentino, Izquierdo, and Cabot (2016) provide a meta-analysis of 93 research papers and find “concerns regarding the dataset collection process and size, the low level of replicability, poor sampling techniques, lack of longitudinal studies and scarce variety of methodologies.” Hipp and Konrad (2021) used the GitHub API to collect data about open-source software development contributions before and during the COVID-19 pandemic. This was then used to analyze the different impact of the pandemic on the productivity of female and male software developers. They tackled the user sampling issue by employing geographic sampling and narrowing search results so that always the full result set of a user search for a place could be obtained. References Chatziasimidis, Fragkiskos, and Ioannis Stamelos. 2015. “Data Collection and Analysis of GitHub Repositories and Users.” In 2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA), 1–6. https://doi.org/10.1109/IISA.2015.7388026. Cosentino, Valerio, Javier Luis Cánovas Izquierdo, and Jordi Cabot. 2016. “Findings from GitHub: Methods, Datasets and Limitations.” In 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR), 137–41. Hipp, Lena, and Markus Konrad. 2021. “Has Covid-19 Increased Gender Inequalities in Professional Advancement? Cross-Country Evidence on Productivity Differences Between Male and Female Software Developers.” Journal of Family Research, September. https://doi.org/10.20377/jfr-697. Lima, Antonio, Luca Rossi, and Mirco Musolesi. 2014. “Coding Together at Scale: GitHub as a Collaborative Social Network.” In Eighth International AAAI Conference on Weblogs and Social Media. The author asked for permission for fetching and displaying this data.↩︎ Please note that the git2r package does not provide access to the GitHub API. It provides access to git repositories which is something completely different to accessing the web API of a social coding platform.↩︎ For example, the PyGithub package for Python provides a comprehensive set of tools for interacting with the GitHub API which usually requires much less programming effort than with R and jsonlite.↩︎ Try that out yourself via curl https://api.github.com/user in the terminal.↩︎ "],["google-news-api.html", "Chapter 8 Google News API 8.1 Prerequisites 8.2 Simple API call 8.3 API access in R 8.4 Social science examples", " Chapter 8 Google News API Bernhard Clemm von Hohenberg With the News API (formerly Google News API), you can get article snippets and news headlines, both up to four years old and real-time, from over 80,000 news sources worldwide. You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;dplyr&#39;, &#39;httr&#39;) 8.1 Prerequisites What are the prerequisites to access the API (authentication)? You need an API key, which can be requested via https://newsapi.org/register. One big drawback of the News API is that the free version (“Developer”) does not get you very far. Some serious limitations are that you can only get articles that are up to a month old; that it is restricted to 100 requests per day; that the article content is truncated to the first 200 characters. In addition, the Developer key expires after a while even if you stick to those limits, although it is easy to sign up for a new Developer account (gmail address or else). The “Business” version costs $449 per month and allows searching for articles to up to 4 years old as well as 250,000 requests per month. More details on pricing can be found here. Up until at least 2019, the Business version also allowed you to get the entire news article. The documentation is not clear whether this is still the case. 8.2 Simple API call What does a simple API call look like? The documentation of the API is available here. A couple of good examples can be found on the landing page of the API. For instance, we could get all articles mentioning Biden since four weeks ago, sorted by recency, with the following call: library(httr) endpoint_url &lt;- &quot;https://newsapi.org/v2/everything?&quot; my_query &lt;- &quot;biden&quot; my_start_date &lt;- Sys.Date() - 28 my_api_key &lt;- Sys.getenv(&quot;GoogleNews_token&quot;) # &lt;YOUR_API_KEY&gt; params &lt;- list( &quot;q&quot; = my_query, &quot;from&quot; = my_start_date, &quot;language&quot; = &quot;en&quot;, &quot;sortBy&quot; = &quot;publishedAt&quot;) news &lt;- httr::GET(url = endpoint_url, httr::add_headers(Authorization = my_api_key), query = params) httr::content(news) # the resulting articles[[1]]$content shows that the article content is truncated 8.3 API access in R How can we access the API from R (httr + other packages)? To date, there is no R package facilitating access, but the API structure is simple enough to rely on httr. The API has three main endpoints: https://newsapi.org/v2/everything?, documented at https://newsapi.org/docs/endpoints/everything https://newsapi.org/v2/top-headlines/sources?, documented at https://newsapi.org/docs/endpoints/sources https://newsapi.org/v2/top-headlines?, documented at https://newsapi.org/docs/endpoints/top-headlines We have already explored the everything endpoint. Additional parameters to use are, for example, searchIn (specifying whether you want to search in the title, the description or the main text), to (specifying until what date to search) or pageSize (how many results to return per page). Though perhaps not so interesting from a research perspective, the sources endpoint is useful because it allows to explore the list of sources in each country (not really documented anywhere). Let’s get all sources from Germany—we can see that there are ten from which the News API draws content: library(dplyr) library(httr) endpoint_url &lt;- &quot;https://newsapi.org/v2/top-headlines/sources?&quot; my_country &lt;- &quot;de&quot; my_api_key &lt;- Sys.getenv(&quot;GoogleNews_token&quot;) # &lt;YOUR_API_KEY&gt; params &lt;- list(&quot;country&quot; = my_country) sources &lt;- httr::GET(url = endpoint_url, httr::add_headers(Authorization = my_api_key), query = params) sources_df &lt;- bind_rows(httr::content(sources)$sources) sources_df[,c(&quot;id&quot;, &quot;name&quot;, &quot;url&quot;, &quot;category&quot;)] # A tibble: 10 × 4 id name url category &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 bild Bild http://www.bild.de general 2 der-tagesspiegel Der Tagesspiegel http://www.tagesspiegel.de general 3 die-zeit Die Zeit http://www.zeit.de/index business 4 focus Focus http://www.focus.de general 5 gruenderszene Gruenderszene http://www.gruenderszene.de technology 6 handelsblatt Handelsblatt http://www.handelsblatt.com business 7 spiegel-online Spiegel Online http://www.spiegel.de general 8 t3n T3n https://t3n.de technology 9 wired-de Wired.de https://www.wired.de technology 10 wirtschafts-woche Wirtschafts Woche http://www.wiwo.de business This illustrates another weakness of the News API: The selection of sources is not neither comprehensive nor transparent. In any case, let’s use this information to try out the headlines endpoint, getting breaking headlines from Bild (via its id), with 5 results per page. Note that in the Developer version, these headlines are not really breaking, but actually from one hour ago. endpoint_url &lt;- &quot;https://newsapi.org/v2/top-headlines?&quot; my_source &lt;- &quot;bild&quot; my_api_key &lt;- Sys.getenv(&quot;GoogleNews_token&quot;) # &lt;YOUR_API_KEY&gt; params &lt;- list( &quot;sources&quot; = my_source, &quot;pageSize&quot; = 5) headlines &lt;- httr::GET(url = endpoint_url, httr::add_headers(Authorization = my_api_key), query = params) headlines_df &lt;- bind_rows(httr::content(headlines)$articles) %&gt;% mutate(source = tolower(source)) %&gt;% unique() headlines_df[,c(&quot;publishedAt&quot;,&quot;title&quot;)] # A tibble: 5 × 2 publishedAt title &lt;chr&gt; &lt;chr&gt; 1 2022-02-16T15:52:26.3782532Z Köln: Erweiterte Anklage gegen Pfarrer Hans U. – 85 weitere Missbrauchsfälle! 2 2022-02-16T15:46:52Z Hai-Attacke in Australien: Schwimmer vor der Küste Sydneys zerfetzt 3 2022-02-16T15:37:24.2222797Z Essen: Polizei befreit Hundewelpen aus Kofferraum 4 2022-02-16T15:20:45Z BVB – Erling Haaland: Idol verrät brisante Interna vom Adidas-Geheimtreffen 5 2022-02-16T15:07:32Z Texas, USA: Raubopfer ballert um sich, um Dieb aufzuhalten – Mädchen (9) tot 8.4 Social science examples Are there social science research examples using the API? A search on Google Scholar (queries “Google News API” and “News API”) reveals that surprisingly few social-science studies make use of the News API, although many rely the web site of Google News for research (e.g., Haim, Graefe, and Brosius (2018)). One example from the social sciences is Chrisinger et al. (2021), who ask how the discourse on food stamps in the United States has changed over time. Through the News API, they collected 13,987 newspaper articles using keyword queries, and ran a structural topic model. In one of my papers, I ask whether US conservatives and liberals differ in their ability to discern true from false information, and in their tendency to give more credit to information that is ideologically congruent. As I argue that these questions can best be answered if the news items used in a survey represent the universe of news well, the News API helps me get a decent approximation of this universe. Note, however, that at the time I was still able to get complete articles through the Business version of the API, and it is unclear whether this is still the case (Clemm von Hohenberg 2022)10). References Chrisinger, Benjamin W., Eliza W. Kinsey, Ellie Pavlick, and Chris Callison-Burch. 2021. “SNAP Judgments into the Digital Age: Reporting on Food Stamps Varies Significantly with Time, Publication Type, and Political Leaning.” Commun. Methods Meas., November, 1–18. Clemm von Hohenberg, Bernhard. 2022. “Truth and Bias, Left and Right.” Haim, Mario, Andreas Graefe, and Hans-Bernd Brosius. 2018. “Burst of the Filter Bubble?” Digital Journalism 6 (3): 330–43. https://doi.org/10.1080/21670811.2017.1338145. https://osf.io/pqct8/↩︎ "],["google-natural-language-api.html", "Chapter 9 Google Natural Language API 9.1 Provided services/data 9.2 Prerequisites 9.3 Simple API call 9.4 API access in R 9.5 Social science examples", " Chapter 9 Google Natural Language API Paul C. Bauer, Camille Landesvatter, Malte Söhren You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;googleLanguageR&#39;, &#39;tidyverse&#39;, &#39;tm&#39;, &#39;ggwordcloud&#39;) 9.1 Provided services/data What data/service is provided by the API? The API is provided by Google. Google Cloud offers two Natural Language Products: AutoML Natural Language and Natural Language API. See here to read about which of the two products is the one more useful to you. In short, option 1, the Auto Machine Learning (ML) Natural Language allows you to train a new, custom model to either classify text, extract entities or detect sentiment. For instance, you could provide an already pre-labeled subset of your data which the API will then use to train a custom classifier. With this classifier at hand you could then classify and analyze further similar data of yours. This API review focuses on option 2, the Natural Language API. This API uses pre-trained models to analyze your data. Put differently, instead of providing only a pre-labeled subset of your data, here you normally provide the API with your complete (unlabeled) data which it will then analyze. The following requests are available: Analyzing Sentiment (analyzeSentiment) Analyzing Entities (analyzeEntities) Analyzing Syntax (analyzeSyntax) Analyzing Entity Sentiment (analyzeEntitySentiment) Classifying Content (classifyText) A demo of the API that allows you to input text and explore the different classification capabilities can be found here. 9.2 Prerequisites What are the prerequisites to access the API (authentication)? The prerequisite to access Google Natural Language API is a Google Cloud Project. To create this you will need a Google account to log into the Google Cloud Platform (GCP). Within your Google Cloud Platform, you must enable the Natural Language API for your respective Google Cloud Project here. Additionally, if you are planning to request the Natural Language API from outside a Google Cloud environment (e.g., R) you will be required to use a private (service account) key. This can be achieved by creating a service account which in turn will allow you to download your private key as a JSON file. To create your API key for authentication from within the GCP, go to APIs &amp; Services &gt; Credentials. Below we provide an example of how to authenticate from within the Google Cloud Platform (Cloud Shell + API key) and how to authenticate from within R (authentication via JSON key file). 9.3 Simple API call What does a simple API call look like? Here we describe how a simple API call can be made from within the Google Cloud Platform environment via the Google Cloud Shell: To activate your Cloud Shell, inspect the upper right-hand corner of your Google Cloud Platform Console and click the icon called “Activate Shell”. Google Cloud Shell is a command line environment running in the cloud. Via the Cloud Shell command line, add your individual API key to the environment variables, so it is not required to be called for each request. export API_KEY=&lt;YOUR_API_KEY&gt; Via the built-in Editor in Cloud Shell create a JSON file (call it for instance ‘request.json’) with the text that you would like to perform analysis on. Consider that text can be uploaded in the request (shown below) or integrated with Cloud Storage. Supported types of your text are PLAIN_TEXT (shown below) or HTML. { &quot;document&quot;:{ &quot;type&quot;:&quot;PLAIN_TEXT&quot;, &quot;content&quot;:&quot;Enjoy your vacation!&quot; }, &quot;encodingType&quot;: &quot;UTF8&quot; } For sending your data, pass a curl command to your Cloud Shell command line where you refer (via @) to your request.json file from the previous step. curl &quot;https://language.googleapis.com/v1/documents:analyzeEntities?key=${API_KEY}&quot; -s -X POST -H &quot;Content-Type: application/json&quot; --data-binary @request.json Depending on to which endpoint you send the request (here: analyzeEntities) you will receive your response with many different insights into your text data. 9.4 API access in R How can we access the API from R (httr + other packages)? The input (i.e., text data) one provides to the API most often will go beyond a single word or sentence. The most convenient way which also produces the most insightful and structured results (on which you can directly perform further analysis on) are achieved when using the ‘googleLanguageR’ R package - a package which among other options (there are other examples in this review) allows calling the Natural Language API: In this small example we demonstrate how to.. .. authenticate with your Google Cloud Account within R .. how to analyze the syntax of exemplary twitter data (we are using twitter data from two popular german politicians, which we (via the Google Translation API) beforehand also translated to english) .. how to extract terms that are nouns only .. plot your nouns in a word cloud Step 1: Load package library(googleLanguageR) library(tidyverse) library(tm)#stopwords Step 2: Authentication gl_auth(&quot;./your-key.json&quot;) Step 3: Analysis Start with loading your text data. For this example, we retrieve data inherit to the quanteda.corpora R package which in a broader sense is associated with the famous quanteda package. The data we choose to download (‘data_corpus_guardian’) contains Guardian newspaper articles in politics, economy, society and international sections from 2012 to 2016. See here for a list of even more publicy available text corpora from the quanteda.corpora package. # Download and store corpus guardian_corpus &lt;- quanteda.corpora::download(&quot;data_corpus_guardian&quot;) # Keep text only from the corpus text &lt;- guardian_corpus[[&quot;documents&quot;]][[&quot;texts&quot;]] # For demonstration purposes, subset the text data to 20 observations only text &lt;- text[1:20] # Turn text into a data frame and add an identifier df &lt;- as.data.frame(text) df &lt;- tibble::rowid_to_column(df, &quot;ID&quot;) Note: Whenever you choose to work with textual data, a very common procedure is to pre-process the data via a set of certain transformations. For instance, you will convert all letters to lower case, remove numbers and punctuation, trim words to their word stem and eventually remove so-called stopwords. There are many tutorials (for example here or here). After having retrieved and prepared our to-be-analyzed (text) data, we can now call the API via the function gl_nlp(). Here you will have to specify the quantity of interest (here: analyzeSyntax). Depending on what specific argument you make use of (e.g., analyzeSyntax, analyzeSentiment, etc.) a list with information on different characteristics of your text is returned, e.g., sentences, tokens, tags of tokens. syntax_analysis &lt;- gl_nlp(df$text, nlp_type = &quot;analyzeSyntax&quot;) Importantly, find the list tokens inherit to the large list syntax_analysis. This list stores two variables: content (contains the token) and tag (contains the tag, e.g., verb or noun). Let’s have a look at the first document. head(syntax_analysis[[&quot;tokens&quot;]][[1]][,1:3]) content beginOffset tag 1 London 0 NOUN 2 masterclass 7 NOUN 3 on 19 ADP 4 climate 22 NOUN 5 change 30 NOUN 6 | 37 PUNCT Now imagine you are interested in all the nouns that were used in the Guardian Articels while removing all other types of words (e.g., adjectives, verbs, etc.). We can simply filter for those using the “tag”-list. # Add tokens from syntax analysis to original dataframe df$tokens &lt;- syntax_analysis[[&quot;tokens&quot;]] # Keep nouns only df &lt;- df %&gt;% dplyr::mutate(nouns = map(tokens, ~ dplyr::filter(., tag == &quot;NOUN&quot;))) Step 4: Visualization Finally, we can also plot our nouns in a wordcloud using the ggwordcloud package. # Load package library(ggwordcloud) # Create the data for the plot data_plot &lt;- df %&gt;% # only keep content variable mutate(nouns = map(nouns, ~ select(., content))) %&gt;% # Write tokens in all rows into a single string unnest(nouns) %&gt;% # unnest tokens # Unnest tokens tidytext::unnest_tokens(output = word, input = content) %&gt;% # generate a wordcloud anti_join(tidytext::stop_words) %&gt;% dplyr::count(word) %&gt;% filter(n &gt; 10) #only plot words that appear more than 10 times # Visualize in a word cloud data_plot %&gt;% ggplot(aes(label = word, size = n)) + geom_text_wordcloud() + scale_size_area(max_size = 10) + theme_minimal() Figure 9.1: Wordcloud of nouns found within guardian articles 9.5 Social science examples Are there social science research examples using the API? Text-as-data has become quite a common approach in the social sciences (see e.g., Grimmer and Stewart (2013) for an overview). For the usage of Google’s Natural Language API we however have the impression that it is relatively unknown in NLP and among social scientists. Hence, we want to emphasize the usefulness and importance the usage of Google’s NLP API could have in many research projects. However, if you are considering making use of it, keep two things in mind: some might interpret using the API as a “blackbox” approach (see Dobbrick et al. (2021) for very recent developments of “glass-box machine learning appraoches” for text analysis) potentially standing in way of transparency and replication (two important criteria of good research?). However it is always possible to perform robustness and sensitivity analysis and to add the version of the API one was using. depending on how large your corpus of text data, Google might charges you some money. However for up to 5,000 units (i.e., terms) the different variants of sentiment and syntax analysis are free. Check this overview by Google to learn about prices for more units here. Generally, also consider that if you are pursuing a CSS-related project in which the GCP Products would come in useful, there is the possibility to achieve Google Cloud Research credits (see here). References Dobbrick, Timo, Julia Jakob, Chung-Hong Chan, and Hartmut Wessler. 2021. “Enhancing Theory-Informed Dictionary Approaches with ‘Glass-Box’ Machine Learning: The Case of Integrative Complexity in Social Media Comments.” Commun. Methods Meas., November, 1–18. Grimmer, Justin, and Brandon M Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Polit. Anal. 21 (3): 267–97. "],["google-places-api.html", "Chapter 10 Google Places API 10.1 Provided services/data 10.2 Prerequisites 10.3 Simple API call 10.4 API access in R 10.5 Social science examples", " Chapter 10 Google Places API Lukas Isermann and Clara Husson You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;googleway&#39;, &#39;ggplot2&#39;, &#39;tidyverse&#39;, &#39;mapsapi&#39;, &#39;stars&#39;) 10.1 Provided services/data What data/service is provided by the API? The following five requests are available: Place Search, Place Details, Place Photos, Place Autocomplete and Query Autocomplete. Place Search returns a list of places along with summary information about each place based on a user’s location (by proximity) or search string. Once you find a place_id from a Place Search, you can request more details about a particular place by doing a Place Details request. A Place Details request returns more detailed information about the indicated place such as its complete address, phone number, user rating and reviews. Place Photos provides access to the millions of place-related photos stored in Google’s Place database. When you get place information using a Place Details request, photo references will be returned for relevant photographic content. Find Place, Nearby Search, and Text Search requests also return a single photo reference per place, when relevant. Place Autocomplete automatically fills in the name and/or address of a place as users type. Query Autocomplete service provides a query prediction for text-based geographic searches, by returning suggested queries as you type. Note: You can display Places API results on a Google Map, or without a map but it is prohibited to use Places API data on a map that is not a Google map. 10.2 Prerequisites What are the prerequisites to access the API (authentication)? The prerequisites to access Google Places API are a Google Cloud project (to create it you need a Google account to log into the Google Cloud Platform) and an API Key. Before creating your API Key, don’t forget to enable Places API! To create your API key, go the APIs &amp; Services &gt; Credentials &gt; API key page. 10.3 Simple API call What does a simple API call look like? The API provides different searches and services that can be accessed via HTTP Urls. These Urls requests all take the same general form and pattern: https://maps.googleapis.com/maps/api/place/service/output?parameters Here, service can take the inputs findplacefromtext for find place requests, nearbysearch to look for nearby places, details for a request of place details and more. output may take the value json or xml, dependent on the requested output format. Furthermore, certain parameters are required for all requests. Most importantly, every request must entail a key parameter, indicating the API key. Second, all search places requests take an input parameter that identifies the search target and an inputtype parameter that identifies the type of input given in the input parameter. For place requests, the inputtype parameter can take the values textquery and phonenumber. Nearby requests take a location parameter setting longitude and latitude of the requested place as well as a radius parameter. Detail request, however, take a mandatory parameter place_id, which indicates the place for which the details are requested. Additionally, different optional parameters can be used. These entail a language parameter, a fields parameter indicating the types of place data to return and more. An examples for an API request for pizza places in Mannheim can look like this: https://maps.googleapis.com/maps/api/place/textsearch/xml?query=pizza&amp;location=49.487459,8.466039&amp;radius=5000&amp;key=YOUR_API_KEY 10.4 API access in R How can we access the API from R (httr + other packages)? Instead of typing the API request into our browser, we can use the httr package’s GET() function to access the API from R. # Option 1: Accessing the API with base &quot;httr&quot; commands library(httr) key &lt;- Sys.getenv(&quot;GooglePlaces_token&quot;) res&lt;-GET(&quot;https://maps.googleapis.com/maps/api/place/textsearch/json?&quot;, query = list( query = &quot;pizza&quot;, location = &quot;49.487459,8.466039&quot;, radius = 5000, key = key )) Alternatively, we can use a wrapper function for R provided by the R-Package googleway. Authentication library(googleway) key &lt;- Sys.getenv(&quot;GooglePlaces_token&quot;) set_key(key) API call Step 1: Load packages library(ggplot2) library(tidyverse) # Option 2: Accessing the API with googleway # Request &#39;Mannheim&#39; to get latitude and longitude information location &lt;- googleway::google_places(&quot;Mannheim&quot;) # Save latitude and longitude information in vector location &lt;- location$results$geometry location &lt;- c(location$location$lat, location$location$lng) # Plot places to google map library(mapsapi) # for this you will also need to activate the &quot;maps static API&quot; r = mapsapi::mp_map(center = (&quot;49.48746,8.466039&quot;), zoom = 14, key = key, quiet = TRUE) library(stars) plot(r) # Google places request with googleway pizza &lt;- google_places(&quot;Pizza&quot;, location = location, radius = 5000, place_type = &quot;food&quot;) # Plot rankings as barplot pizza$results %&gt;% ggplot() + geom_col(aes(x = reorder(name, rating), y = rating)) + geom_text(aes(x = reorder(name, rating), y = rating), label = paste0(pizza$results$user_ratings_total, &quot; \\n ratings&quot;), size = 2) + ylab(&quot;Average Rating&quot;)+ xlab(&quot;&quot;) + ggtitle(&quot;Pizza Places in Mannheim by Rating&quot;) + theme_minimal() + theme( axis.text.x = element_text(angle = 90, size = 8, hjust=0.95,vjust=0.2)) # Plot pizza places to google map #important: in order to display the map correctly, you will also have to enable the Maps JavaScript API on GCP # unfortunately we can not display an intercative card in this document, but check out the below code in your own rmd-file! map&lt;-googleway::google_map(location = location) googleway::add_markers(map, data = pizza$results$geometry$location) 10.5 Social science examples Are there social science research examples using the API? In his study “Using Google places data to analyze changes in mobility during the COVID-19 pandemic”, Konrad (2020) looked at the “popular times” data provided by Google Places to measure the effect of social distancing effort on mobility. References Konrad, Markus. 2020. “Using Google Places Data to Analyze Changes in Mobility During the COVID-19 Pandemic.” https://datascience.blog.wzb.eu/2020/05/11/using-google-places-data-to-analyze-changes-in-mobility-during-the-covid-19-pandemic/. "],["google-speech-to-text-api.html", "Chapter 11 Google Speech-to-Text API 11.1 Provided services/data 11.2 Prerequisites 11.3 Simple API call 11.4 API access in R 11.5 Social science examples", " Chapter 11 Google Speech-to-Text API Camille Landesvatter You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;tidyverse&#39;, &#39;googleLanguageR&#39;) 11.1 Provided services/data What data/service is provided by the API? Google’s Speech-to-Text API allows you to convert audio files to text by applying powerful neural network models. Audio content can be transcribed in real time and of course (and possibly of higher relevance for social science research) from stored files. The API currently recognizes more than 125 languages. It supports multiple audio formats, and audio files can either be transcribed directly (if the content does not exceed 60 seconds) or perform asynchronous requests for audio files longer than 60 seconds. A demo of the API that allows you to record text via your microphone (or to upload an audio file) and explore the transcript can be found here. Also consider that there is a Text-to-Speech API - simply performing operations the other way around - offered by Google. 11.2 Prerequisites What are the prerequisites to access the API (authentication)? To access and to use the API the following steps are necessary: Create a google account (if you do not already have one). With this google account login to the google cloud platform and create a Google Cloud Project. Within this Google Cloud Project enable the Google Speech-to-text API. For authentication you will need to create an API key (which you additionally should restrict to the Translation API). If however, you are planning to request the Natural Language API from outside a Google Cloud environment (e.g., R) you will be required to use a private (service account) key. This can be achieved by creating a service account which in turn will allow you to download your private key as a JSON file (we show an example below). 11.3 Simple API call What does a simple API call look like? Note. For both Google’s Translation API as well as Google’s Natural-Language API, in this review we demonstrate an example for a simple API call via the Google Cloud Shell. In principle (and in a very similar procedure) this can be achieved for the Speech-to-Text API. However, your audio file will need some pre-processing. Audio data (such as our exemplary file in wav-format) is binary data. To make your REST request (via the Google Cloud Shell) however JSON is used. JSON eventually does not support binary data which is why you will have to transform your binary audio file into text using Base64 encoding (also refer to this documentation from the Google Website for more information). If you enter audio data which is not Base64 encoded, the Google Cloud Shell will give you an error 400 stating that Base64 decoding failed for your (wav-)file. Nevertheless, in the box below we will provide the basic structure of the request. To activate your Cloud Shell, inspect the upper right-hand corner of your Google Cloud Platform Console and click the icon called “Activate Shell”. Google Cloud Shell is a command line environment running in the cloud. Via the built-in Editor in Cloud Shell create a JSON file (call it for instance ‘request.json’). You can either upload your audio file directly via the Google Cloud Shell (search for the three-dotted “More” menu in the Shell and select “Upload file”), alternatively audio content can be integrated with Cloud Storage. The wav.file we uploaded for this example is an exemplary wav.file that comes along with the ‘googleLanguageR’ R package. { &quot;audio&quot;: { &quot;content&quot;: &quot;woman1_wb&quot; }, &quot;config&quot;: { &quot;enableAutomaticPunctuation&quot;: true, &quot;encoding&quot;: &quot;LINEAR16&quot;, &quot;languageCode&quot;: &quot;en-US&quot;, &quot;model&quot;: &quot;default&quot; } } For sending your data, pass a curl command to your Cloud Shell command line where you refer (via @) to your request.json file from the previous step. Don’t forget to insert your individual API key (alternatively, you could define it beforehand via a variable in your environment -&gt; see example in the API call for Google’s NLP API later in this document). curl &quot;https://speech.googleapis.com/v1p1beta1/speech:recognize?key=APIKEY&quot; -s -X POST -H &quot;Content-Type: application/json&quot; --data-binary @request.json 11.4 API access in R How can we access the API from R (httr + other packages)? Example using R-Package ‘googleLanguageR’ In this small example we demonstrate how to.. *.. authenticate with your Google Cloud Account within R *.. how to import an exemplary audiofile from the “GoogleLanguageR” package *.. how to transcribe this audio file and calculate a confidence score For the usage of further arguments, also read the gl_speech() documentation and this vignette. 1. Load packages library(tidyverse) library(googleLanguageR) Step 2: Authentication gl_auth(&quot;./your-key.json&quot;) Step 3: Analysis We will now get a sample source file which comes along with the googleLanuageR package. The transcript of this file is: “To administer medicine to animals is frequently a very difficult matter, and yet sometimes it’s necessary to do so” - which according to Edmondson (2017) (one of the authors of the ‘googleLanguageR’ R package) is a fairly difficult sentence for computers to parse. exemplary_audio &lt;- system.file(&quot;woman1_wb.wav&quot;, package = &quot;googleLanguageR&quot;) We can now call the API via the function gl_speech(). Here you will have to specify the quantity of interest, namely the audio_source (this can either be a local file or a Google Cloud Storage URI) as well as the languageCode (language spoken in your audio file). exemplary_audio_analysis &lt;- gl_speech(audio_source=exemplary_audio, languageCode = &quot;en-GB&quot;) The result is a list containing two dataframes: transcript and timings. dimnames(exemplary_audio_analysis$transcript) ## [[1]] ## [1] &quot;1&quot; ## ## [[2]] ## [1] &quot;transcript&quot; &quot;confidence&quot; &quot;languageCode&quot; &quot;channelTag&quot; The timings dataframe stores timestamps telling us when each specific term was recognised. The transcript dataframe importantly provides the transcript as well as a confidence score. We can see that the transcript misses one term (“a”) and indicates its confidence with a score close to 1.0. # Show transcript exemplary_audio_analysis$transcript$transcript ## [1] &quot;to administer medicine to animals is frequently very difficult matter and yet sometimes it&#39;s necessary to do so&quot; # Show confidence exemplary_audio_analysis$transcript$confidence #0.92 ## [1] &quot;0.9151854&quot; 11.5 Social science examples Are there social science research examples using the API? Gavras et al. (2022) published research on how to use voice recording in smartphone surveys, i.e. they asked respondents to record their answer via microphone. For pre-processing and ultimately analyzing this data, they explicitly state to have used Google’s Speech-to-Text API (Gavras et al. 2022, 9). They also refer to another source, namely Proksch, Wratil, and Wäckerle (2019), who found the results of the API to be highly similar (e.g., r&gt;0.9 between Google-transcribed and human-transcribed political speeches in German) to manual transcriptions carried out by humans. Importantly, collecting survey data via audio rather than text format has received increasing attention in the recent past (see e.g., Schober et al. 2015; Revilla and Couper 2021) as it is also part of a larger “text-as-data” movement. References Edmondson, Mark. 2017. “Google Cloud Speech API.” https://mran.microsoft.com/snapshot/2017-09-27/web/packages/googleLanguageR/vignettes/speech.html. Gavras, Konstantin, Jan Karem Höhne, Annelies G Blom, and Harald Schoen. 2022. “Innovating the Collection of Open-Ended Answers: The Linguistic and Content Characteristics of Written and Oral Answers to Political Attitude Questions.” J. R. Stat. Soc. Ser. A Stat. Soc. tba (tba): 1–19. Proksch, Sven-Oliver, Christopher Wratil, and Jens Wäckerle. 2019. “Testing the Validity of Automatic Speech Recognition for Political Text Analysis.” Polit. Anal. 27 (3): 339–59. Revilla, Melanie, and Mick P Couper. 2021. “Improving the Use of Voice Recording in a Smartphone Survey.” Soc. Sci. Comput. Rev. 39 (6): 1159–78. Schober, Michael F, Frederick G Conrad, Christopher Antoun, Patrick Ehlen, Stefanie Fail, Andrew L Hupp, Michael Johnston, Lucas Vickers, H Yanna Yan, and Chan Zhang. 2015. “Precision and Disclosure in Text and Voice Interviews on Smartphones.” PLoS One 10 (6): e0128337. "],["google-translation-api.html", "Chapter 12 Google Translation API 12.1 Provided services/data 12.2 Prerequisites 12.3 Simple API call 12.4 API access in R 12.5 Social science examples", " Chapter 12 Google Translation API Paul C. Bauer, Camille Landesvatter You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;googleLanguageR&#39;) 12.1 Provided services/data What data/service is provided by the API? The API is provided by Google. Google’s Translation API translates texts into more than one hundred languages. Note that the approach via the API is a lot more refined than the free version on Googles’ translation website and of course comes in very useful if text in large scale needs to be translated (possibly with longer and more complex content or syntax). For instance, you can choose to specify a certain model to further improve the translation (Neural Machine Translation vs. Phrase-Based Machine Translation). The API limits in three ways: characters per day, characters per 100 seconds, and API requests per 100 seconds. All can be set in the API manager of your Google Cloud Project. Consider that additionally to the Translation API which we demonstrate in this review, Google provides us with two further APIs for translation: AutoML Translation and the Advanced Translation API (see here for a short comparison). 12.2 Prerequisites What are the prerequisites to access the API (authentication)? To access and to use the API the following steps are necessary: Create a google account (if you do not already have one). Using this google account login to the google cloud platform and create a Google Cloud Project. Within this Google Cloud Project enable the Google Translation API. For authentication you will need to create an API key (which you additionally should restrict to the Translation API). If however, you are planning to request the Natural Language API from outside a Google Cloud environment (e.g., R) you will be required to use a private (service account) key. This can be achieved by creating a service account which in turn will allow you to download your private key as a JSON file (we show an example below). 12.3 Simple API call What does a simple API call look like? Here we describe how a simple API call can be made from within the Google Cloud Platform environment via the Google Cloud Shell: To activate your Cloud Shell, inspect the upper right-hand corner of your Google Cloud Platform Console and click the icon called “Activate Shell”. Google Cloud Shell is a command line environment running in the cloud. Via the built-in Editor in Cloud Shell create a JSON file (call it for instance ‘request.json’) with the text that you would like to perform analysis on. Consider that text can be uploaded in the request (shown below) or integrated with Cloud Storage. Supported types of your text are PLAIN_TEXT (shown below) or HTML. { &quot;q&quot;: [&quot;To administer medicine to animals is frequently a very difficult matter, and yet sometimes it’s necessary to do so&quot;], &quot;target&quot;: &quot;de&quot; } For sending your data, pass a curl command to your Cloud Shell command line where you refer (via @) to your request.json file from the previous step. Don’t forget to insert your individual API key in the curl command (alternatively, you could define it beforehand via adding a global variable to your environment → see example in the API call for Googles’ NLP API earlier in this document). curl &quot;https://translation.googleapis.com/language/translate/v2?key=APIKEY&quot; -s -X POST -H &quot;Content-Type: application/json&quot; --data-binary @request.json 12.4 API access in R How can we access the API from R (httr + other packages)? The following example makes use of the ‘googleLanguageR’ package from R which among other options (e.g., syntax analysis → see Chapter 2 of this review) allows calling the Cloud Translation API. In this small example we demonstrate how to… … authenticate with your Google Cloud Account within R … translate an exemplary sentence Step 1: Load package library(googleLanguageR) Step 2: Authentication gl_auth(&quot;./your-key.json&quot;) Step 3: Analysis - API call and Translation First, we create exemplary data. For demonstration purposes, we here again make use of the sentence from above. Of course, for your own project use a complete vector containing text data from within your data. df_original &lt;- data.frame(text = &quot;To administer medicine to animals is frequently a very difficult matter, and yet sometimes it’s necessary to do so&quot;) df_original$text &lt;- as.character(df_original$text) Next, using this data, call the API via the function ‘gl_translate()’ and importantly specify the target language (german in this example) within the ‘target’ argument. google_translate &lt;- gl_translate(df_original$text, target = &quot;de&quot;) This API call eventually provides results in a dataframe with three columns: translatedText ( → contains the translation), detectedSourceLangauge ( → contains a label for the original language being detected) and text ( → contains the original text). Let’s check the translation. google_translate$translatedText Tieren Medikamente zu verabreichen ist oft eine sehr schwierige Angelegenheit, und doch ist es manchmal notwendig 12.5 Social science examples Are there social science research examples using the API? Searching on literature using Google Translation Services we found a study by Prates, Avelar, and Lamb (2018) (Assessing gender bias in machine translation: a case study with Google Translate). We are not aware of any further research / publications that made explicit usage of Google’s Translation API. However, we assume that the API (or at least Google’s Free Translation Service) is involved in many research projects. Generally, we are convinced that making use of automated translation (as well as converting speech to text ( → example also in this review) eventually in combination with translation) can be of great advantage for all kinds of qualitative or mixed-methods research projects. For instance, automated translation could be very useful for easily and very reliably translating data from qualitative interviews or other (field) experiments or observational data (where data might be collected in a foreign language). Also consider the other way around where data is available in the principal language of the research project but some of the textual data has to be communicated and presented in (for instance) english language. References Prates, Marcelo O R, Pedro H C Avelar, and Luis Lamb. 2018. “Assessing Gender Bias in Machine Translation – a Case Study with Google Translate,” September. https://arxiv.org/abs/1809.02208. "],["googletrends-api.html", "Chapter 13 GoogleTrends API 13.1 Provided services/data 13.2 Prerequisites 13.3 Simple API call 13.4 API access in R 13.5 Social science examples", " Chapter 13 GoogleTrends API Jan Behnert, Dean Lajic You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;gtrendsR&#39;, &#39;ggplot2&#39;, &#39;dplyr&#39;) 13.1 Provided services/data What data/service is provided by the API? The API is provided by Google. With Google Trends, one gets access to a largely unfiltered sample of actual search topics (up to 36h before your search) and a filtered and representative sample for search topics older than 36 hours starting from the year 2004. The data is anonymized, can be obtained from different Google products like “Web search”, “News”, “Images”, “Shopping” and “Youtube,” can be filtered by different categories to get the data for the correct meaning of the word, and is aggregated, which means that the searches of all cities/regions are aggregated to the federal state level, country level or world level. The results you get are a standardized measure of search volume for single search terms, a combination of search terms using operators (see table below), or comparisons (one input in relation to the other inputs) over a selected time period. Google calculates how much search volume in each region a search term or query had, relative to all searches in that region. Using this information, Google assigns a measure of popularity to search terms (scale of 0 - 100), leaving out repeated searches from the same person over a short period of time and searches with apostrophes and other special characters. No quotation marks (e.g. Corona symptoms) You get results for each word in your query Quotation marks (e.g. “Corona symptoms”) You get results for the coherent search phrase Plus sign (e.g. corona +covid) Serves as function of an OR-operator Minus sign (e.g. corona -symptoms) Excludes word after the operator 13.2 Prerequisites What are the prerequisites to access the API (authentication)? It can be used without an API key by anyone for free directly in the internet browser (no sign up needed). 13.3 Simple API call What does a simple API call look like? Just click here. 13.4 API access in R How can we access the API from R (httr + other packages)? Example using “httr” package: library(httr) GET(&quot;https://trends.google.com/trends/explore&quot;, query=list(q = &quot;Covid&quot;,geo = &quot;US&quot;)) but just html-output, we recommend to use the gtrendsR package Example using “gtrendsR” package: # visualizing google searches for the word &quot;corona symptoms&quot; in # Germany and Austria in the period 01/01/2020 - 27/04/2021 library(gtrendsR) library(ggplot2) library(dplyr) data(&quot;countries&quot;) # get abbreviations of all countries to filter data data(&quot;categories&quot;) # get numbers of all categories to filter data # Simple call res &lt;- gtrends(&quot;corona symptome&quot;,geo=c(&quot;DE&quot;, &quot;AT&quot;)) plot(res) Note (1): the use of c() in the keyword argument of the gtrends function allows comparisons of up to 5 searches (separator = comma). Note (2): the use of the pattern ‘“xyz”’ in the keyword argument of the gtrends function corresponds to the inverted commas in the table above, all other punctuation methods in the table above can be used as indicated in the table. #Combination using dplyr and ggplot trend = gtrends(keyword=&quot;corona symptome&quot;, geo=c(&quot;DE&quot;, &quot;AT&quot;), time = &quot;2020-01-01 2021-04-27&quot;, gprop=&quot;web&quot;) trend_df &lt;- trend$interest_over_time trend_df &lt;- trend_df %&gt;% mutate(hits = as.numeric(hits), date = as.Date(date)) %&gt;% replace(is.na(.), 0) ggplot(trend_df, aes(x=date, y=hits, group=geo, col=geo)) + geom_line(size=2) + scale_x_date(date_breaks = &quot;2 months&quot; , date_labels = &quot;%b-%y&quot;) + labs(color= &quot;Countries&quot;) + ggtitle(&quot;Frequencies for the query -corona symptoms- in the period: 01/01/2020 - 27/04/2021&quot;) 13.5 Social science examples Are there social science research examples using the API? Google Trends can be used to predict the outcomes of elections. For example a study by (Prado-Román C (2021)) uses Google Trends data to predict the past four elections in the United States and the past five in Canada, since Google first published its search statistics in 2004. They analysed which candidate had the most Google searches in the months leading up to election day and show, that with the help of this data, all actual winners in all the elections held since 2004 could be predicted. Another example is a study by Mavragani (2019) which uses Google Trends data to predict the results of referendums (Scottish referendum 2014, Greek referendum 2015, British referendum 2016, Hungarian referendum 2016, Italian referendum 2016 and the Turkish referendum 2017). It can be shown that the results from Google Trends data are quite similar to the actual referendum results and in some cases are even more accurate than official polls. It is argued that with the help of Google Trends data revealed preferences instead of users’ stated preferences can be analyzed and this data source could be a helpful source to analyze and predict human behavior (given areas where the Internet is widely accessible and not restricted). Furthermore, Google Trends data can also be utilized in other fields, for example to examine whether COVID-19 and the associated lockdowns initiated in Europe and America led to changes in well-being related topic search-terms. The study by Brodeur (2021) finds an increase in queries addressing boredom, loneliness, worry and sadness, and a decrease for search terms like stress, suicide and divorce. Indicating that the people’s mental health could have been strongly affected by the pandemic and the lockdowns. References Brodeur, Clark, A. 2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google Trends.” Journal of Public Economics. Mavragani, Tsagarakis, A. 2019. “Predicting Referendum Results in the Big Data Era.” J Big Data. Prado-Román C, Orden-Cruz C., Gómez-Martínez R. 2021. “Google Trends as a Predictor of Presidential Elections: The United States Versus Canada.” American Behavioral Scientist. "],["instagram-basic-display-api.html", "Chapter 14 Instagram Basic Display API 14.1 Provided services/data 14.2 Prerequisites 14.3 Simple API call 14.4 API access in R", " Chapter 14 Instagram Basic Display API Madleen Meier-Barthold Initially released in 2010, Instagram currently counts 1+ billion monthly active users with 50+ billion photos stored (Tankovska (n.d.)). User engagement is high, with an average of 28 minutes per day spent on the platform in 2020 (Aslam (2021)). Needless to say, the photo and video sharing social networking service holds invaluable data that could be leveraged by social researchers. You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;) 14.1 Provided services/data What data/service is provided by the API? Instagram offers two types of APIs that allow an application to access data on the platform: the Instagram Graph API and the Instagram Basic Display API. Previously, other APIs were available which allowed developers and researchers a less restricted access to data collection (Instagram (2021)). These older APIs are now depreciated. The Instagram Basic Display API gives read-access to basic profile information, photos and videos on authenticated users’ accounts (Facebook for Developers (2021)). Particularly, it is possible to fetch a user’s profile, including fields like account type and account name, as well as a user’s media (images, videos and albums), including fields like media type, caption, URL and timestamp. The API does not allow to modify data like publishing media or moderating comments (see Instagram Graph API). It is a RESTful API, meaning that queries are made for static information at the current moment. Queries are subject to rate limits. Responses are in the form of JSON-formatted objects containing the default and requested fields and edges. 14.2 Prerequisites What are the prerequisites to access the API (authentication)? In order to access the Instagram Basic Display API, developers are required to first register as a Facebook developer on developers.facebook.com, to further create a Facebook App here and to submit the application for review. Another prerequisite to access the API is to get authentication. API authentication is handled through Instagram User Access Tokens that conform to the OAuth 2.0 protocol. The process of getting an access token includes two parts. First, each application user must grant an application permission to read the user node and/or the media node. These permissions are controlled via an Authorization Window. https://api.instagram.com/oauth/authorize ?client_id={appId} &amp;redirect_uri={redirectURI} &amp;scope=user_profile,user_media &amp;response_type=code Give this URL to the application users. The next steps have to be completed by each user. Open a new browser window and load the Authorization Window URL. Authenticate your Instagram test user by signing into the Authorization Window Click Authorize to grant your app access to your profile data. Upon success, the page will redirect you to the redirect URI you included in the previous step and append an Authorization Code. For example: https://mmeierba.github.io/?code=AQD…#_ Copy the code except for the #_ portion in the end. Send this Authorization Code to researcher. When a user successfully grants the application permission to access their data, the user is redirected to a redirect URI which appended with an Authorization Code. Second, the Authorization Code can be exchanged for a short-lived access token (i.e., valid for 1 hour). Then, the API can be queried. library(httr) appId = &quot;126...&quot; #use Instagram App ID and secret (not Facebook App) appSecret = &quot;b73...&quot; redirectUri = &quot;https://mmeierba.github.io/&quot; #example code = &quot;AQD...&quot; id&lt;-POST(&quot;https://api.instagram.com/oauth/access_token&quot;, body=list(client_id=appId, client_secret=appSecret, grant_type=&quot;authorization_code&quot;, redirect_uri=redirectUri, code=code)) 14.3 Simple API call What does a simple API call look like? The Instagram Basic Display API is http-based. To query a node or edge, a GET() call can be used. The base URLs are api.instagram.com and graph.instagram.com. accessToken = &quot;...&quot; userId = &quot;...&quot; ## Query the user node GET(&quot;https://graph.instagram.com/userId?fields=id,username&amp;access_token=accessToken&quot;) GET(&quot;https://graph.instagram.com/me?fields=id,username&amp;access_token=accessToken&quot;) #alternative ## Query the user media edge GET(&quot;https://graph.instagram.com/me/media?fields=id,caption&amp;access_token=accessToken&quot;) ## Query the user media node mediaId = &quot;...&quot; GET(&quot;https://graph.instagram.com/mediaId?fields=id,media_type,media_url,username,timestamp&amp;access_token=accessToken&quot;) 14.4 API access in R How can we access the API from R (httr + other packages)? The Instagram Graphic Display API can be accessed from R using the httr package. Current R packages specific to APIs from Instagram (e.g., instaR) are related to deprecated versions of Instagram APIs and are therefore no longer useful in the current version (Instagram (2021)). To the knowledge of the authors, there are no R packages specific to the Instagram Basic Display API. Are there social science research examples using the API? There are a couple of examples of studies in the field of social science that have used an Instagram API. However, the presented examples use older, depreciated versions of the Instagram API. Hu, Manikonda, and Kambhampati (2014) collected 50 user profiles and their most recent photos, as well as users’ lists of friends and followers using the Instagram API. Analyzing the data, the authors identified a range of photo categories and types of Instagram users. Ferwerda, Schedl, and Tkalcic (2015) used the Instagram API to extract Instagram pictures from survey participants, who had previously filled in a personality questionnaire. 113 survey participants granted the researchers access to their Instagram accounts through the API. The authors found that distinct features of the Instagram pictures (i.e., hue, brightness, saturation) associated with users’ personality traits. These examples show the potential that extracting data using the Instagram Basic Display API has for social science researchers. Yet, a lot of data are not yet being leveraged (e.g., captions). References Aslam, Salman. 2021. “• Instagram by the Numbers (2021): Stats, Demographics &amp; Fun Facts.” https://www.omnicoreagency.com/instagram-statistics/. Facebook for Developers. 2021. “Instagram Basic Display API.” https://developers.facebook.com/docs/instagram-basic-display-api/. Ferwerda, Bruce, Markus Schedl, and Marko Tkalcic. 2015. “Predicting Personality Traits with Instagram Pictures.” In Proceedings of the 3rd Workshop on Emotions and Personality in Personalized Systems 2015, 7–10. EMPIRE ’15. New York, NY, USA: Association for Computing Machinery. Hu, Yuheng, L Manikonda, and S Kambhampati. 2014. “What We Instagram: A First Analysis of Instagram Photo Content and User Types.” ICWSM. Instagram. 2021. “Instagram Developer Documentation.” https://www.instagram.com/developer/. Tankovska, H. n.d. “Topic: Instagram.” https://www.statista.com/topics/1882/instagram/. "],["instagram-graph-api.html", "Chapter 15 Instagram Graph API 15.1 Provided services/data 15.2 Prerequisites 15.3 Simple API call 15.4 API access in R 15.5 Social science examples", " Chapter 15 Instagram Graph API Philipp Kadel You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;) 15.1 Provided services/data What data/service is provided by the API? The Instagram Graph API is provided by Facebook. There are two main APIs for Instagram, the Instagram Basic Display API and the Instagram Graph API. The latter is described in the following. The API can be used to get and manage published photos, videos, and stories as well as getting basic data about other Instagram Business users and Creators. It is also possible to moderate comments and their replies and to measure media and profile interaction. Photos and videos can be published directly from the API. It can also be used to discover hashtagged media and mentions. For photos and videos different metrics can be obtained: engagement – Total number of likes and comments on the media object. Impressions – Total number of times the media object has been seen. Reach – Total number of unique accounts that have seen the media object. Saved – Total number of unique accounts that have saved the media object. Video_views – (Videos only) Total number of times the video has been seen. Returns 0 for videos in carousel albums. Likewise, there are several metrics about stories that are provided by the API: Exits – Number of times someone exited the story. Impressions – Total number of times the story has been seen. Reach – Total number of unique accounts that have seen the story. Replies – Total number of replies to the story. Taps_forward – Total number of taps to see this story’s next photo or video. 15.2 Prerequisites What are the prerequisites to access the API (authentication)? For most endpoints you need an Instagram Business Account, a Facebook Page that is connected to that account, a Facebook Developer Account and a Facebook App with Basic settings configured. Facebook provides a tutorial for setting this up here. 15.3 Simple API call What does a simple API call look like? First of all you have to load the required httr package. library(httr) Below you can find expamples of simple API calls for the Instagram Graph API. Get Fields and Edges on an IG Media. Fields can be e.g., “caption”, “comments_count”, “like_count”, or “timestamp”. GET (“https://graph.facebook.com/v10.0/{ig-media-id} ?fields={fields} &amp;access_token={access-token}”) Example: GET(&quot;https://graph.facebook.com/v10.0/17895695668004550 ?fields=id,media_type,media_url,owner, timestamp&amp;access_token=IGQVJ...&quot;) Response: { &quot;id&quot;: &quot;17895695668004550&quot;, &quot;media_type&quot;: &quot;IMAGE&quot;, &quot;media_url&quot;: &quot;https://fb-s-b-a.akamaihd.net/h-ak-fbx/t51.2885-9/21227247_1640962412602631_3222510491855224832_n.jpg?_nc_log=1&quot;, &quot;owner&quot;: { &quot;id&quot;: &quot;17841405822304914&quot; }, &quot;timestamp&quot;: &quot;2017-08-31T18:10:00+0000&quot; Return Fields and Edges on an IG Hashtag. Field can be the name of the hashtag without the “#” symbol or a hashtag ID. GET (“https://graph.instagram.com/{ig-hashtag-id} ?fields={fields} &amp;access_token={access-token}”) Example: GET(&quot;https://graph.facebook.com/17841593698074073 ?fields=id,name &amp;access_token=EAADd...&quot;) Response: { &quot;id&quot;: &quot;17841593698074073&quot;, &quot;name&quot;: &quot;coke&quot; } Get fields and edges on an Instagram Business or Creator Account. Fields can be e.g., “biography”, “id”, “followers_count”, or “media_count”. GET(“https://graph.facebook.com/v10.0/{ig-user-id}?fields={fields} &amp;access_token={access-token}”) Example: GET(&quot;https://graph.facebook.com/v3.2/17841405822304914 ?fields=biography%2Cid%2Cusername%2Cwebsite&amp;access_token=EAACwX...&quot;) Response: { &quot;biography&quot;: &quot;Dino data crunching app&quot;, &quot;id&quot;: &quot;17841405822304914&quot;, &quot;username&quot;: &quot;metricsaurus&quot;, &quot;website&quot;: &quot;http://www.metricsaurus.com/&quot; } 15.4 API access in R How can we access the API from R (httr + other packages)? The httr package can be used to access the Instagram Graph API. There used to be a instaR package but it was made for the old Instagram API and can not be used anymore. The FBinsightsR package provides access to the Insights API. Its fbins_insta function can be used to collect Instagram insights. Detailed information on the packages’ functions can be found here and more information on the deprecated instaR package here. 15.5 Social science examples Are there social science research examples using the API? In their study, Ferwerda, Schedl, and Tkalcic (2015) tried to infer personality traits from the way users take pictures and apply filters to them. The authors found distinct picture features (e.g., hue, brightness, saturation) that are related to personality traits. Brown et al. (2019) investigated the link between acute suicidality and language use as well as activity on Instagram. Differences in activity and language use on Instagram were not associated with acute suicidality. The goal of a study by Hosseinmardi et al. (2015) was to automatically detect and predict incidents of cyberbullying on Instagram. Based on a sample data set consisting of Instagram images and their associated comments, media sessions were labeled for cyberbullying. Associations are investigated between cyberbullying and a host of features such as cyber aggression, profanity, social graph features, temporal commenting behavior, linguistic content, and image content. References Brown, Rebecca C, Eileen Bendig, Tin Fischer, A David Goldwich, Harald Baumeister, and Paul L Plener. 2019. “Can Acute Suicidality Be Predicted by Instagram Data? Results from Qualitative and Quantitative Language Analyses.” PLoS One 14 (9): e0220623. Ferwerda, Bruce, Markus Schedl, and Marko Tkalcic. 2015. “Predicting Personality Traits with Instagram Pictures.” In Proceedings of the 3rd Workshop on Emotions and Personality in Personalized Systems 2015, 7–10. EMPIRE ’15. New York, NY, USA: Association for Computing Machinery. Hosseinmardi, Homa, Sabrina Arredondo Mattson, Rahat Ibn Rafiq, Richard Han, Qin Lv, and Shivakant Mishr. 2015. “Prediction of Cyberbullying Incidents on the Instagram Social Network,” August. https://arxiv.org/abs/1508.06257. "],["internet-archive-api.html", "Chapter 16 Internet Archive API 16.1 Provided services/data 16.2 Prerequesites 16.3 Simple API call 16.4 API access 16.5 Social science examples", " Chapter 16 Internet Archive API Lukas Isermann You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;jsonlite&#39;, &#39;tibble&#39;, &#39;archiveRetriever&#39;) 16.1 Provided services/data What data/service is provided by the API? The Internet Archive API (Internet Archive 2016) and the archiveRetriever (Gavras and Isermann 2022) give access to mementos stored in the Internet Archive. The Internet Archive is a non-profit organisation which builds and provides a digital library of Internet sites and other digitally stored artefacts such as books, audio recordings, videos, images and software programs. Today, the library stores about 588 billion web pages and covers a time span of over 25 years (Internet Archive 2022). The Internet Archive API offers options to search and query the Internet Archive and to retrieve information on whether a certain Url is archived and currently available in the Internet Archive. Additionally, the API allows users to retrieve the Urls and the specific time stamps of the mementos available, and gives options to limit the time frame of any search and the type of resource searched for. The R-package archiveRetriever offers easy access to the base functions of the Internet Archive API. Additionally, the archiveRetriever is designed to help with web scraping information from the Internet Archive. Besides offering access to the availability and Urls of Internet Archive mementos, the archiveRetriever allows to retrieve the Urls of the mementos of any sub pages that may be linked to in the original memento. Further, the archiveRetriever offers a function to easily scrape information from these mementos. 16.2 Prerequesites What are the prerequisites to access the API (authentication)? The API is free and accessible without any authentification via HTTP Urls, httr and the archiveRetriever. 16.3 Simple API call What does a simple API call look like? The API provides searches and options that can be accessed via HTTP Urls. These Urls take the pattern: http://web.archive.org/cdx/search/cdx?parameters/ As parameters, the API takes the obligatory input url. Further, you can refine your search by setting a timeframe for your search with the parameters from and to, and specify the match type of the Url with matchType as being exact, prefix, host, or domain to indicate whether you are looking for results matching the exact Url, results with the given Url as prefix, all matching results from the host archive.org, or all matching results from the host archive.org including all sub hosts *.archive.org (Internet Archive 2016). Further, you can collapse search results by any of the other parameters with collapse. To collapse results by day you could for example use the option collapse=timestamp:8 to collapse by time unit indicated by the 8th character of the time stamp (time stamps are stored in the format yyyymmddhhmmss). A complete API call can then look like the example below: http://web.archive.org/cdx/search/cdx?url=nytimes.com&amp;from=20191201&amp;to=20191202&amp;collapse=timestamp:8&amp;matchType=exact&amp;output=json/ The output of the API is a json-file containing information on mementos stored in the Internet Archive. Most importantly, the json-file returns the timestamp and the original Url of the memento. This information can then be used to generate memento Urls. A detailed description of the different functions of the API can be found on github (Internet Archive 2016). 16.4 API access How can we access the API from R (httr + other packages)? Instead of typing the API request into our browser, we can use the httr package’s GET() function to access the API from R. # Load required packages library(httr) library(jsonlite) library(tibble) # API call res &lt;- GET(&quot;http://web.archive.org/cdx/search/cdx?&quot;, query = list( url = &quot;nytimes.com&quot;, matchType = &quot;url&quot;, from = &quot;20191201&quot;, to = &quot;20191202&quot;, collapse = &quot;timestamp:8&quot;, output = &quot;json&quot; )) # Translate json output to tibble result &lt;- httr::content(res, type = &quot;text&quot;) result &lt;- fromJSON(result) result &lt;- as_tibble(result) names(result) &lt;- result[1,] result &lt;- result[-1,] result This output can also be used to generate the Urls of the stored mementos. # Generate vector of memento Urls urls &lt;- paste0(&quot;https://web.archive.org/web/&quot;, result$timestamp, &quot;/&quot;, result$original) urls Using the archiveRetriever Alternatively, we can use the archiveRetriever to access selected functions of the Internet Archive API. The archiveRetriever offers four different functions, archive_overview(), retrieve_urls(), retrieve_links(), and scrape_urls(). While archive_overview() and retrieve_urls() work as a wrapper for the Internet Archive API, retrieve_links() and scrape_urls() go beyond the functionality of the API and simplify larger data collections and web scraping from the Internet Archive. archive_overview() queries the Internet Archive API to get an overview over the dates for which mementos of a specific Url are stored in the Internet Archive. The function outputs a nicely formatted calendar to graphically display the Url’s availability in the Internet Archive. library(archiveRetriever) archive_overview(&quot;nytimes.com&quot;, startDate = &quot;20051101&quot;, endDate = &quot;20051130&quot;) retrieve_urls() offers direct access to the memento Urls of any Url stored in the Internet Archive. The function takes the base Url and the time frame as inputs and returns a vector of memento Urls. For convenience, retrieve_urls() automatically collapses mementos by day. However, this can be deactivated with the option collapseDate = FALSE. urls &lt;- retrieve_urls(homepage = &quot;nytimes.com&quot;, startDate = &quot;20191201&quot;, endDate = &quot;20191202&quot;, collapseDate = FALSE) head(urls) Oftentimes researchers and practitioners do not know the specific Urls they are interested in, but want to retrieve information from all or some subpages of a base Url, e.g. when scraping news content from online newspapers. For this purpose, the archiveRetriever offers the function retrieve_links(). retrieve_links() takes any number of memento Urls (obtained via retrieve_urls()) as input and returns a data.frame listing all links of mementos referenced to in the original memento. This enables users e.g. to obtain all mementos of newspaper articles from the New York Times linked to on the memento of the homepage of the newspaper. links &lt;- retrieve_links(&quot;http://web.archive.org/web/20191201001433/https://www.nytimes.com/&quot;) head(links) Finally, memento Urls and their time stamps are seldom the information of interest for anyone using the Internet Archive. Therefore, the archiveRetriever offers the function scrape_urls() to easily scrape any content from Internet Archive mementos obtained by the previous functions. The function takes a memento of the Internet Archive and a named vector of XPaths (or CSS) as obligatory inputs and results in a tibble with the content scraped using the XPath/CSS selectors. nytimes_article &lt;- scrape_urls(Urls = &quot;http://web.archive.org/web/20201001004918/https://www.nytimes.com/2020/09/30/opinion/biden-trump-2020-debate.html&quot;, Paths = c(title = &quot;//h1[@itemprop=&#39;headline&#39;]&quot;, author = &quot;//span[@itemprop=&#39;name&#39;]&quot;, date = &quot;//time//text()&quot;, article = &quot;//section[@itemprop=&#39;articleBody&#39;]//p&quot;)) nytimes_article scrape_urls() comes with many options that facilitate the scraping of large amounts of Internet Archive mementos and make the scraping more flexible. A detailed description of the different functions of the archiveRetriever including scrape_urls() can be found on github and in the package documentation. 16.5 Social science examples Are there social science research examples using the API? The Internet Archive is still a rarely used source for social sciences. While there are some works focused directly on the Internet Archive and its contents (e.g. Milligan, Ruest, and Lin 2016; Littman et al. 2018; Hale, Blank, and Alexander 2017), or the development of the internet itself (e.g. Hale et al. 2014; Brügger, Laursen, and Nielsen 2017), research using he Internet Archive as data source for questions unrelated to the Internet or the Internet Archive itself are scarce. A notable exception is Gavras (2022), who uses the Internet Archive to access newspaper articles from a total of 86 online newspapers in 29 countries across Europe to research European media discourse. References Brügger, Niels, Ditte Laursen, and Janne Nielsen. 2017. “Exploring the Domain Names of the Danish Web.” In The Web as History: Using Web Archives to Understand the Past and the Present, edited by Niels Brügger and Ralph Schroeder. UCL Press. https://doi.org/10.2307/j.ctt1mtz55k. Gavras, Konstantin. 2022. “The Conditions and Nature of Europeanized Public Discourse ? A Multi-Lingual QTA Analysis of Public Discourses in Europe Using the Internet Archive, 2016-2019.” PhD thesis, Mannheim. https://madoc.bib.uni-mannheim.de/62270/. Gavras, Konstantin, and Lukas Isermann. 2022. archiveRetriever: Retrieve Archived Web Pages from the ’Internet Archive’. https://CRAN.R-project.org/package=archiveRetriever. Hale, Scott A., Grant Blank, and Victoria D. Alexander. 2017. “Live Versus Archive: Comparing a Web Archive to a Population of Web Pages.” In The Web as History: Using Web Archives to Understand the Past and the Present, edited by Niels Brügger and Ralph Schroeder. UCL Press. https://doi.org/10.2307/j.ctt1mtz55k. Hale, Scott A., Taha Yasseri, Josh Cowls, Eric T. Meyer, Ralph Schroeder, and Helen Margetts. 2014. “Mapping the UK Webspace: Fifteen Years of British Universities on the Web.” In Proceedings of the 2014 ACM Conference on Web Science - WebSci ’14, 62–70. Bloomington, Indiana, USA: ACM Press. https://doi.org/10.1145/2615569.2615691. Internet Archive. 2016. Wayback CDX Server API - BETA. https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server#readme. ———. 2022. “About the Internet Archive.” 2022. https://archive.org/about/. Littman, Justin, Daniel Chudnov, Daniel Kerchner, Christie Peterson, Yecheng Tan, Rachel Trent, Rajat Vij, and Laura Wrubel. 2018. “API-based Social Media Collecting as a Form of Web Archiving.” International Journal on Digital Libraries 19 (1): 21–38. https://doi.org/10.1007/s00799-016-0201-7. Milligan, Ian, Nick Ruest, and Jimmy Lin. 2016. “Content Selection and Curation for Web Archiving: The Gatekeepers Vs. The Masses.” In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, 107–10. Newark New Jersey USA: ACM. https://doi.org/http://dx.doi.org/10.1145/2910896.2910913. "],["large-language-model-apis.html", "Chapter 17 Large Language Model APIs 17.1 Prerequisites 17.2 Simple API Call in R 17.3 Social science examples", " Chapter 17 Large Language Model APIs Felix Rusche This chapter provides an introduction to APIs for Large Language Models (LLMs). Given the wealth of models, I focus on text based models only. I provide information on one commercial and one open source ‘platform’: OpenAI and Ollama. Both offer a range of different models, depending on use case and budget. While OpenAI’s (and other commercial providers’) models are (potentially) more capable and accurate, open source models provided on Ollama are free to use, offer additional features (such as uncensored versions), and can be run on local machines. They may also better replicate given that commercial providers tend to deprecate outdated models once new ones are made available. In this short introduction, I show how to make API calls to LLMs via R. Overall, LLMs offer substantial efficiency gains in certain tasks (such as classifying text) and allow researchers to delve into new data sets or revisit old ones with new tools at hand. However, it should also be noted that working with LLMs poses risks, some of which are yet to be discovered. For example, LLMs may amplify stereotypes and are well known to ‘hallucinate’, i.e. provide false information with much confidence. This requires researchers to thoroughly evaluate the quality of LLMs’ output. Final disclaimer: this chapter solely provides a short introduction to the use of LLMs via API requests and does not provide an in-depth introduction to the fine tuning of LLMs or prompts. 17.1 Prerequisites 17.1.1 Software and Registration For OpenAI, the key requirement is a registration via their website, including the provision of a method of payment. Users can then generate an API key. As suggested in the Best Practices Chapter, it is recommended to store the key as an environment variable. To do so, type the following in the console: usethis::edit_r_environ(scope = &quot;user&quot;) A document will open. Add a new line with the key and re-start R: OPENAI_API_KEY=ENTER_KEY_HERE The key can now be called using the Sys.getenv(“OPENAI_API_KEY”) command (see below). While not recommended, users may also replace this command with the actual key. To use Ollama, users simply need to download, install, and run Ollama. 17.1.2 Choosing a Model Depending on use case and budget, researchers can choose from a host of different models from both Ollama and OpenAI. These mainly differ in their power and accuracy. They may also differ in other dimensions, e.g. if models are created for more specific use cases. Starting with OpenAI, the firm offers models of different quality and pricing. For some tasks (like simple classification tasks), cheaper models may be sufficient. For more complex ones, users may prefer to draw on more expensive and capable ones. It is generally advisable to test the quality of different models to determine which one is the best fit. Models are paid by the length of input and output text. OpenAI’s pricing page allows users to estimate costs. At Ollama, the use of models is free of charge. However, open source models currently remain less powerful than commercial models. On the website, users can choose from a wide range of different models. For this article, Llama3 is chosen, a capable open source model developed by Meta. After choosing the model, a version of the model may need to be selected. Usually, multiple versions of models with the same name are offered. These vary by use case and, more importantly, parameter size. For example, Llama3 comes in two sizes: 8 billion and 70 billion parameters. While a higher number of parameters translates into a more powerful model, it also requires substantially more (GPU) RAM and storage space. For instance, while the 8B version is likely to run on a (good) notebook or computer, the 70B one likely requires an external server / high speed computer. To install the 8B version of Llama3, users simply open their terminal/console and type: ollama run llama3 This will download an start the model. It also enables users to directly chat with the model via the terminal. This window can be closed once the respective model was downloaded. In order to call Ollama’s API, one then needs to start the previously installed application and run it in the background. This will create an active access point for API calls. 17.2 Simple API Call in R To access the APIs and prepare its results in R, the following three packages are required: library(httr) library(jsonlite) library(stringr) Further, a prompt needs to be defined, e.g. prompt &lt;- &quot;Briefly answer: What is the most unusual item that has ever been used as currency?&quot; 17.2.1 OpenAI: ChatGPT-4o For OpenAI, I choose the model GPT-4o. Models and their respective names can be found via OpenAI’s website. A simple API call then looks like this: response_OpenAI &lt;- POST( url = &quot;https://api.openai.com/v1/chat/completions&quot;, add_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))), content_type_json(), encode = &quot;json&quot;, body = list( model = &quot;gpt-4o&quot;, # choose model messages = list(list(role = &quot;user&quot;, content = prompt)), # enter prompt to be sent to model temperature = 0 # choose &quot;temperature&quot; ) ) # here, the answer is extracted from the json file provided by the API answer_OpenAI &lt;- content(response_OpenAI)$choices[[1]]$message$content answer_OpenAI &lt;- str_trim(answer_OpenAI) Print answer: cat(answer_OpenAI) One of the most unusual items ever used as currency is the **rai stones** of Yap Island in Micronesia. These large, circular stone disks, some of which can be up to 12 feet in diameter, were used in various transactions, including dowries and political deals. Despite their size and weight, ownership of the stones, rather than physical possession, was often what mattered, making them a unique form of currency. 17.2.1.1 Alternative: Using ChatGPT-4o via an API Wrapper: An alternative to directly calling the API via httr is the use of an API wrapper, i.e. a package that simplifies the call further. For Python, OpenAI maintains its own wrapper. For R (which this article is focused on) Rudnytskyi (2023) maintains a package. This keeps getting updated, so please visit the package’s website for updates. The package is applied as follows: # if not yet installed, install the package remotes::install_github(&quot;irudnyts/openai&quot;, ref = &quot;r6&quot;) #------- # load it library(openai) # load the API key. The package expects it to be stored as an # environment variable called OPENAI_API_KEY!! # Make sure it is stored this way (see Prerequisites above) client &lt;- OpenAI() # send API request completion &lt;- client$chat$completions$create( model = &quot;gpt-4o&quot;, # choose model messages = list(list(role = &quot;user&quot;, content = prompt)), # enter prompt to be sent to model temperature = 0 # choose &quot;temperature&quot; (and potentially other settings) ) # Extract answer from returned object answer_OpenAI2 &lt;- completion[[&quot;choices&quot;]][[1]][[&quot;message&quot;]][[&quot;content&quot;]] Print answer: cat(answer_OpenAI2) ## Historically, some unusual items used as currency include large stones, shells, and even human teeth. 17.2.2 Ollama: Llama3 Similarly, a simple API call using Llama3 can be conducted as follows: response_Llama &lt;- POST( url = &quot;http://localhost:11434/api/generate&quot;, body = list( model = &quot;llama3&quot;, # choose model prompt = prompt, # enter prompt to be sent to model stream = FALSE, options = list( temperature = 0 # choose &quot;temperature&quot; )), encode = &quot;json&quot; ) # here, the answer is extracted from the json file provided by the API and prepared as a full text file response_text &lt;- content(response_Llama, &quot;text&quot;) json_strings &lt;- strsplit(response_text, &quot;\\n&quot;)[[1]] parsed_jsons &lt;- lapply(json_strings, fromJSON) responses &lt;- sapply(parsed_jsons, function(x) x$response) answer_Llama &lt;- paste(responses, collapse = &quot; &quot;) Print answer: cat(answer_Llama) ## One of the most unusual items used as currency is whale vomit, also known as ambergris. In the 18th century, it was used as a form of currency in some Pacific Island cultures, particularly in Fiji and Tonga. Ambergris is a rare and valuable substance produced by sperm whales, and its unique properties made it highly sought after for use in perfumes and medicines. The value of ambergris was so great that it was even used to settle debts and buy land! 17.2.3 Some Parameter Choices and Settings For better replicability, the temperature (usually defined between 0 and 2) of the above LLMs is set to 0. A lower temperature ensures that the algorithm will tend to select words with the highest probability. While some randomness remains, this increases the probability that results will replicate. It also decreases the creativity/diversity of responses and, hence, the probability of ‘halucination’. Models also allow users to control a number of additional parameters. For example, this includes the maximum response length, the number of responses created, or (sometimes even) to set a seed. To find out about specific models’ parameters, it is recommendable to visit their documentation pages. Another setting that may be relevant is the role assigned to the model. Specifically, one can tell the OpenAI model to answer and behave in specific ways via the messages item. The model will then behave accordingly. For example, one of the most common roles assigned is that of the “helpful assistant”: messages = list(list(role = &quot;system&quot;, content = &quot;You are a helpful assistant.&quot;), list(role = &quot;user&quot;, content = prompt)) To use this setting, simply replace the message item in the API request above. The most important ‘setting’ is the prompt itself. Prompts can substantially affect the quality of answers. It is advisable to read guides on how to best prompt specific models and to test different versions of the ‘same’ prompt. Models also differ in how well their prompting works. For instance, it is argued that OpenAI models are easy to prompt while, e.g., Llama3 can produce results of similar quality but the prompt is more difficult to get right. Finally, OpenAI allows users to send batch requests. In theory, these should be particularly interesting to power users that aim to send many of the same requests on different texts. However, at the time this article is written, batch requests only start getting interesting once users made a considerable number of requests and moved up OpenAI’s user ladder. Specifically, to increase rate limits, users have to effectively spend money and time on the platform. They subsequently move up the user ladder from “free” to tier 1 and finally tier 5. Through this, they receive higher rate limits. Batch request only really get interesting once users reach tier 3. 17.3 Social science examples LLMs are a recent tool and its applications in research are still being explored. One application is the use of LLMs as a cheap research assistant. LLMs can read and classify hundreds or even thousands of texts within minutes and at very lost costs. For example, in Evsyukova, Rusche, and Mill (2023) my co-authors and I send responses received in an experiment to ChatGPT-4 in order to evaluate their usefulness and classify their content. In a different application, Djourelova et al. (2024) explore newspaper coverage following extreme weather events. Specifically, the authors send local newspaper articles on the event to a LLM, asking whether the respective article draws a causal connection between the event and climate change (among other questions). In both papers, the authors find that agreement between LLMs and human annotators is at a similar level as agreement between any two human annotators. Another potential pathway for social science is ‘random silicon sampling’ as suggested by Sun et al. (2024). Specifically, LLMs can be assigned specific demographic features and be asked to answer surveys or questions in ways that resemble this demographic group. References Djourelova, Milena, Ruben Durante, Elliot Motte, and Eleonora Patacchini. 2024. “Experience, Narratives, and Climate Change Beliefs.” Working Paper. Evsyukova, Yulia, Felix Rusche, and Wladislaw Mill. 2023. “LinkedOut? A Field Experiment on Discrimination in Job Network Formation.” CRC TR 224 Discussion Paper Series No. 482. https://www.crctr224.de/research/discussion-papers/archive/dp482. Rudnytskyi, Iegor. 2023. Openai: R Wrapper for OpenAI API. https://github.com/irudnyts/openai. Sun, Seungjong, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, and Jang Hyun Kim. 2024. “Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information.” https://arxiv.org/abs/2402.18144. "],["media-cloud-api.html", "Chapter 18 Media Cloud API 18.1 Provided services/data 18.2 Prerequisites 18.3 Simple API call 18.4 API access in R 18.5 Social science examples", " Chapter 18 Media Cloud API Chung-hong Chan You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;stringr&#39;, &#39;mediacloud&#39;, &#39;tidytext&#39;, &#39;quanteda&#39;, &#39;quanteda&#39;) 18.1 Provided services/data What data/service is provided by the API? According to the official FAQ, Media Cloud is “an open source and open data platform for storing, retrieving, visualizing, and analyzing online news.” It is a consortium project across multiple institutions, including the University of Massachusetts Amherst, Northeastern University, and the Berkman Klein Center for Internet &amp; Society at Harvard University. The full technical information about the project and the data provided are available in Roberts et al. (2021). In short, the system continuously crawls RSS and similar feeds from a large collection of media sources (as of writing: &gt; 25,000 media sources). Based on this large corpus of media contents, the system provides three services: Topic Mapper, Media Explorer, and Source Manager. The services are accessible through the web interface and csv export is also supported from there. For programmatic access, Media Cloud also provides several APIs. I will focus on the main v2.0 API, because it is currently the only public API. The main API provides functions to retrieve stories, tags, and sentences. Probably due to copyright reasons, the API does not provide full-text stories. But it is possible to pull document-term matrices from the API. 18.2 Prerequisites What are the prerequisites to access the API (authentication)? * An API Key is required. One needs to register for an account at the official website of Media Cloud. After having the access, click on your profile to obtain the API Key. It is recommended to set the API key as the environment variable MEDIACLOUD_API_KEY. Please consult Chapter 2 on how to do that in the section on Environment Variables. 18.3 Simple API call What does a simple API call look like? The API documentation is available here. Please note the request limit. The most important end points are: GET api/v2/media/list/ GET api/v2/stories_public/list GET api/v2/stories_public/count GET api/v2/stories_public/word_matrix It is also important to learn about how to write a solr query. It is used in either q (“query”) and fq (“filter query”) of many end point requests. For example, to search for stories with both “mannheim” and “university” in the New York Times (media_id = 1), the solr query should be: text:mannheim+AND+text:university+AND+media_id:1. In this example, we are going to search for 20 stories in the New York Times (Media ID: 1) mentioning “mannheim AND university”. library(httr) library(stringr) url &lt;- parse_url(&quot;https://api.mediacloud.org/api/v2/stories_public/list&quot;) params &lt;- list(q = &quot;text:mannheim+AND+text:university+AND+media_id:1&quot;, key = Sys.getenv(&quot;MEDIACLOUD_API_KEY&quot;)) url$query &lt;- params final_url &lt;- str_replace_all(build_url(url), c(&quot;%3A&quot; = &quot;:&quot;, &quot;%2B&quot; = &quot;+&quot;)) res &lt;- GET(final_url) httr::content(res) 18.4 API access in R How can we access the API from R (httr + other packages)? As of writing, there are (at least) four R packages for accessing the Media Cloud API. Although the mediacloudr package by Dix Jan is available on CRAN, I recommend using the mediacloud package by Julian Unkel (LMU). It is available on Dr Unkel’s Github. By default, the package always returns “tidy” objects. The package can be installed by: devtools::install_github(&quot;joon-e/mediacloud&quot;) The above “mannheim” example can be replaced by (the package looks for the environment variable MEDIACLOUD_API_KEY automatically): library(mediacloud) mc_mannheim &lt;- search_stories(text = &quot;mannheim AND university&quot;, media_id = 1, n = 20) mc_mannheim ## # A tibble: 20 × 9 ## stories_id media_id publish_date title url processed_stories_id media_name collect_date tags ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dttm&gt; &lt;name&gt; ## 1 44790862 1 2011-11-23 15:20:21 Special Report: Education: A Scholarly Role for Consumer Technology http://feeds.nytimes.com/click.phdo?i=f14ddd7045ae5899bdaed53548ea0fac 99320532 New York … 2011-11-23 23:02:06 &lt;list&gt; ## 2 78373861 1 2012-04-08 21:47:48 Germany Is the Place to Go to Block a Rival&#39;s Technology http://feeds.nytimes.com/click.phdo?i=0b033a29b0a1cfd89e7d94303e90af5b 102657410 New York … 2012-04-08 23:57:08 &lt;list&gt; ## 3 82249892 1 2012-05-29 20:45:00 European Official Calls for an Economic Road Map http://feeds.nytimes.com/click.phdo?i=b2370b329b6db682100a36312c9bc154 103821112 New York … 2012-05-29 20:57:57 &lt;list&gt; ## 4 28218544 1 2010-12-09 03:43:41 Back Injury Sidelines Klitschko http://feeds1.nytimes.com/~r/nyt/rss/Sports/~3/ctnUdsJRHzk/sports-uk-boxing-klitschko-injury.html 338790634 New York … 2010-12-09 17:55:43 &lt;list&gt; ## 5 314056973 1 2015-02-01 00:00:00 As New York Moves People With Developmental Disabilities to Group Homes, Some Families Struggle http://www.nytimes.com/2015/02/01/nyregion/as-new-york-moves-people-with-developmental-disabilities-t… 402084602 New York … 2015-01-29 20:55:31 &lt;list&gt; ## 6 85744480 1 2012-07-02 06:12:03 M.B.A. Candidates Learn Leadership, in the Mud http://feeds.nytimes.com/click.phdo?i=e758f43cc4e99833540e6d909491b96a 419439006 New York … 2012-07-02 08:52:45 &lt;list&gt; ## 7 225224567 1 2014-04-23 21:47:59 Personal Journeys: All Aboard the Orient Local http://rss.nytimes.com/c/34625/f/642561/s/39ae85bd/sc/38/l/0L0Snytimes0N0C20A140C0A40C270Ctravel0Call… 419552946 New York … 2014-04-24 03:30:12 &lt;list&gt; ## 8 94259735 1 2012-11-24 05:03:13 Arts | New Jersey: New Jersey Holiday Concerts Offer a Wide Range of Styles http://www.nytimes.com/2012/11/25/nyregion/new-jersey-holiday-concerts-offer-a-wide-range-of-styles.h… 420309253 New York … 2012-11-24 15:01:11 &lt;list&gt; ## 9 93835753 1 2012-11-18 05:15:34 Alexandra Gutowski, Zachary Allen &amp;#x2014; Weddings http://www.nytimes.com/2012/11/18/fashion/weddings/alexandra-gutowski-zachary-allen-weddings.html?par… 420359845 New York … 2012-11-18 12:53:46 &lt;list&gt; ## 10 87016471 1 2012-06-05 02:31:14 Germany Open to Deal on Pooling Euro Debt, With Limits http://feeds.nytimes.com/click.phdo?i=5d966d2d43aaad1a056e5ae0e1d3e559 420417459 New York … 2012-08-16 00:42:29 &lt;list&gt; ## 11 85768000 1 2012-07-02 08:57:47 IHT Rendezvous: The Zany Business of Teaching Management http://feeds.nytimes.com/click.phdo?i=6ab66f3da93480f3e56d6c286b75723b 420549716 New York … 2012-07-02 19:37:32 &lt;list&gt; ## 12 82322640 1 2012-06-01 15:14:14 European Crisis Bolsters Illegal Sales of Body Parts http://feeds.nytimes.com/click.phdo?i=71ef4d87201aca8be1194d44b2cf6012 420757923 New York … 2012-06-12 20:13:56 &lt;list&gt; ## 13 298730990 1 2014-12-05 05:32:31 Calendar: Events in Connecticut for Dec. 7-13, 2014 http://rss.nytimes.com/c/34625/f/640367/s/41240614/sc/10/l/0L0Snytimes0N0C20A140C120C0A70Cnyregion0Ce… 428490928 New York … 2014-12-05 06:17:41 &lt;list&gt; ## 14 23790844 1 2010-09-09 16:24:59 In Land of Fast Cars and Trains, Buses Try to Make Inroads http://feeds.nytimes.com/click.phdo?i=bb0541910fe17957f2cb52a2b205ae58 630486887 New York … 2010-09-09 21:30:45 &lt;list&gt; ## 15 354583774 1 2015-06-19 11:02:56 The Saturday Profile: A German Writer Translates a Puzzling Illness Into a Best-Selling Book http://rss.nytimes.com/c/34625/f/642565/s/47654dfb/sc/33/l/0L0Snytimes0N0C20A150C0A60C20A0Cworld0Ceur… 778925402 New York … 2015-06-19 11:21:31 &lt;list&gt; ## 16 385106169 1 2015-10-07 05:42:02 Well: Homing In on the Source of Runner’s High http://rss.nytimes.com/c/34625/f/640347/s/4a79477a/sc/14/l/0Lwell0Bblogs0Bnytimes0N0C20A150C10A0C0A70… 833493930 New York … 2015-10-07 07:04:59 &lt;list&gt; ## 17 371289843 1 2015-08-20 07:26:16 News: Morning Agenda: Slow Inflation Makes Fed Cautious Over Rates http://news.blogs.nytimes.com/2015/08/20/morning-agenda-slow-inflation-makes-fed-cautious-over-rates/… 869026798 New York … 2015-08-20 08:57:27 &lt;list&gt; ## 18 601772076 1 2011-10-20 08:00:00 Gauging the Value of Your M.B.A. http://www.nytimes.com/2011/10/20/education/20iht-SReducEmploy20.html?pagewanted=all 898178364 New York … 2017-03-30 20:11:59 &lt;list&gt; ## 19 651215537 1 2017-07-02 00:25:00 Nashonme Johnson, Pasquale Di Stasio https://www.nytimes.com/2017/07/02/fashion/weddings/nashonme-johnson-pasquale-di-stasio.html?partner=… 948038684 New York … 2017-07-02 02:00:29 &lt;list&gt; ## 20 656857074 1 2006-09-10 08:00:00 On Self http://www.nytimes.com/2006/09/10/magazine/10sontag.html?_r=1&amp;oref=slogin&amp;pagewanted=all 954374793 New York … 2017-07-13 13:55:39 &lt;list&gt; 18.4.1 Media keywords of “Universität Mannheim” In the following slightly more sophisticated example, we are going to first search for a list of all national German media outlets, search for a bunch of (German) articles mentioning “Universität Mannheim”, and then extract keywords using term frequency-inverse document frequency (TF-IDF). There are three steps. 18.4.1.1 Search for all national German media outlets All major German media outlets are tagged with Germany___National. The function search_media() is used to retrieve information about all national German media outlets. de_media &lt;- search_media(tag = &quot;Germany___National&quot;, n = 100) ## # A tibble: 50 × 5 ## media_id name url start_date tags ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;named list&gt; ## 1 19831 Spiegel http://www.spiegel.de 2013-06-10 00:00:00 &lt;list [19]&gt; ## 2 20001 taz.de http://www.taz.de 2017-04-03 00:00:00 &lt;list [17]&gt; ## 3 21558 neues-deutschland.de http://www.neues-deutschland.de 2017-04-03 00:00:00 &lt;list [12]&gt; ## 4 21854 jungewelt.de http://www.jungewelt.de 2018-06-04 00:00:00 &lt;list [21]&gt; ## 5 21917 berlinerumschau.com http://www.berlinerumschau.com 2014-12-29 00:00:00 &lt;list [7]&gt; ## 6 22009 bild.de http://www.bild.de 2013-06-10 00:00:00 &lt;list [21]&gt; ## 7 23037 manager-magazin.de http://www.manager-magazin.de 2017-04-10 00:00:00 &lt;list [9]&gt; ## 8 23538 n-tv.de http://www.n-tv.de 2017-04-03 00:00:00 &lt;list [9]&gt; ## 9 38697 zeit http://www.zeit.de/index 2013-03-11 00:00:00 &lt;list [20]&gt; ## 10 39206 Tagespiegel http://www.tagesspiegel.de 2013-03-18 00:00:00 &lt;list [20]&gt; ## # ℹ 40 more rows 18.4.1.2 Pull a list of articles The following query gets a list of 100 articles mentioning “universität mannheim” published in a specific date range from all national German media outlets. Unlike the AND operator, this search for the exact term. Also, a query is case insensitive. The function search_stories() can be used for this. unima_articles &lt;- search_stories(text = &quot;\\&quot;universität mannheim\\&quot;&quot;, media_id = de_media$media_id, n = 100, after_date = &quot;2021-01-01&quot;, before_date = &quot;2021-12-01&quot;) unima_articles ## # A tibble: 88 × 9 ## stories_id media_id publish_date title url processed_stories_id media_name collect_date tags ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;name&gt; ## 1 1815576509 385413 2021-01-05 10:57:12 Studie: Erneuter Lockdown belastete Unternehmensgewinne nicht spürbar https://www.boersen-zeitung.de/index.php?l=5&amp;isin=&amp;dpas… 2225420413 Börsen-Ze… 2021-01-05 11:14:23 &lt;list&gt; ## 2 1815575525 282241 2021-01-05 10:56:38 Studie: Erneuter Lockdown belastete Unternehmensgewinne nicht spürbar https://www.boerse-online.de/nachrichten/aktien/studie-… 2225420496 boerse-on… 2021-01-05 11:14:17 &lt;list&gt; ## 3 1815573995 42022 2021-01-05 11:05:00 Studie: Erneuter Lockdown belastete Unternehmensgewinne nicht spürbar https://www.finanznachrichten.de/nachrichten-2021-01/51… 2225423365 finanznac… 2021-01-05 11:14:05 &lt;list&gt; ## 4 1815653090 39727 2021-01-05 12:44:00 Bund-Länder-Beratungen: Jetzt live: Kanzlerin Merkel und Ministerpräsidenten stellen die neuen Beschlüsse vor https://www.stern.de/gesundheit/corona-gipfel-im-livebl… 2225488400 stern 2021-01-05 12:45:59 &lt;list&gt; ## 5 1815772594 21558 2021-01-05 11:07:16 Das Prinzip Links https://www.neues-deutschland.de/artikel/1146618.das-pr… 2225615666 neues-deu… 2021-01-05 15:43:01 &lt;list&gt; ## 6 1821852038 42022 2021-01-12 13:11:00 Medizinjurist: Impfpflicht für Pflegekräfte rechtlich vertretbar https://www.finanznachrichten.de/nachrichten-2021-01/51… 2231370715 finanznac… 2021-01-12 13:29:47 &lt;list&gt; ## 7 1822816931 41519 2021-01-13 10:54:24 Staatsverschuldung: Rückhalt für Schuldenbremse nimmt ab – Landespolitiker für Infrastrukturinvestitionen https://www.handelsblatt.com/politik/deutschland/staats… 2232460589 Handelsbl… 2021-01-13 11:04:27 &lt;list&gt; ## 8 1822860428 39206 2021-01-13 11:32:47 Schützen Konzerne ihre Beschäftigten ausreichend vor Corona-Ausbrüchen? https://www.tagesspiegel.de/politik/weniger-homeoffice-… 2232506700 Tagespieg… 2021-01-13 11:41:40 &lt;list&gt; ## 9 1823395678 42022 2021-01-14 01:07:00 PRESSESPIEGEL/Zinsen, Konjunktur, Kapitalmärkte, Branchen https://www.finanznachrichten.de/nachrichten-2021-01/51… 2233108356 finanznac… 2021-01-14 01:32:49 &lt;list&gt; ## 10 1829126009 41519 2021-01-20 09:57:59 Homeoffice und Hygienevorschriften: „Wir können die Zentrale nicht einfach abschließen“: Wirtschaft reagiert gespalten auf neue Corona-Regeln https://www.handelsblatt.com/karriere/homeoffice-und-hy… 2239054366 Handelsbl… 2021-01-20 10:17:51 &lt;list&gt; ## # ℹ 78 more rows 18.4.1.3 Pull word matrices With the list of stories_id, we can then use the function get_word_matrices() to obtain word matrices. 11 unima_mat &lt;- get_word_matrices(stories_id = unima_articles$stories_id, n = 100) ## # A tibble: 30,353 × 4 ## stories_id word_counts word_stem full_word ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1815573995 1 deutschland deutschland ## 2 1815573995 1 befragt befragt ## 3 1815573995 1 irgendeinem irgendeinem ## 4 1815573995 1 vorgelegten vorgelegten ## 5 1815573995 1 solo-selbstständig solo-selbstständige ## 6 1815573995 1 wert wert ## 7 1815573995 1 zufolg zufolge ## 8 1815573995 1 dpa-afx dpa-afx ## 9 1815573995 1 universität universität ## 10 1815573995 1 gewinnsitu gewinnsituation ## # ℹ 30,343 more rows The data frame unima_mat is in the so-called “tidytext” format (Silge and Robinson 2016). It can be used directly for analysis if one is fond of tidytext. For users of quanteda (Benoit et al. 2018), it is also possible to cast the data frame into a Document-Feature Matrix (DFM) 12. library(tidytext) library(quanteda) unima_dfm &lt;- cast_dfm(unima_mat, stories_id, word_stem, word_counts) unima_dfm ## Document-feature matrix of: 88 documents, 14,614 features (97.64% sparse) and 0 docvars. ## features ## docs deutschland befragt irgendeinem vorgelegten solo-selbstständig wert zufolg dpa-afx universität gewinnsitu ## 1815573995 1 1 1 1 1 1 1 1 1 1 ## 1815575525 1 1 1 1 1 1 1 1 1 1 ## 1815576509 0 0 0 0 0 0 0 1 1 0 ## 1815653090 1 1 0 1 0 1 0 0 1 0 ## 1815772594 0 0 0 0 0 0 0 0 1 0 ## 1821852038 0 0 0 0 0 0 0 0 1 0 ## [ reached max_ndoc ... 82 more documents, reached max_nfeat ... 14,604 more features ] And then standard operations can be done. unima_dfm %&gt;% dfm_tfidf() %&gt;% topfeatures(n = 20) ## aufsichtsrat hauptversammlung gesellschaft westw group vorstand aktionär vergütung aktien ab aktg se verwaltungsrat gemäß vergütungssystem mitglied geschäftsjahr ## 706.2057 619.3220 583.4058 494.6793 400.0420 379.0577 363.6897 333.5708 333.1135 332.9208 280.5663 260.5542 252.3862 242.9785 209.4179 201.7753 200.6090 ## board supervisori ermächtigung ## 187.9392 184.7259 167.2792 The faculties of BWL (Business Administration) and Jura (Law) would be happy with this finding. 18.5 Social science examples Are there social science research examples using the API? According to the paper by the official Media Cloud Team (Roberts et al. 2021), there are over 100 papers mentioning Media Could. Many papers use the counting endpoint to generate a time series of media attention to specific keywords (e.g. Benkler et al. 2015; Huckins et al. 2020). This function is widely used also in many data journalism pieces. The URLs collected from Media Cloud can also be used to do further crawling (e.g. Huckins et al. 2020). It is perhaps worth mentioning that the openly available useNews dataset (Puschmann and Haim 2021) provides a large collection of content from Media Cloud together with meta data other data sources. References Benkler, Yochai, Hal Roberts, Robert Faris, Alicia Solow-Niederman, and Bruce Etling. 2015. “Social Mobilization and the Networked Public Sphere: Mapping the SOPA-PIPA Debate.” Political Communication 32 (4): 594–624. https://doi.org/10.1080/10584609.2014.986349. Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774. Huckins, Jeremy F, Alex W daSilva, Weichen Wang, Elin Hedlund, Courtney Rogers, Subigya K Nepal, Jialing Wu, et al. 2020. “Mental Health and Behavior of College Students During the Early Phases of the COVID-19 Pandemic: Longitudinal Smartphone and Ecological Momentary Assessment Study.” Journal of Medical Internet Research 22 (6): e20185. https://doi.org/10.2196/20185. Puschmann, Cornelius, and Mario Haim. 2021. “useNews.” https://doi.org/10.17605/OSF.IO/UZCA3. Roberts, Hal, Rahul Bhargava, Linas Valiukas, Dennis Jen, Momin M Malik, Cindy Bishop, Emily Ndulue, et al. 2021. “Media Cloud: Massive Open Source Collection of Global News on the Open Web.” arXiv Preprint arXiv:2104.03702. Silge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037. For the sake of education, I split step 2 and 3 into two steps. Actually, it is possible to merge step 2 and step 3 by simply: get_word_matrices(text = \"\\\"universität mannheim\\\"\")↩︎ It is quite obvious that there are (many) duplicates in the retrieved data. For example, the first few documents are almost the same in the feature space. Packages such as textsdc might be useful for deduplication.↩︎ "],["overpass-api.html", "Chapter 19 Overpass API 19.1 Provided Services/Data 19.2 Prerequisites 19.3 Simple API Call 19.4 API Access in R 19.5 Social Science Examples", " Chapter 19 Overpass API Grace Olzinski and Nina Osenbrügge You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;tidyverse&#39;, &#39;osmdata&#39;, &#39;sf&#39;, &#39;ggmap&#39;, &#39;osmdata&#39;, &#39;osmdata&#39;, &#39;osmdata&#39;, &#39;ggmap&#39;) 19.1 Provided Services/Data What service/data is provided by the API? The Overpass API grants access to OpenStreetMap (OSM) data (“Overpass API” (2022)). OpenStreetMap is a project founded in 2004 that aims to create a free, open world map using their own data on streets, buildings, rivers, etc. (“FAQs” (2022)). This differs from Google Maps in that the “raw” geo data is provided, meaning that you can easily contribute to the project and tailor the maps (“FAQs” (2022)). The API thus allows you to select certain parts of the OSM data by entering a specific place or type of objects (“Overpass API” (2022)). Some additional services that utilize the API are (“Overpass API/Applications” (2022)): Achavi OSM Buildings Bicycle features CoinMap Opening_hours map 19.2 Prerequisites What’s needed to access the API? Because the Overpass API is open source, no API key or authentication procedures are needed. It should be noted that the main API server is limited in terms of data size and rate limits (“Overpass API” (2022)). The size of the data can only be known after completing the respective download. Thus, the general rule-of-thumb is that the API can most efficiently download the data of single geographical regions at a time, and data on country-sized regions should rather be obtained via planet.osm mirrors. Regarding rate limits, ca. 1,000,000 requests are allowed per day, and an even safer option is 10,000 queries or 5 GB max. of downloaded data per day. 19.3 Simple API Call You can use Overpass Turbo provided by Martin Raifer to test Overpass queries and view them in the interactive map. With the Wizard option, you can simply input the elements you are searching for and the corresponding code will be written and executed for you. For example, the default location of the Overpass Turbo is Rome, and by entering the term “Restaurant” into the Wizard, you will then see the code (displayed below) and map results for restaurants in Rome. /* This has been generated by the overpass-turbo wizard. The original search was: “restaurant” */ [out:json][timeout:25]; // gather results ( // query part for: “restaurant” node[&quot;amenity&quot;=&quot;restaurant&quot;]({{bbox}}); way[&quot;amenity&quot;=&quot;restaurant&quot;]({{bbox}}); relation[&quot;amenity&quot;=&quot;restaurant&quot;]({{bbox}}); ); // print results out body; &gt;; out skel qt; Or if you are already familiar with the query language, you can write your queries directly in the console. Here you also have the option to load, export, or share your data. Alternatively, it is recommended to use the Wget program. Click here for further details on how to write short and long queries using https. 19.4 API Access in R What does a simple API call look like in R? To access the data in R, the package osmdata is needed. This can be installed and loaded as follows: #install.packages(&quot;tidyverse&quot;) #install.packages(&quot;osmdata&quot;) #install.packages(&quot;sf&quot;) #install.packages(&quot;ggmap&quot;) library(tidyverse) library(osmdata) library(sf) library(ggmap) API queries via Overpass are made using the opq command. As shown below, the argument bbox needs to be specified. For this, you enter the area you want to analyze. In this case, we want to analyze the area of Mannheim, Germany, so we first want to find out the coordinates. The results of the coordinate shows that the degree of latitude is 49.4874592 and degree of longitude is 8.4660395. If you are unsure about the coordinates that you need for the call, you can simply enter the place you want to research into the getbb() command. The following code then returns the coordinates that you need for your analyses: library(osmdata) getbb(&quot;Mannheim&quot;) ## min max ## x 8.41416 8.58999 ## y 49.41036 49.59047 Since we are not interested in just one point, but an entire area, we specify with the usage of a vector that entails the minimum and maximum degrees of latitude and longitude. Using the command opq we build an Overpass Query that returns the data needed for the analyses. For the case of Mannheim, the command looks like this: # opq(bbox = c(minLongitude , minLatitude , maxLongitude , maxLatitude)) library(osmdata) Mannheim_data &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) # Mannheim, Germany To make the query, the addition of the command add_osm_feature is necessary. It refers to physical features on the ground (e.g., roads or buildings) using tags attached to its basic data structures. Each tag describes a geographic attribute of the feature shown by the specific node, way, or relation. It builds the basis of all following analyses. The argument key specifies the primary features that can be analyzed. It can take on the following terms: “Aerialway” “Aeroway” “Amenity” “Barrier” “Boundary” “Building” “Craft” “Emergency” “Geological” “Healthcare” “Highway” “Historic” “Landuse” “Leisure” “Man-made” “Military” “Natural” “Office” “Place” “Power” “Public Transport” “Railway” “Route” “Shop” “Sport” “Telecom” “Tourism” “Water” “Waterway” Value is the second argument that needs to be specified. It further defines the feature key and defines the kind of physical feature that is loaded with the key- argument. For example, we could be interested in restaurants in Mannheim. Restaurants are part of the general physical feature amenity. The following code returns all restaurants in Mannheim: Mannheim_restaurants &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Restaurant&quot;) A list of content is returned. At first glance, this data seems confusing because no single data frame is returned, but we instead receive nested data. However, the list obtained is crucial for further analyses and contains important information. We will transform the data set in the last part of this report and provide further insights into the data structure. Results can be further filtered by adding another value. If we want to filter and receive Italian restaurants, the value term Italian can be added. Restaurants with Italian in the name are then returned. Italian_restaurants &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Restaurant&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Italian&quot;) It is important to be aware that different languages may be represented in the data downloaded by the Overpass API. It could be, for instance, that a given Italian restaurant does not entail the English word Italian in its name, but rather the German or Italian terms. One could thus adjust the code in the following way: Italian_restaurants &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Restaurant&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = c(&quot;Italian&quot;, &quot;Italia&quot;, &quot;Italien&quot;, &quot;Italienisch&quot;)) Or via a longer way: Italian_restaurants &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Restaurant&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Italian&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Italia&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Italien&quot;) %&gt;% add_osm_feature(key = &#39;name&#39;, value = &quot;Italienisch&quot;) It is also possible to exclude certain values of a feature. This is done by adding an exclamation mark in front of the value. Wo_restaurants &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;!Restaurant&quot;) Moreover, one can also add and combine several requests. For example, we now search for restaurants and pubs: Restaurants_pubs &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Restaurant&quot;) %&gt;% add_osm_feature(key = &#39;amenity&#39;, value = &quot;Pub&quot;) Lastly, there is the option to combine via an OR operator. The following code returns restaurants or pubs: Restaurants_or_pubs &lt;- opq(bbox = getbb(&quot;Mannheim&quot;)) %&gt;% add_osm_feature(key = c (&quot;\\&quot;amenity\\&quot;=\\&quot;restaurant\\&quot;&quot;,&quot;\\&quot;amenity\\&quot;=\\&quot;pub\\&quot;&quot;)) Now that we covered the queries, we need to specify the conversion into either Simple Feature Objects (sf), Spatial Objects (sp), Silicate Objects (sc), or XML data. Simple Feature and Spatial Objects provide OSM components (points, lines, and polygons). osmdata_sf and osmdata_sq return the same data structure, with the exception that osmdata_sf returns data.frame for the spatial variable osm_lines, while osmdata_sq returns SpaitalLinesDataFrame. Silicate Objects represent the original OSM hierarchy of nodes, ways, and relations. It can convert between complex data types and is especially useful for exploratory aims. However, one needs to be careful using it. Finally, XML data can be produced. With the function osmdata_xml, raw data are produced and can be saved in XML format. We use the SF object function, because there is a preexisting geometry function for using the ggplot2 package. SF_Mannheim &lt;- osmdata_sf(Mannheim_restaurants) SF_Mannheim Using the ggmap package, we can visualize our results. First, we need the background map, which is in our case a map of Mannheim. For this, we use the get_map() function. There are more specifications that one can use with the function; such information can be obtained by searching for the function on R or the internet. To build the graph, we use the ggmap() function, including the object with our background map. In addition, we specify the points of the restaurants in Mannheim with geom_sf(). The argument inherit.aesneeds to be set to FALSE. Depending on our preferences, we can adjust the following settings: colour fill transparency (alpha) size shape (Royé (2018)). library(osmdata) library(ggmap) #our background map Mannheim_Map &lt;- get_map(getbb(&quot;Mannheim&quot;),maptype = &quot;toner-background&quot;) #final map ggmap(Mannheim_Map)+ geom_sf(data=SF_Mannheim$osm_points, inherit.aes =FALSE, colour=&quot;#238443&quot;, fill=&quot;#004529&quot;, alpha=.5, size=4, shape=21)+ labs(x=&quot;&quot;,y=&quot;&quot;) 19.5 Social Science Examples Are there any social science examples using the API? There is a relatively recent history of utilizing geodata in the social sciences. Ostermann, K. et al. (2022) claimed that today’s spatial research is limited by administrative divisions, e.g., districts and counties. Thus, a major advantage of geodata is its flexibility to be used outside of pre-determined boundaries. Ostermann, K. et al. (2022) applied geodata in labor market research to create a data set of employment biographies of the German working population from 2000 to 2017. They further demonstrated the potential of geodata both on the macro level, such as examining the effect of economic developments on regions, and on the micro scale, for instance determining neighborhood effects and patterns of segregation. Another line of studies combined geographical information with survey data, such as Hintze and Lakes (2009) who analyzed Germany’s Socio-Economic Panel (SOEP) data. They claimed that adding geodata is beneficial because it provides a complementary source of information, allows for an assessment of spatial patterns and non-spatial variables, and can be transformed into descriptive maps and scatter plots, among others. In the context of the SOEP, they not only used geodata to locating SOEP households but also research the economic and social components of specific areas. Hintze and Lakes (2009) further mentioned the potential of spatial indicators in answering research questions. For instance, households’ accessibility to local infrastructure could be measured by how close homes are to hospitals, schools, public transportation, and cultural infrastructure. According to Steinberg and Steinberg (2006), geodata and geographic information systems also have potential in policy-based fields, such as: Crime analysis Public health Public planning Social services Social change References “FAQs.” 2022. https://www.openstreetmap.de/faq.html#was_ist_osm. Hintze, P, and T. Lakes. 2009. “Geographically Referenced Data for Social Science.” Working {Paper} No. 125. German Council for Social; Economic Data (RatSWD). http://ssrn.com/abstract=1498449. Ostermann, K., Eppelsheimer, J., Gläser, N., Haller, P., and Oertel, M. 2022. “Geodata in Labor Market Research: Trends, Potentials, and Perspectives” 1 (115). “Overpass API.” 2022. https://wiki.openstreetmap.org/wiki/Overpass_API. “Overpass API/Applications.” 2022. https://wiki.openstreetmap.org/wiki/Overpass_API/Applications. Royé, Dominic. 2018. “Accessing OpenStreetMap Data with R.” https://www.r-bloggers.com/2018/11/accessing-openstreetmap-data-with-r/. Steinberg, S. J., and S. L. Steinberg. 2006. GIS: Geographic Information Systems for the Social Sciences: Investigating Space and Place. Thousand Oaks, London, New Delhi: SAGE Publications. "],["reddit-api.html", "Chapter 20 Reddit API 20.1 Provided services/data 20.2 Prerequisites 20.3 Simple API call 20.4 API access in R 20.5 Social science examples", " Chapter 20 Reddit API Domantas Undzėnas You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;kableExtra&#39;) 20.1 Provided services/data What data/service is provided by the API? The Reddit API is provided by Reddit itself. It allows data collection from subreddits13, threads14, users and many other interesting things! The data primarily come in text format. As such, you can use it for various text analyses. Data gathered also contains popularity of posts which allows you to test popularity of certain topics. In this review I will be focusing on describing the data you can gather from the API and not delving into complex analyses. The section on social science examples elaborates how Reddit data is used by researchers. 20.2 Prerequisites What are the prerequisites to access the API (authentication)? When the API was created it required users to have a Reddit account and use OAuth2 authentication. This required the user to create an application in their Reddit developer page. This would then generate a unique access token that users could use to gather data with various programming languages. As of 2022, users can still access their developer accounts and generate authentication keys, but this has become redundant. While the API de jure requires users to authenticate, R packages do not require any authentication or even creating an account to collect data from Reddit. This makes the authentication de facto not required if one wishes to access Reddit data. 20.3 Simple API call What does a simple API call look like? The Reddit API limits calls per user to one request per second. This may not allow one to collect lots of data during a short period of time. Fortunately data dumps such as this one exist to make your life easier. If you do want to collect your own data on Reddit, then let’s continue! A simple API call in R is possible using the httr package while having a link to the content that you want. Say you want to analyse a community dedicated to cats in Reddit. You just need to obtain the link to the subreddit and paste it into R. However, you should specify what structure of data you want. Let’s try to obtain JSON data - a format that is very popular for storing data structures. Specifying what data you want is very easy, as you just add a full stop and your data abbreviation to the already existing link. For example, the “r/cats” subreddit link is https://www.reddit.com/r/cats/. To obtain a JSON file you simply change the link to https://www.reddit.com/r/cats/.json. Let’s collect some data! One thing to keep in mind with all of these calls is that they change over time due to Reddit being an active platform. As you may be reading a year from the publication of this chapter, your calls will not be the same as mine. I have saved the datasets that I use for this chapter, so you can access them at any time, provided you load them in your working directory. # this is the url with the file extension you want to get # url &lt;- &#39;https://www.reddit.com/r/cats/.json&#39; # this is how you should use the httr function to get data # if you want to collect your own data, run this code # response &lt;- GET(url, user_agent(&#39;Extracting data from Reddit&#39;)) # this returns a list of infornmation about the subreddit # this saves the data # saveRDS(response, &#39;reddit_cats.RDS&#39;) # importing the data that I have saved cats &lt;- readRDS(&#39;data/reddit_cats.RDS&#39;) # exctracting the content from the response list cats &lt;- content(cats, type = &#39;application/json&#39;) This returns a big list (1.3 MB) of information about the “r/cats” subreddit. We can obtain information on the most popular post in the subreddit by extracting some of the data. In total, the 25 most popular posts will be extracted during your search. #extracting all the posts posts_data &lt;- cats$data$children #extracts data for the most popular post popular_post &lt;-posts_data[[1]]$data # extracts rverything we need from the list popular_data &lt;- c(popular_post$title, popular_post$ups, popular_post$upvote_ratio) # preparing data to be plotted popular_data &lt;- matrix(popular_data) # putting quotes on the title popular_data[1,1] &lt;- paste0(&#39;&quot;&#39;,popular_data[1,1],&#39;&quot;&#39;) rownames(popular_data) &lt;- c(&#39;post title&#39;, &#39;upvotes&#39;, &#39;upvote_ratio&#39;) colnames(popular_data) &lt;- &#39;Information&#39; # making a nice table library(kableExtra) kable(popular_data) %&gt;% kable_styling(position = &#39;center&#39;) Information post title “Sister took my cat because of financial things. Covid happened. First time seeing my old bud in quite a few years.” upvotes 21346 upvote_ratio 0.97 We can see that the most popular post in this subreddit is about a person who recently got to see his cat after the COVID pandemic. The post has a lot of upvotes (akin to likes on platforms like Facebook or Twitter) as well as having 97% of the people who interacted with the post upvoting it. It seems that people generally like seeing reunions with pets. 20.4 API access in R How can we access the API from R (httr + other packages)? Gathering data with the httr package is definitely possible, but requires a lot of further work to process needed data. If you don’t want to write massive chunks of code, you can turn to people who have already done that! Here we use the RedditExtractoR package developed by Rivera (2022) to gather data more easily. Let’s begin with a simple call to find all the subreddits that use the word politics in their subreddit. The find_subreddits() will look for communities that use the word in question in its name, description or the various threads that users post. # this gets us all the subreddits that use the word politics # subred &lt;- find_subreddits(&#39;politics&#39;) # you can use this to collect your own data # this saves the data you have gathered for the analysis # saveRDS(subred, &#39;reddit_subreddits.RDS&#39;) Now that we have the subreddits, let’s visualise ten of those that have the highest amount of subscribers and use the word politics subreddit description or discussions. # this function imports the data I have collected subreddits &lt;- readRDS(&#39;data/reddit_subreddits.RDS&#39;) # this cleans the data to be more presentable subreddits &lt;- subreddits %&gt;% select(subreddit, title, subscribers) %&gt;% # selects variables you want to explore mutate(subscribers_million = subscribers/1000000, subscribers = NULL, title = paste0(&#39;&quot;&#39;,title,&#39;&quot;&#39;)) %&gt;% # creates new variables arrange(desc(subscribers_million)) # arranges data from highest subscriber count # this removes the unique id of each subreddit rownames(subreddits) &lt;- NULL # making a nice table to display our results kable(head(subreddits, 10)) %&gt;% kable_styling(position = &#39;center&#39;) subreddit title subscribers_million AskReddit “Ask Reddit…” 35.187653 worldnews “World News” 28.234964 memes “/r/Memes the original since 2008” 17.964003 AdviceAnimals “Advice Animals” 9.458493 politics “Politics” 7.974496 europe “Europe” 3.039875 teenagers “r/teenagers” 2.752169 atheism “atheism” 2.709896 unpopularopinion “For your Opinions that are Unpopular” 2.365810 PrequelMemes “PrequelMemes - Memes of the Star Wars Prequels” 2.003999 We have our usual suspects here. Of course the people discussing world news, politics and Europe are talking about politics! But then we can also see that teenager, atheist, unpopular opinion communities and even people who make memes are talking about politics. However, this “talking” may not always mean that users actively discuss politics on the subreddit. The unpopular opinion, meme and prequel meme subreddits explicitly put a “no politics” clause in their community rules. Even though the word politics is used here only once, the find_subreddits() function still returns the subreddits. Researchers should be careful when using the function to make sure they don’t stumble onto subreddits that explicitly ban the keyword they are interested in. Okay, let’s say we want to find out what is going on in one of these communities. To make this analysis interesting, let’s see what unpopular opinions Reddit users have. # this code is for collecting unpopular opinions yourself # opinions &lt;- find_thread_urls(subreddit = &#39;unpopularopinion&#39;, #specifies the subreddit # sort_by = &#39;top&#39;, #specifies what posts we are looking for # period = &#39;week&#39;) #specifies the period you want to search for # saveRDS(opinions, file = &#39;reddit_unpopular_opinions.RDS&#39;) # this code was used to save the top threads that I collected when writing of this chapter Let’s dive right in! The full dataset contains time stamps, titles, full text, url to the post and much more. Here we are mostly interested in the titles and engagement with the individual posts (measured in number of comments). # this loads my data again unpop_opinions &lt;- readRDS(&#39;data/reddit_unpopular_opinions.RDS&#39;) # and cleans the data unpop_opinions &lt;- unpop_opinions %&gt;% select(title, comments) %&gt;% # selects variables we want mutate(title = paste0(&#39;&quot;&#39;,title,&#39;&quot;&#39;)) %&gt;% # adds quotation marks to title arrange(desc(comments)) # arranges data by larges comment count rownames(unpop_opinions) &lt;- NULL # making a nice table again kable(head(unpop_opinions, 10)) %&gt;% kable_styling(position = &#39;center&#39;) title comments “5 years later, on St Patrick’s Day, I return to tell Americans that they aren’t Irish” 8885 “Kinkshaming should be encouraged” 3029 “It is impossible to be super fit and work a 9-6 job.” 2758 “Can you explain this gap in your resume? Is totally rude and completely inappropriate to ask”     22 Are these opinions really unpopular? You be the judge. The find_thread_urls() function also allows you to search for threads based on a keyword. While it is useful to highlight what is being talked about right now regarding a subreddit or keyword, you can also use to gather data for generating word clouds, sentiment analysis and much more! Let’s say you are not interested in a specific community or keyword but a user. The package has that covered too. If you don’t have anyone in mind, Reddit has a list of it’s users here. For this chapter we will explore the user profile of former California senator Arnold Schwarzenegger. You do not need to limit yourself to only one person, however, as the package allows you to gather information about multiple users at once. # this collects data about a single user # arnold &lt;- get_user_content(&#39;GovSchwarzenegger&#39;) # this function will collect data for two users, you can add as many as you want # arnold_nasa &lt;- get_user_content(c(&#39;GovSchwarzenegger&#39;, &#39;NASA&#39;)) # saving data # saveRDS(arnold, file = &#39;reddit_arnold.RDS&#39;) This returns a list. It contains basic information about the user as well lists for comments and threads they have posted. Since Arnold Schwarzenegger’s profile is very popular and active in Reddit, the list will be quite large (1.1 MB). It is important to keep this in mind if you plan to collect data about many popular user accounts. # loading the data I collected for Arnold&#39;s account my_arnold &lt;- readRDS(&#39;data/reddit_arnold.RDS&#39;) # this is a vector of descriptions about the account about &lt;- unlist(my_arnold$GovSchwarzenegger$about) # this is a data frame of all of the user&#39;s comments arnold_comments &lt;- my_arnold$GovSchwarzenegger$comments # this is a data frame of all of the threads that the user has started arnold_threads &lt;- my_arnold$GovSchwarzenegger$threads Let’s check the most popular (measured by upvotes minus downvotes) thread names the former senator has created and in which subreddits he posted them. # cleaning the dataset arnold_threads &lt;- arnold_threads %&gt;% select(subreddit, title, score) %&gt;% # gets the variables we want to use mutate(title = paste0(&#39;&quot;&#39;,title,&#39;&quot;&#39;)) %&gt;% # adds quotation marks to the titles arrange(desc(score)) # arranges the data from with the highest score at the top rownames(arnold_threads) &lt;- NULL # a table for viewing where Arnold posts kable(head(arnold_threads, 10)) %&gt;% kable_styling(position = &#39;center&#39;) subreddit title score aww “Meet the newest member of the family, Dutch!” 266522 aww “Say hello to Schnitzel” 178734 movies “Terminator came out 35 years ago - here are some of my personal behind the scenes shots” 118233 movies “Finally filming Kung Fury 2” 96316 MasterReturns “Dutch was very happy when I got back from Cleveland last night” 75899 movies “An original storyboard from The Terminator by Jim Cameron” 68475 movies “Were back. Heres your Terminator: Dark Fate trailer that doesnt give the movie away.”  67 Cool! So Arnold’s top threads are in subreddits about cute things, movies and bodybuilding. For those who are interested, this is Dutch, the newest addition to Arnold’s family. Figure 1: Dutch 20.5 Social science examples Are there social science research examples using the API? In the timespan from 2010 and 2020 there have been 727 manuscripts analysing Reddit (Proferes et al. 2021). 338 of them are journal articles. Out of these journal aritlces, around 23% have been published in social science journals. It seems that Reddit is mostly studied by sociologists and psychologists with political scientists only boasting 3 articles on Reddit during a 10 year period. Specific examples include Apostolou (2019) using a Reddit thread about self-reported reasons why men stay single to explore the topic through an evolutionary psychology lens. The author finds that among the most prevalent reasons that men stay single are poor flirting skills, low self-confidence and poor looks. It is argued that the change societal patterns of finding a mate for men has changed significantly over the past centuries. From pre-industrial societies where men gained access to women through conquest or through arranged marriages by parents, traits like flirting skills, looks and confidence were not important. In modern societies, however, where people are free to choose their spouses, these traits play a much bigger role. Reddit is also used for research into the extreme right. For example Gaudette et al. (2021) perform an analysis on a sample of most popular comments compared to random comments sampled from the The_Donald subreddit. The authors show that the top comments in the subreddit paint a strongly negative view and sometimes advocate violence towards Muslims and left-wingers as opposed to a random sample of comments from the subreddit. The authors argue that this encourages a collective identity of the subreddit that calls for violence against minorities and political opponents of right-wingers. Chipidza et al. (2022) discusses the reliability of news content being discussed in regards to the COVID pandemic among conservatives and liberals in the US. The authors find that liberal subreddits discuss topics related to Trump, the White House and economic relief topics. For conservatives the primary topics of discussion are China and deaths caused by COVID. Additionally, liberal subreddits contain more articles from credible news sources compared to conservative subreddits. References Apostolou, Menelaos. 2019. “Why Men Stay Single? Evidence from Reddit.” Evolutionary Psychological Science 5 (1): 87–97. https://doi.org/10.1007/s40806-018-0163-7. Chipidza, Wallace, Christopher Krewson, Nicole Gatto, Elmira Akbaripourdibazar, and Tendai Gwanzura. 2022. “Ideological Variation in Preferred Content and Source Credibility on Reddit During the COVID-19 Pandemic.” Big Data &amp; Society 9 (1): 20539517221076486. https://doi.org/10.1177/20539517221076486. Gaudette, Tiana, Ryan Scrivens, Garth Davies, and Richard Frank. 2021. “Upvoting Extremism: Collective Identity Formation and the Extreme Right on Reddit.” New Media &amp; Society 23 (12): 3491–3508. https://doi.org/10.1177/1461444820958123. Proferes, Nicholas, Naiyan Jones, Sarah Gilbert, Casey Fiesler, and Michael Zimmer. 2021. “Studying Reddit: A Systematic Overview of Disciplines, Approaches, Methods, and Ethics.” Social Media + Society 7 (2): 205630512110190. https://doi.org/10.1177/20563051211019004. Rivera, Ivan. 2022. “RedditExtractoR: Reddit Data Extraction Toolkit.” https://CRAN.R-project.org/package=RedditExtractoR. Communities focusing on a specific topic within Reddit like cats↩︎ Posts about a specific topic in a subreddit↩︎ "],["spotify-api.html", "Chapter 21 Spotify API 21.1 Provided services/data 21.2 Prerequisites 21.3 Simple API call 21.4 API access in R 21.5 Social science examples 21.6 References", " Chapter 21 Spotify API Johanna Hölzl, Marie-Lou Sohnius You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;spotifyr&#39;, &#39;tidyverse&#39;, &#39;lubridate&#39;) library(pacman) pacman::p_load(&#39;spotifyr&#39;, # To access the API &#39;tidyverse&#39;, # Data wrangling and plots &#39;plotly&#39;, # Interactive plots &#39;ggimage&#39;, # Adding album covers to ggplot &#39;kableExtra&#39;,# Format tables &#39;httpuv&#39;, # To be able to access the Spotify URL &#39;httr&#39;) # In case you want to access the API w/o # the package 21.1 Provided services/data What data/service is provided by the API? The Spotify Web API allows you to pull data from the platform on listed artists, albums, tracks, and playlists. Possible requests include getting information on track audio features (e.g., danceability, score, or pace) as well as popularity metrics of single tracks and albums. Beyond these general query options, you can also collect data on individual users’ (including your own) listening behavior. Accessing personal information, however, depends on users’ consent. 21.2 Prerequisites What are the prerequisites to access the API (authentication)? 21.2.1 Authentication To access the Spotify API, you need to have a Spotify account. Don’t have one yet? Then sign up for free here! It does not matter whether you have a Premium account or not. Once you’re ready to use your Spotify account, you can set up a developer account to access the Spotify Web API. With the developer access, you can create new integrations and manage your Spotify credentials. Once you have a developer account, you will need to create an app on the dashboard page. Figure 1: Create the app on the dashboard (Screenshot from the Spotify for Developers page) 21.2.2 Authorization code flow If you want to access information on your own account (e.g. your favorite artists, your playlists), you need to complete one additional step: Go to the app you created in the Spotify API dashboard, go to edit settings and add a Redirect URI. A recommended option for your Redirect URI is http://localhost:1410/. See Spotify’s developer guide for more information. Figure 2: Open settings of the app (Screenshot from the Spotify for Developers page) Figure 3: Add the Redirect URI (http://localhost:1410/) to the settings (Screenshot from the Spotify for Developers page) 21.2.3 Savely storing your credentials in the R environment Via the app you created, you receive your client ID and client secret. You can find them on the app page in your Spotify developer account. Save both credentials in the R environment. For accessing your personal data, also add the redirect URI (the same one you added in your app’s settings!): # Here you can store the credentials as follows: # Sys.setenv(SPOTIFY_CLIENT_ID=&quot;xxx&quot;) # # Sys.setenv(SPOTIFY_CLIENT_SECRET=&quot;xxx&quot;) # # Sys.setenv(SPOTIFY_REDIRECT_URI=&quot;http://localhost:1410/&quot;) # # Beware: The credential locals are case-sensitive, thus must be stored # exactly as above to work correctly with the spotifyr package. access_token &lt;- get_spotify_access_token() # Stores your client ID in a #local object 21.3 Simple API call What does a simple API call look like? In this chapter, we solely focus on how to make API calls via the corresponding spotifyr package. There also exists the option to access the Spotify API using the more common httr package. The latter option is more complicated than using the customized spotifyr package, though, as the manual authentication process is more difficult to implement. If you are interested in making API calls without the package, have a look at this detailed guide by Ray Heberer. 21.4 API access in R How can we access the API from R (httr + other packages)? Instead of typing the API request into our browser or using the httr package, we can use the spotifyr package to easily access the API from R. Note: For information on all possible queries available through the spotifyr wrapper, see the online documentation or this detailed introduction to the package which also informed this chapter. You can also check the R built-in help function on the package: library(spotifyr) ?spotifyr 21.4.1 Playlist features So let’s submit our first query to the API! In this example, we are interested in the features of current global top 50 tracks. To get this information, we first retrieve the Spotify playlist ID by opening the playlist we want to analyze in the browser and then copying the id part from the link. In our example of the Spotify global top 50 playlist: Playlist link: https://open.spotify.com/playlist/37i9dQZEVXbMDoHDwVN2tF Playlist ID: 37i9dQZEVXbMDoHDwVN2tF Now that we have the ID, we can retrieve all information on tracks in the playlist by calling the function get_playlist_audio_features. # Store the data in a dataframe top50 &lt;- get_playlist_audio_features(playlist_uris = &#39;37i9dQZEVXbMDoHDwVN2tF&#39;) # Global Top 50 # Add the tracks&#39; rank to the dataset: # the data comes sorted as listed in the playlist but does not contain a # specific variable indicating the rank. Therefore, we create a new # variable that contains the rank in ascending order, ranging from 1 to # 50. top50$rank &lt;- seq.int(nrow(top50)) # So far, so good. Looking at the data, artist names are currently stored # in lists. # The next snippet moves artist names into a new variable for easier # access. Also, we add the album cover link to a new variable image to # plot the covers later. for (i in 1:50) { top50$artist[i] &lt;- top50[[28]][[i]]$name top50$image[i] &lt;- c(top50[[49]][[i]]$url[2], size=10, replace = TRUE) } # Now that we have the data set ready to go, let&#39;s take a look at what # variables are in there. names(top50) %&gt;% kbl() %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;)) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;300px&quot;) x playlist_id playlist_name playlist_img playlist_owner_name playlist_owner_id danceability energy key loudness mode speechiness acousticness instrumentalness liveness valence tempo track.id analysis_url time_signature added_at is_local primary_color added_by.href added_by.id added_by.type added_by.uri added_by.external_urls.spotify track.artists track.available_markets track.disc_number track.duration_ms track.episode track.explicit track.href track.is_local track.name track.popularity track.preview_url track.track track.track_number track.type track.uri track.album.album_group track.album.album_type track.album.artists track.album.available_markets track.album.href track.album.id track.album.images track.album.is_playable track.album.name track.album.release_date track.album.release_date_precision track.album.total_tracks track.album.type track.album.uri track.album.external_urls.spotify track.external_ids.isrc track.external_urls.spotify video_thumbnail.url key_name mode_name key_mode rank artist image In the next step, we want to take a closer look at track popularity. That is, how does a track’s rank on the top 50 playlist correlate with Spotify’s popularity measure? Note that the index is calculated by Spotify not solely based on a track’s recent stream count, but also taking other metrics into account. Beyond, we’ll have a look at more fun features such as a track’s danceability and valence (happiness). top50 %&gt;% select(rank, track.name, artist, track.popularity, danceability, valence) %&gt;% kbl() %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;)) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;300px&quot;) rank track.name artist track.popularity danceability valence 1 Die For You - Remix The Weeknd 92 0.531 0.5020 2 TQG KAROL G 92 0.720 0.6070 3 Flowers Miley Cyrus 100 0.707 0.6460 4 Kill Bill SZA 93 0.644 0.4180 5 Boy’s a liar Pt. 2 PinkPantheress 94 0.696 0.8570 6 As It Was Harry Styles 89 0.520 0.6620 7 Shakira: Bzrp Music Sessions, Vol. 53 Bizarrap 96 0.778 0.4980 8 Creepin’ (with The Weeknd &amp; 21 Savage) Metro Boomin 94 0.715 0.1720 9 Yandel 150 Yandel 85 0.783 0.5800 10 Calm Down (with Selena Gomez) Rema 92 0.801 0.8020 11 Unholy (feat. Kim Petras) Sam Smith 84 0.712 0.2060 12 Tormenta (feat. Bad Bunny) Gorillaz 85 0.637 0.2970 13 Shorty Party Cartel De Santa 79 0.928 0.4700 14 La Bachata Manuel Turizo 93 0.835 0.8500 15 I’m Good (Blue) David Guetta 93 0.561 0.3040 16 Die For You The Weeknd 87 0.586 0.5080 17 Last Night Morgan Wallen 77 0.492 0.4780 18 Anti-Hero Taylor Swift 91 0.637 0.5330 19 Escapism. RAYE 78 0.538 0.2500 20 Here With Me d4vd 91 0.574 0.2880 21 OMG NewJeans 91 0.804 0.7390 22 Hey Mor Ozuna 89 0.901 0.3990 23 Until I Found You (with Em Beihold) - Em Beihold Version Stephen Sanchez 90 0.551 0.3420 24 Starboy The Weeknd 89 0.679 0.4860 25 Another Love Tom Odell 60 0.442 0.1390 26 X SI VOLVEMOS KAROL G 81 0.794 0.5750 27 golden hour JVKE 88 0.515 0.1530 28 La Jumpa Arcángel 85 0.887 0.1970 29 Players Coi Leray 89 0.954 0.6240 30 Quevedo: Bzrp Music Sessions, Vol. 52 Bizarrap 91 0.621 0.5500 31 I Wanna Be Yours Arctic Monkeys 90 0.464 0.4790 32 I Ain’t Worried OneRepublic 45 0.697 0.8220 33 AMG Natanael Cano 89 0.772 0.7860 34 ceilings Lizzy McAlpine 86 0.516 0.2610 35 Snooze SZA 89 0.559 0.3920 36 Feliz Cumpleaños Ferxxo Feid 86 0.865 0.5630 37 Sure Thing Miguel 90 0.684 0.4980 38 PRC Peso Pluma 88 0.784 0.8930 39 Bones Imagine Dragons 83 0.772 0.5870 40 Just Wanna Rock Lil Uzi Vert 89 0.486 0.0385 41 on the street (with J. Cole) j-hope 83 0.681 0.8130 42 Ditto NewJeans 89 0.814 0.1830 43 Mockingbird Eminem 89 0.637 0.2540 44 Under The Influence Chris Brown 89 0.733 0.3100 45 Me Porto Bonito Bad Bunny 89 0.911 0.4250 46 Superhero (Heroes &amp; Villains) [with Future &amp; Chris Brown] Metro Boomin 88 0.526 0.4920 47 Bebe Dame Fuerza Regida 80 0.529 0.7560 48 Blinding Lights The Weeknd 88 0.514 0.3340 49 Rich Flex Drake 89 0.561 0.4240 50 Watch This - ARIZONATEARS Pluggnb Remix Lil Uzi Vert 85 0.686 0.3550 Let’s plot this data! f1 &lt;- ggplot(data = top50, aes(x = track.popularity, y = rank, text = ( paste( &quot;Track:&quot;, track.name, &quot;&lt;br&gt;&quot;, &quot;Artist:&quot;, artist, &quot;&lt;br&gt;&quot;, &quot;Release date:&quot;, track.album.release_date ) ))) + geom_point() + theme_minimal() + ylab(&quot;Playlist rank&quot;) + xlab(&quot;Popularity&quot;) # This code snippet creates an interactive version of our plot that allows you to # hover over each data point to receive more information. ggplotly(f1, tooltip = c(&quot;text&quot;)) We can see in the graph above that tracks in the top 50 playlist are definitely rather on the popular side, however, some tracks have a comparably low popularity score. When you look at the interactive plotly graph, you’ll be able to identify the outlier that ranks below 70 on Spotify’s the popularity scale despite being in the charts: Another Love by Tom Odell (2013), While the exact estimation of the score is confidential, there exists evidence that the age of a track factors into its popularity score. That way, two tracks with 100,000 streams can have different popularity scores dependent on when they were released. In the algorithm’s logic, the more recent track gained the same number of streams in a shorter time and is therefore evaluated as more popular. Applying this to our outliers, we can see that the age of both tracks likely affects their low popularity score. Spotify also provides you with album cover links in varying sizes, so why not use the covers instead of black scatterpoints in a plot? Before we used geom_point for the scatterplot, now we simply need to replace that command with geom_image and specify the variable containing the image link. In the following plot, we explore the correlation between track happiness and danceability. ggplot(data = top50, aes(x = valence, y = danceability, text = ( paste( &quot;Track:&quot;, track.name, &quot;&lt;br&gt;&quot;, &quot;Artist:&quot;, artist ) ))) + geom_image(aes(image=image), asp = 1.7) + theme_minimal() + ylab(&quot;Danceability&quot;) + xlab(&quot;Happiness&quot;) 21.4.2 Your Spotify data You can also analyze your personal listening behavior with the Spotify API. For example, this snippet using the get_my_top_artists_or_tracks function allows you to explore your favorite artists of all time. ## Finding all time favorite artists topartists &lt;- get_my_top_artists_or_tracks(type = &#39;artists&#39;, time_range = &#39;long_term&#39;, limit = 50) %&gt;% select(name, genres) %&gt;% rowwise %&gt;% mutate(genres = paste(genres, collapse = &#39;, &#39;)) %&gt;% ungroup topartists$rank &lt;- seq.int(nrow(topartists)) # add rank variable topartists %&gt;% select(rank, name, genres) %&gt;% kbl() %&gt;% kable_styling(bootstrap_options = c(&quot;hover&quot;)) %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;300px&quot;) You can retrieve your all-time favourite tracks by still using the get_my_top_artists_or_tracks function but changing type= to tracks. As you have seen, the Spotify API opens up many opportunities for data analysis. The functionality of the Spotifyr wrapper goes beyond the simple examples demonstrated here. Now it’s your time to explore the data! 21.5 Social science examples Are there social science research examples using the API? So far, not much has been done with the Spotify API and music data from the field of social sciences (this is where you could step in!). Some notable exceptions below: MacTaggart (2018) and Lacognata and Poole (2021) both looked at the link between music and politics. At an aggregate level, MacTaggart (2018) examined the association of chart trends within pop music and trends within politics in the US from 1959 to 2016. The author finds that musical trends reflect trends in society and politics. Lacognata and Poole (2021) investigated a potential correlation of political orientation, personality traits, and music taste. The authors used individual-level survey data and linked these data to respondents’ Spotify accounts. However, they did not find any association between music taste and neither political orientation nor personality traits. In a recent poster presentation, Song, Chun, and Elkins (2021) examined how the Covid-19 pandemic affected emotion-driven listening behavior with the help of the Spotify AP. The authors used monthly Spotify chart data from December 2019 to December 2021 and extracted measures on the valence, energy, and danceability of the tracks (we already know these measures from our own example on the global top 50 tracks). The authors did not find any correlation of tracks’ energy and danceability with Covid-19 related events. At the same time, Song et al did find that the tracks’ valence reflects the course of the pandemic: When the Covid-19-vaccine got distributed, people listened to happier music, while when the news of the Omicron variant spread, less happy tracks became more popular. For more inspiration, you can also check out Spotify’s Developer Showcase website! What will you do with the Spotify API? 21.6 References References Lacognata, Anthony, and Jennifer Poole. 2021. “The Melodies of Politics: Assessing a Correlation Between Music Taste and Political Views with Spotify.” Journal of Student Research 10 (November). https://doi.org/10.47611/jsrhs.v10i3.1898. MacTaggart, Andrew. 2018. “Pop Music and Politics: Tracking Political Trends Through the Hot 100 Chart, 1959-2016 - ProQuest.” Harvard University. https://www.proquest.com/docview/2487180443?fromopenview=true&amp;pq-origsite=gscholar&amp;parentSessionId=PiWLVy66xd%2FXPi84svQkAEg0ze49z52W1qhA3%2BBQlVs%3D. Song, Hemmi, Jon Chun, and Katherine Elkins. 2021. “Tracking the Emotion of Music Across the Covid-19 Pandemic.” In. https://doi.org/10.3109/10673229.2011.549769. "],["twitter-api.html", "Chapter 22 Twitter API 22.1 Provided services/data 22.2 Prerequisites 22.3 Simple API call 22.4 API access in R 22.5 Social science examples 22.6 Addendum: Twitter v1.1", " Chapter 22 Twitter API Chung-hong Chan You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;academictwitteR&#39;, &#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;dplyr&#39;, &#39;rtweet&#39;) 22.1 Provided services/data What data/service is provided by the API? The API is provided by Twitter. As of 2021, there are 5 different tracks of API: Standard (v1.1), Premium (v1.1), Essential (v2), Elevated (v2), and Academic Research (v2). They offer different data as well as cost differently. For academic research, one should use Standard (v1.1) or Academic Research (v2). I recommend using Academic Research (v2) track, not least because v1.1 is very restrictive for academic research and the version is now in maintenance mode (i.e. it will soon be deprecated). If one still wants to use the Standard track v1.1, please see the addendum below. Academic Research Track provides the following data access Full archive search of tweets Tweet counts User lookup Compliance check (e.g. whether a tweet has been deleted by Twitter) and many other. 22.2 Prerequisites What are the prerequisites to access the API (authentication)? One needs to have a Twitter account. To obtain Academic Research access, one needs to apply it from here. In the application, one will need to provide a research profile (e.g. Google Scholar profile or a link to the profile in the student directory) and a short description about the research project that would be done with the data obtained through the API. Twitter will then review the application and grant the access if appropriate. For undergraduate students without a research profile, Twitter might ask for endorsement from academic supervisors. With the granted access, a bearer token is available from the dashboard of the developer portal. For more information about the entire process, please read this vignette of academictwitteR. It is recommended to set the bearer token as the environment variable TWITTER_BEARER. Please consult Chapter 2 on how to do that in the section on Environment Variables replacing MYSECRET=ROMANCE in the example with TWITTER_BEARER=YourBearerToken. 22.3 Simple API call What does a simple API call look like? The documentation of the API is available here. The bearer token obtained from Twitter should be supplied as an HTTP header preceding with “Bearer”, i.e. ## You should get &quot;bearer YourBearerToken&quot; paste0(&quot;bearer &quot;, Sys.getenv(&quot;TWITTER_BEARER&quot;)) For using the full archive search and tweet counts endpoints, one needs to build a search query first. For example, to search for all German tweets with the hashtags “#ichbinhanna” or “#ichwarhanna”, the query looks like so: #ichbinhanna OR #ichwarhanna lang:DE. To make a call using httr to obtain tweets matched the above query from 2021-01-01 to 2021-07-31 library(httr) my_query &lt;- &quot;#ichbinhanna OR #ichwarhanna lang:DE&quot; endpoint_url &lt;- &quot;https://api.twitter.com/2/tweets/search/all&quot; params &lt;- list( &quot;query&quot; = my_query, &quot;start_time&quot; = &quot;2021-01-01T00:00:00Z&quot;, &quot;end_time&quot; = &quot;2021-07-31T23:59:59Z&quot;, &quot;max_results&quot; = 500 ) r &lt;- httr::GET(url = endpoint_url, httr::add_headers( Authorization = paste0(&quot;bearer &quot;, Sys.getenv(&quot;TWITTER_BEARER&quot;))), query = params) httr::content(r) If one is simply interested in the time series data, just make a simply change to the endpoint and some parameters. params &lt;- list( &quot;query&quot; = my_query, &quot;start_time&quot; = &quot;2021-01-01T00:00:00Z&quot;, &quot;end_time&quot; = &quot;2021-07-31T23:59:59Z&quot;, &quot;granularity&quot; = &quot;day&quot; ## obtain a daily time series ) endpoint_url &lt;- &quot;https://api.twitter.com/2/tweets/counts/all&quot; r &lt;- httr::GET(url = endpoint_url, httr::add_headers( Authorization = paste0(&quot;bearer &quot;, Sys.getenv(&quot;TWITTER_BEARER&quot;))), query = params) httr::content(r) 22.4 API access in R How can we access the API from R (httr + other packages)? The package academictwitteR (Barrie and Ho 2021) can be used to access the Academic Research Track. In the following example, the analysis by Haßler et al. (2021) is reproduced. The research question is: How has the number of tweets published with the hashtag #fridaysforfuture been affected by the lockdowns? (See time series in the Figure 3 of the paper) The study period is 2019-06-01 to 2020-05-31 and restricted to only German tweets. The original analysis was done with the v1.1 API. But one can get a better dataset with the Academic Research Track. academictwitteR looks for the environment variable TWITTER_BEARER for the bearer token. To collect the time series data, we use the count_all_tweets() function. library(academictwitteR) library(tidyverse) library(lubridate) fff_ts &lt;- count_all_tweets(query = &quot;#fridaysforfuture lang:DE&quot;, start_tweets = &quot;2019-06-01T00:00:00Z&quot;, end_tweets = &quot;2020-05-31T00:00:00Z&quot;, granularity = &quot;day&quot;, n = Inf) head(fff_ts) # the data is in reverse chronological order ## end start tweet_count ## 1 2020-05-01T00:00:00.000Z 2020-04-30T00:00:00.000Z 366 ## 2 2020-05-02T00:00:00.000Z 2020-05-01T00:00:00.000Z 345 ## 3 2020-05-03T00:00:00.000Z 2020-05-02T00:00:00.000Z 407 ## 4 2020-05-04T00:00:00.000Z 2020-05-03T00:00:00.000Z 244 ## 5 2020-05-05T00:00:00.000Z 2020-05-04T00:00:00.000Z 465 ## 6 2020-05-06T00:00:00.000Z 2020-05-05T00:00:00.000Z 483 The daily time series of number of German tweets tweets is displayed in Figure below. However, this is not a perfect replication of the original Figure 3. It is because the original Figure only considers Fridays. 15 The Figure below is an “enhanced remake”. library(dplyr) lockdown_date &lt;- as.POSIXct(as.Date(&quot;2020-03-23&quot;)) fff_ts %&gt;% mutate(start = ymd_hms(start)) %&gt;% select(start, tweet_count) %&gt;% ggplot(aes(x = start, y = tweet_count)) + geom_line() + geom_vline(xintercept = lockdown_date, color = &quot;red&quot;, lty = 2) + xlab(&quot;Date&quot;) + ylab(&quot;Number of German #fridaysforfuture tweets&quot;) 22.5 Social science examples Are there social science research examples using the API? There are so many (if not too many) social science research examples using this API. Twitter data have been used for network analysis, text analysis, time series analysis, just to name a few. If one is really interested in examples using the API, on Google Scholar there are around 1,720,000 results (as of writing). Some scholars link the abundance of papers using Twitter data to the relative openness of the API. Burgess and Bruns (2015) label these Twitter data as “Easy Data”. This massive overrepresentation of easy Twitter data in the literature draws criticism from some social scientists, especially communication scholars. Matamoros-Fernández and Farkas (2021), for example, see this as problematic and the overrepresentation of Twitter in the literature “mak[es] all other platforms seem marginal.” (p. 215) Ironically, in many countries the use of Twitter is marginal. According to the Digital News Report (Newman et al. 2021), only 12% of the German population uses Twitter, which is much less than YouTube (58%), Facebook (44%), Instagram (29%), and even Pinterest (16%). I recommend thinking thoroughly whether the easy Twitter data are really suitable for answering one’s research questions. If Twitter data are really needed to use, consider also alternative data sources for comparison if possible (e.g. Rauchfleisch, Siegen, and Vogler 2021). 22.6 Addendum: Twitter v1.1 The Standard Track of Twitter v1.1 API is still available and probably will still be available in the near future. If one for any reason doesn’t want to — or cannot — use the Academic Research Track, Twitter v1.1 API is still accessible using the R package rtweet (Kearney 2019). The access is relatively easy because in most cases, one only needs to have a Twitter account. Before making any actual query, rtweet will do the OAuth authentication automatically 16. To “replicate” the #ichbinhanna query above: library(rtweet) search_tweets(&quot;#ichbinhanna OR #ichwarhanna&quot;, lang = &quot;de&quot;) However, this is not a replication. It is because the Twitter v1.1 API can only be used to search for tweets published in the last few days, whereas Academic Research Track supports full historical search. If one wants to collect a complete historical archive of tweets with v1.1, continuous collection of tweets is needed. References Barrie, Christopher, and Justin Ho. 2021. “academictwitteR: An r Package to Access the Twitter Academic Research Product Track V2 API Endpoint.” Journal of Open Source Software 6 (62): 3272. https://doi.org/10.21105/joss.03272. Burgess, Jean, and Axel Bruns. 2015. “Easy Data, Hard Data: The Politics and Pragmatics of Twitter Research After the Computational Turn.” Compromised Data: From Social Media to Big Data. Bloomsbury Academic New York, NY. Haßler, Jörg, Anna-Katharina Wurst, Marc Jungblut, and Katharina Schlosser. 2021. “Influence of the Pandemic Lockdown on Fridays for Future’s Hashtag Activism.” New Media &amp; Society, 14614448211026575. https://doi.org/https://doi.org/10.1177/14614448211026575. Kearney, Michael. 2019. “Rtweet: Collecting and Analyzing Twitter Data.” Journal of Open Source Software 4 (42): 1829. https://doi.org/10.21105/joss.01829. Matamoros-Fernández, Ariadna, and Johan Farkas. 2021. “Racism, Hate Speech, and Social Media: A Systematic Review and Critique.” Television &amp; New Media 22 (2): 205–24. https://doi.org/http://dx.doi.org/10.1177/1527476420982230. Newman, Nic, Richard Fletcher, Anne Schulz, Simge Andi, Craig T Robertson, and Rasmus Kleis Nielsen. 2021. “Reuters Institute digital news report 2021.” Rauchfleisch, Adrian, Dario Siegen, and Daniel Vogler. 2021. “How COVID-19 Displaced Climate Change: Mediated Climate Change Activism and Issue Attention in the Swiss Media and Online Sphere.” Environmental Communication, November, 1–9. https://doi.org/10.1080/17524032.2021.1990978. To truly answer the research question, I recommend using time series causal inference techniques such as Google’s CausalImpact.↩︎ The authentication information will be stored by default to the hidden file .rtweet_token.rds in home directory.↩︎ "],["mediawiki-action-api.html", "Chapter 23 MediaWiki Action API 23.1 Provided services/data 23.2 Prerequisites 23.3 Simple API call 23.4 API access in R 23.5 Social science examples", " Chapter 23 MediaWiki Action API Noam Himmelrath, Jacopo Gambato You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;WikipediR&#39;, &#39;rvest&#39;, &#39;xml2&#39;) 23.1 Provided services/data What data/service is provided by the API? To access Wikipedia, MediaWiki provides the MediaWiki Action API. The API can be used for multiple things, such as accessing wiki features, interacting with a wiki and obtaining meta-information about wikis and public users. Additionally, the web service can provide access data and post changes of Wikipedia-webpages. 23.2 Prerequisites What are the prerequisites to access the API (authentication)? No pre-registration is required to access the API. However, for certain actions, such as very large queries, a registration is required. Moreover, while there is no hard and fast limit on read requests, the system administrators heavily recommend limiting the request rate to secure the stability of the side. It is also best practice to set a descriptive User Agent header. 23.3 Simple API call What does a simple API call look like? As mentioned, the API can be used to communicate with Wikipedia for a variety of actions. As it is most likely for social scientist to extract information rather than post changes to a Wikipedia page, we focus here on obtaining from Wikipedia the information we need. We include a basic API call to obtain information about the Albert Einstein Wikipedia page https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;prop=info&amp;titles=Albert%20Einstein to be plugged into the search bar of a browser to obtain the basic information on the page. result: {&quot;batchcomplete&quot;:&quot;&quot;,&quot;query&quot;: {&quot;pages&quot;: {&quot;736&quot;: {&quot;pageid&quot;:736,&quot;ns&quot;:0,&quot;title&quot;:&quot;AlbertEinstein&quot;, &quot;contentmodel&quot;:&quot;wikitext&quot;,&quot;pagelanguage&quot;:&quot;en&quot;, &quot;pagelanguagehtmlcode&quot;:&quot;en&quot;,&quot;pagelanguagedir&quot;:&quot;ltr&quot;, &quot;touched&quot;:&quot;2022-02-06T12:46:49Z&quot;,&quot;lastrevid&quot;:1070093046,&quot;length&quot;:184850} } } } Notice that the first line is common for all calls of the API, while the second line relates to the specific action you are trying to perform. 23.4 API access in R How can we access the API from R (httr + other packages)? The most common tool is WikipediR, a wrapper around the Wikipedia API. It allows R to access information and “directions” for the relevant page or pages of Wikipedia and the content or metadata therein. Importantly, the wrapper only allows to gather information, which implies that the instrument needs to be accompanied by other packages such as rvest for scraping and XML or jsonlite for parsing. WikipediR allows us to get different information like general page info, backlinks, categories in page etc. In this example we are interested in the titles of the first ten backlinks of the Albert Einstein site. library(WikipediR) all_bls &lt;- page_backlinks(&quot;en&quot;,&quot;wikipedia&quot;, page = &quot;Albert Einstein&quot;, limit = 10) #using &quot;page_backlings&quot; function of the WikipediR package bls_title &lt;- data.frame() for(i in 1:10){ bls_title &lt;- rbind(bls_title,all_bls$query$backlinks[[i]]$title) } colnames(bls_title) &lt;- &quot;backlinks&quot; bls_title We can also scrape data of Wikipedia by simple using the “rvest” package to scrape all kind of informations like tables, which is done in the following example. library(rvest) library(xml2) url &lt;- &quot;https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population&quot; html &lt;- read_html(url) # reading the html code into memory #with the commands of the rvest package we are able to scrape the information we need, here html_table() tab &lt;- html_table(html, fill=TRUE) # shows all tables on the website with the number of the table in double brackets [[number]] #we are interested in table 1, the inequality index of the countries data &lt;- tab[[2]] data &lt;- data[-1,] # delete first row data[1:5,1:4] #for a better overview we are just loooking at the first 5 rows and the first 4 columns 23.5 Social science examples Are there social science research examples using the API? Some papers using Wikipedia-information rely on the API to access the data. These papers cover a wide range of social and economical sciences. Political science papers are, for example, concerned with political elections, more specifically election prediction (Margolin et al. 2016; Salem and Stephany 2021). Other papers use the data accessed through the API to analyze media coverage of the COVID-19 pandemic (Gozzi et al. 2020) or the interplay between online information and investment markets (ElBahrawy, Alessandretti, and Baronchelli 2019). References ElBahrawy, Abeer, Laura Alessandretti, and Andrea Baronchelli. 2019. “Wikipedia and Cryptocurrencies: Interplay Between Collective Attention and Market Performance.” Frontiers in Blockchain 2: 12. Gozzi, Nicolò, Michele Tizzani, Michele Starnini, Fabio Ciulla, Daniela Paolotti, André Panisson, and Nicola Perra. 2020. “Collective Response to Media Coverage of the COVID-19 Pandemic on Reddit and Wikipedia: Mixed-Methods Analysis.” Journal of Medical Internet Research 22 (10): e21597. Margolin, Drew B, Sasha Goodman, Brian Keegan, Yu-Ru Lin, and David Lazer. 2016. “Wiki-Worthy: Collective Judgment of Candidate Notability.” Information, Communication &amp; Society 19 (8): 1029–45. Salem, Hamza, and Fabian Stephany. 2021. “Wikipedia: A Challenger’s Best Friend? Utilizing Information-Seeking Behaviour Patterns to Predict US Congressional Elections.” Information, Communication &amp; Society, 1–27. "],["youtube-api.html", "Chapter 24 Youtube API 24.1 Provided services/data 24.2 Prerequisites 24.3 Simple API call 24.4 API access in R 24.5 Social science examples", " Chapter 24 Youtube API Melike Kaplan, Jana Klein, Dean Lajic You will need to install the following packages for this chapter (run the code): # install.packages(&#39;pacman&#39;) library(pacman) p_load(&#39;httr&#39;, &#39;jsonlite&#39;, &#39;here&#39;, &#39;dplyr&#39;, &#39;ggplot2&#39;, &#39;tuber&#39;, &#39;tidyverse&#39;, &#39;purrr&#39;) 24.1 Provided services/data What data/service is provided by the API? The API is provided by Google, Youtube’s parent company. There are different types of Youtube APIs that serve different purposes: YouTube Analytics API: retrieves your YouTube Analytics data. YouTube Data API v3: provides access to YouTube data, such as videos, playlists, and channels. YouTube oEmbed API: oEmbed is an elegant way to embed multimedia for a link. YouTube Reporting API: Schedules reporting jobs containing your YouTube Analytics data and downloads the resulting bulk data reports in the form of CSV files. The google developer site provides sample requests and a summary of the possible metrics that the API can give you data on. You can actually run your API requests there. All the possible calls you can make are provided on the page: Captions, ChannelBanners, Channels, ChannelSection, Comments, CommentThreads, i18nLanguages, i18nRegrions, Members, MembershipLevels, Playlistitems, Playlists, Search, Subscriptions, Thumbnails, VideoAbuseReportReasons, VideoCategories, and Videos. 24.2 Prerequisites What are the prerequisites to access the API (authentication)? An overview and guide is given on the Youtube Api website. First, you will need a Google account which you will use to log into the Google Cloud Platform. You will need to create a new project unless you already have one (here you can find more information). Then, you can search for the four Youtube APIs (YouTube Analytics API, YouTube Data API v3, YouTube oEmbed API, YouTube Reporting API) mentioned above and enable them (here you can find more information). Then, continue to the “APIs and Services” Page from the sidebar and click on “Credentials.” Click on “+ Create Credentials” at the top of the page. You have three options here: API Key, OAuth client ID or Service account. An API Key will identify your project with a simple key to check quota and access but if you wish to use the YouTube API for your app, you should create an OAuth client ID which will request user consent so that your app can access the user’s data. This is also necessary when you want to use the tuber package. A Service account enables server to server, app-level authentication using robot accounts. We will continue with the option of creating an API Key, and later we provide an example of using the OAuth Client ID with the tuber package. When you click on “API Key” in the “+Create Credentials” list, a screen will appear like below: Your key is created! It is important to restrict the key! 24.3 Simple API call What does a simple API call look like? You can access the API via your browser. The Youtube API website facilitate this where you have just to insert your parameters like part (what kind of information) or q (your search query). There is also an overview of that parameters on the Youtube Api website. The base URL is https://www.googleapis.com/youtube/v3/. With the following API call we tried to get the channel statistics from the MrBeast youtube channel. The channel statistics include information on the viewer count, subscriber count, whether there are hidden subscribers and on the video count. https://youtube.googleapis.com/youtube/v3/channels?part=statistics&amp;id=UCX6OQ3DkcsbYNE6H8uQQuVA&amp;key=[YourAPIKey] Here we are looking for channels, “part” is set on statistics and the id is the Youtube channel id. Inserted in the Browser we will get the following output. { &quot;kind&quot;: &quot;youtube#channelListResponse&quot;, &quot;etag&quot;: &quot;ma9LJ5xRKmEdW36U5jKCcqE5JcU&quot;, &quot;pageInfo&quot;: { &quot;totalResults&quot;: 1, &quot;resultsPerPage&quot;: 5 }, &quot;items&quot;: [ { &quot;kind&quot;: &quot;youtube#channel&quot;, &quot;etag&quot;: &quot;PAtSZnFS2dTafqi67SnYTrL6f2E&quot;, &quot;id&quot;: &quot;UCX6OQ3DkcsbYNE6H8uQQuVA&quot;, &quot;statistics&quot;: { &quot;viewCount&quot;: &quot;14370108796&quot;, &quot;subscriberCount&quot;: &quot;88000000&quot;, &quot;hiddenSubscriberCount&quot;: false, &quot;videoCount&quot;: &quot;718&quot; } } ] } 24.4 API access in R How can we access the API from R (httr + other packages)? To access the API from R we are using the same logic by building URL´s with our search request and using the GET() function of the httr package. In this example we will compare the channel statistics of the the two biggest youtuber´s, PewDiePie and MrBeast. library(httr) library(jsonlite) library(here) library(dplyr) library(ggplot2) #save your API key in the object key key &lt;- Sys.getenv(&quot;Youtube_token&quot;) #YouTube channels either have a channel id or a user id MrBeastChannelID&lt;-&quot;UCX6OQ3DkcsbYNE6H8uQQuVA&quot; #channel id PewDiePieUserID &lt;- &quot;PewDiePie&quot; #user id #save the base URL in the object base base&lt;- &quot;https://www.googleapis.com/youtube/v3/&quot; #get channel info with channel id for MrBeast channel api_params &lt;- paste(paste0(&quot;key=&quot;, key), paste0(&quot;id=&quot;, MrBeastChannelID), &quot;part=statistics&quot;, sep = &quot;&amp;&quot;) api_call &lt;- paste0(base, &quot;channels&quot;, &quot;?&quot;, api_params) api_result &lt;- GET(api_call) json_result &lt;- httr::content(api_result, &quot;text&quot;, encoding=&quot;UTF-8&quot;) #format json into dataframe channel.json &lt;- fromJSON(json_result, flatten = T) channel.MB &lt;- as.data.frame(channel.json) #example with a username for PewDiePie channel. For part you can also insert more at once to get additional information. api_params2 &lt;- paste(paste0(&quot;key=&quot;, key), paste0(&quot;forUsername=&quot;, PewDiePieUserID), &quot;part=snippet,contentDetails,statistics&quot;, sep = &quot;&amp;&quot;) api_call2 &lt;- paste0(base, &quot;channels&quot;, &quot;?&quot;, api_params2) api_result2 &lt;- GET(api_call2) json_result2 &lt;- httr::content(api_result2, &quot;text&quot;, encoding=&quot;UTF-8&quot;) #format json into dataframe channel.json2 &lt;- fromJSON(json_result2, flatten = T) channel.PDP &lt;- as.data.frame(channel.json2) #Now we want to compare the two channels by some indicators like views etc. For this we are subsetting the data and create new indicators like average views per video MBchannel &lt;- channel.MB %&gt;% mutate(Channel = &quot;MrBeast&quot;,Views= as.numeric(items.statistics.viewCount), Videos = as.numeric(items.statistics.videoCount),averViewsVideo = Views/Videos, Subscriber = as.numeric(items.statistics.subscriberCount)) %&gt;% select(Channel,Views,Videos, averViewsVideo, Subscriber) PDPchannel &lt;- channel.PDP %&gt;% mutate(Channel = &quot;PewDiePie&quot;,Views= as.numeric(items.statistics.viewCount), Videos = as.numeric(items.statistics.videoCount),averViewsVideo = Views/Videos, Subscriber = as.numeric(items.statistics.subscriberCount)) %&gt;% select(Channel,Views,Videos, averViewsVideo, Subscriber) YTchannels &lt;- rbind(MBchannel, PDPchannel) YTchannels ## Channel Views Videos averViewsVideo Subscriber ## 1 MrBeast 14672112047 719 20406275 8.98e+07 ## 2 PewDiePie 28104820954 4454 6310018 1.11e+08 On CRAN we found the “tuber” package. The package enables you to get the comments posted on YouTube videos, number of likes of a video, search for videos with specific content and much more. You can also scrape captions from a few videos. To be able to use the tuber package, not an API key but the authentication with OAuth is necessary. OAuth (Open Authorization) uses authorization tokens to prove an identity between consumers and service providers. You can get your client ID and secret on the Google Cloud Platform under Credentials. Setting up the “Consent Screen” First we had to configure the so-called OAuth consent screen, where we put “external” and then had to put an app name. For scopes we did not specify anything and just clicked save &amp; continued. To be able to use the API you have to set your own google mail address that you use for the Google cloud. Get OAuth credentials After setting up the consent screen you can go back and click “create credentials” and add a “OAuth client ID”. In older guides it is recommended to choose the category “others” which no longer exists. Here you have to choose “desktop application”. As a result you get an OAuth client id and secret. You can download this information stored in a JSON file. With the Yt_oauth() function you can then authenticate yourself. This will forward us to logging into our google account. Allow the access to your google account. (like with google bigquery). This page provides some example API calls you can make with the tuber package. library(tuber) # youtube API library(tidyverse) # all tidyverse packages library(purrr) # package for iterating/extracting data #save client id and secret in an object client_id&lt;-&quot;put client ID&quot; client_secret&lt;-&quot;put client secret&quot; # use the youtube oauth yt_oauth(app_id = client_id, app_secret = client_secret, token = &#39;&#39;) #Downloading playlist data #first get playlist ID go_psych_playlist_id &lt;- stringr::str_split(string = &quot;https://www.youtube.com/playlist?list=PLD4cyJhQaFwWZ838zZhWVr3RG_nETlICM&quot;, pattern = &quot;=&quot;, n = 2, simplify = TRUE)[ , 2] go_psych_playlist_id #use the tuber function get_playlist_items to collect the videos into a data frame go_psych &lt;- get_playlist_items(filter = c(playlist_id = &quot;PLD4cyJhQaFwWZ838zZhWVr3RG_nETlICM&quot;), part = &quot;contentDetails&quot;) ##now we have the video ids of all videos in that playlist #extracting video id´s go_psych_ids &lt;- (go_psych$contentDetails.videoId) #In the next step we will use the map_df command of the purrr package which needs a function as a parameter #creating a function using the get_stats function of the tuber package get_all_stats &lt;- function(id) { get_stats(video_id = id) } # Now we can store the stats information in a data frame with the map_df command which needs an object .x and a function .f go_psych_AllStats &lt;- map_df(.x = go_psych_ids, .f = get_all_stats) #Overview of the statistics of all videos in the playlist go_psych_AllStats ## # A tibble: 18 × 5 ## id viewCount likeCount favoriteCount commentCount ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0SRmccgFIs8 4071451 147983 0 17428 ## 2 _1GCjggflEU 1261825 57615 0 6112 ## 3 aLOJg2VC76E 395087 15395 0 1395 ## 4 6A2ml3CyLuU 293344 10085 0 1040 ## 5 VxVOuCg-NG0 1042694 54188 0 14416 ## 6 -ISz5tsB7Es 331800 14701 0 1263 ## 7 6GcEhUYoW9E 1577397 68269 0 9820 ## 8 bdNUL8yarR0 309420 15057 0 1408 ## 9 E_nPrZU6PKc 188061 9214 0 461 ## 10 E_nPrZU6PKc 188061 9214 0 461 ## 11 E_nPrZU6PKc 188061 9214 0 461 ## 12 wVTvcxEWClg 1839839 75695 0 25321 ## 13 nHsS3Ta2MQo 394862 21102 0 1670 ## 14 VkHErnASuvA 468368 33674 0 6049 ## 15 FQ0IoPN8Yrs 1051800 63552 0 3695 ## 16 NssIITHZ28Y 1271567 66974 0 9089 ## 17 WWjpw51YWsE 106447 7038 0 643 ## 18 Ti3cCGOfEEM 1333025 65439 0 5248 Package information: * CRAN - Package tuber * here you can find all the functions that the tuber package provides 24.5 Social science examples Are there social science research examples using the API? In the study “Identifying Toxicity Within YouTube Video Comment” (Obadimu (2019)), the researchers utilized the YouTube Data API to collect the comments from eight YouTube channels that were either pro- or anti NATO. To the comments, five types of toxicity scores were assigned to analyze hateful comments. With word clouds the researchers were able to quantify the count of words from comments. The final dataset contained 1,424 pro-NATO videos with 8,276 comments, and 3,461 anti-NATO videos with 46,464 comments. The aim of the study “YouTube channels, uploads and views: A statistical analysis of the past 10 years” (Baertl (2018)) was to give an overview on how YouTube developed over the past 10 years in terms of consumption and production of videos. The study utilizes a random sample of channel and video data to answer the question. The data is retrieved with the YouTube API (did not specify which one) combined with a tool that generated random string searches to find a near-random sample of channels created between 01.01.2016 and 31.12.2016. Results are that channels, views and video uploads differ according to video genre. Furthermore, the analysis revealed that the majority of views are obtained by only a few channels. On average, older channels have a larger amount of viewers. In the study “From ranking algorithms to ‘ranking cultures’: Investigating the modulation of visibility in YouTube search results” (Rieder (2018)), YouTube is conceptualized as an influential source of information that uses a socio-algorithmic process in order to place search recommendations in a hierarchy. This process of ranking is considered to be a construction of relevance and knowledge in a very large pool of information. Therefore, the search function serves as a curator of recommended content. The information that is being transmitted in this content can also impose certain perspectives on users which is why how the algorithm works is especially important when it comes to controversial issues. In order to better understand how the algorithms that determine search rankings on YouTube work, the authors use a scraping approach and the YouTube API v3 to study the ranking of certain sociocultural issues over time. Examples of the keywords that they use are ‘gamergate,’ ‘trump,’ ‘refugees’ and ‘syria.’ They find three general types of morphologies of rank change. References Baertl, M. 2018. “YouTube Channels, Uploads and Views: A Statistical Analysis of the Past 10 Years.” Convergence. Obadimu, Mead, A. 2019. “Identifying Toxicity Within Youtube Video Comment.” In International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation. Rieder, Matamoros-Fernández, B. 2018. “From Ranking Algorithms to ‘Ranking Cultures’: Investigating the Modulation of Visibility in YouTube Search Results.” Convergence. "],["references-1.html", "Chapter 25 References", " Chapter 25 References Apostolou, Menelaos. 2019. “Why Men Stay Single? Evidence from Reddit.” Evolutionary Psychological Science 5 (1): 87–97. https://doi.org/10.1007/s40806-018-0163-7. Aslam, Salman. 2021. “• Instagram by the Numbers (2021): Stats, Demographics &amp; Fun Facts.” https://www.omnicoreagency.com/instagram-statistics/. Baertl, M. 2018. “YouTube Channels, Uploads and Views: A Statistical Analysis of the Past 10 Years.” Convergence. Barrie, Christopher, and Justin Ho. 2021. “academictwitteR: An r Package to Access the Twitter Academic Research Product Track V2 API Endpoint.” Journal of Open Source Software 6 (62): 3272. https://doi.org/10.21105/joss.03272. Bedini, Farazi, I. 2014. “Open Government Data: Fostering Innovation. EJournal of EDemocracy and Open Government.” EJournal of EDemocracy and Open Government. Bene, Márton, and Simon Kruschinski. 2021. “Political Advertising on Facebook.” In Campaigning on Facebook in the 2019 European Parliament Election: Informing, Interacting with, and Mobilising Voters, edited by Jörg Haßler, Melanie Magin, Uta Russmann, and Vicente Fenoll, 283–99. Political Campaigning and Communication. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73851-8_18. Benkler, Yochai, Hal Roberts, Robert Faris, Alicia Solow-Niederman, and Bruce Etling. 2015. “Social Mobilization and the Networked Public Sphere: Mapping the SOPA-PIPA Debate.” Political Communication 32 (4): 594–624. https://doi.org/10.1080/10584609.2014.986349. Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774. Berriche, Manon, and Sacha Altay. 2020. “Internet Users Engage More with Phatic Posts Than with Health Misinformation on Facebook.” Palgrave Communications 6 (1): 1–9. Blevins, Cameron, and Lincoln Mullen. 2015. “Jane, John... Leslie? A Historical Method for Algorithmic Gender Prediction.” DHQ: Digital Humanities Quarterly 9 (3). Brodeur, Clark, A. 2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google Trends.” Journal of Public Economics. Broniatowski, David A, Amelia M Jamison, Neil F Johnson, Nicolás Velasquez, Rhys Leahy, Nicholas Johnson Restrepo, Mark Dredze, and Sandra C Quinn. 2020. “Facebook Pages, the ‘Disneyland’ Measles Outbreak, and Promotion of Vaccine Refusal as a Civil Right, 2009-2019.” Am. J. Public Health 110 (S3): S312–18. Brown, Rebecca C, Eileen Bendig, Tin Fischer, A David Goldwich, Harald Baumeister, and Paul L Plener. 2019. “Can Acute Suicidality Be Predicted by Instagram Data? Results from Qualitative and Quantitative Language Analyses.” PLoS One 14 (9): e0220623. Brügger, Niels, Ditte Laursen, and Janne Nielsen. 2017. “Exploring the Domain Names of the Danish Web.” In The Web as History: Using Web Archives to Understand the Past and the Present, edited by Niels Brügger and Ralph Schroeder. UCL Press. https://doi.org/10.2307/j.ctt1mtz55k. Bruns, Axel. 2019. “After the APIcalypse: Social Media Platforms and Their Fight Against Critical Scholarly Research.” Information, Communication &amp; Society 22 (11): 1544–66. https://doi.org/10.1080/1369118X.2019.1637447. Bruns, Axel, Stephen Harrington, and Edward Hurcombe. 2020. “‘Corona? 5G? Or Both?’: The Dynamics of COVID-19/5G Conspiracy Theories on Facebook.” Media International Australia 177 (1): 12–29. Burgess, Jean, and Axel Bruns. 2015. “Easy Data, Hard Data: The Politics and Pragmatics of Twitter Research After the Computational Turn.” Compromised Data: From Social Media to Big Data. Bloomsbury Academic New York, NY. Calvo, Dafne, Lorena Cano-Orón, and Tomás Baviera. 2021. “Global Spaces for Local Politics: An Exploratory Analysis of Facebook Ads in Spanish Election Campaigns.” Social Sciences 10 (7): 271. https://doi.org/10.3390/socsci10070271. Cano-Orón, Lorena, Dafne Calvo, Guillermo López García, and Tomás Baviera. 2021. “Disinformation in Facebook Ads in the 2019 Spanish General Election Campaigns.” Media and Communication 9 (1): 217–28. https://doi.org/10.17645/mac.v9i1.3335. Capozzi, Arthur, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti, André Panisson, and Daniela Paolotti. 2021. “Clandestino or Rifugiato? Anti-immigration Facebook Ad Targeting in Italy.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 1–15. Yokohama Japan: ACM. https://doi.org/10.1145/3411764.3445082. Chatziasimidis, Fragkiskos, and Ioannis Stamelos. 2015. “Data Collection and Analysis of GitHub Repositories and Users.” In 2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA), 1–6. https://doi.org/10.1109/IISA.2015.7388026. Chipidza, Wallace, Christopher Krewson, Nicole Gatto, Elmira Akbaripourdibazar, and Tendai Gwanzura. 2022. “Ideological Variation in Preferred Content and Source Credibility on Reddit During the COVID-19 Pandemic.” Big Data &amp; Society 9 (1): 20539517221076486. https://doi.org/10.1177/20539517221076486. Chrisinger, Benjamin W., Eliza W. Kinsey, Ellie Pavlick, and Chris Callison-Burch. 2021. “SNAP Judgments into the Digital Age: Reporting on Food Stamps Varies Significantly with Time, Publication Type, and Political Leaning.” Commun. Methods Meas., November, 1–18. Clemm von Hohenberg, Bernhard. 2022. “Truth and Bias, Left and Right.” Corrêa, Corrêa, A. S. 2015. “A Collaborative-Oriented Middleware for Structuring Information to Open Government Data.” Proceedings of the 16th Annual International Conference on Digital Government Research. Cosentino, Valerio, Javier Luis Cánovas Izquierdo, and Jordi Cabot. 2016. “Findings from GitHub: Methods, Datasets and Limitations.” In 2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR), 137–41. Djourelova, Milena, Ruben Durante, Elliot Motte, and Eleonora Patacchini. 2024. “Experience, Narratives, and Climate Change Beliefs.” Working Paper. Dobbrick, Timo, Julia Jakob, Chung-Hong Chan, and Hartmut Wessler. 2021. “Enhancing Theory-Informed Dictionary Approaches with ‘Glass-Box’ Machine Learning: The Case of Integrative Complexity in Social Media Comments.” Commun. Methods Meas., November, 1–18. Edelson, Laura, Tobias Lauinger, and Damon McCoy. 2020. “A Security Analysis of the Facebook Ad Library.” In 2020 IEEE Symposium on Security and Privacy (SP), 661–78. San Francisco, CA, USA: IEEE. https://doi.org/10.1109/SP40000.2020.00084. Edmondson, Mark. 2017. “Google Cloud Speech API.” https://mran.microsoft.com/snapshot/2017-09-27/web/packages/googleLanguageR/vignettes/speech.html. ElBahrawy, Abeer, Laura Alessandretti, and Andrea Baronchelli. 2019. “Wikipedia and Cryptocurrencies: Interplay Between Collective Attention and Market Performance.” Frontiers in Blockchain 2: 12. Evsyukova, Yulia, Felix Rusche, and Wladislaw Mill. 2023. “LinkedOut? A Field Experiment on Discrimination in Job Network Formation.” CRC TR 224 Discussion Paper Series No. 482. https://www.crctr224.de/research/discussion-papers/archive/dp482. Facebook for Developers. 2021. “Instagram Basic Display API.” https://developers.facebook.com/docs/instagram-basic-display-api/. “FAQs.” 2022. https://www.openstreetmap.de/faq.html#was_ist_osm. Ferwerda, Bruce, Markus Schedl, and Marko Tkalcic. 2015. “Predicting Personality Traits with Instagram Pictures.” In Proceedings of the 3rd Workshop on Emotions and Personality in Personalized Systems 2015, 7–10. EMPIRE ’15. New York, NY, USA: Association for Computing Machinery. Fowler, Erika Franklin, Michael M. Franz, Gregory J. Martin, Zachary Peskowitz, and Travis N. Ridout. 2021. “Political Advertising Online and Offline.” American Political Science Review 115 (1): 130–49. https://doi.org/10.1017/S0003055420000696. Gaudette, Tiana, Ryan Scrivens, Garth Davies, and Richard Frank. 2021. “Upvoting Extremism: Collective Identity Formation and the Extreme Right on Reddit.” New Media &amp; Society 23 (12): 3491–3508. https://doi.org/10.1177/1461444820958123. Gavras, Konstantin. 2022. “The Conditions and Nature of Europeanized Public Discourse ? A Multi-Lingual QTA Analysis of Public Discourses in Europe Using the Internet Archive, 2016-2019.” PhD thesis, Mannheim. https://madoc.bib.uni-mannheim.de/62270/. Gavras, Konstantin, Jan Karem Höhne, Annelies G Blom, and Harald Schoen. 2022. “Innovating the Collection of Open-Ended Answers: The Linguistic and Content Characteristics of Written and Oral Answers to Political Attitude Questions.” J. R. Stat. Soc. Ser. A Stat. Soc. tba (tba): 1–19. Gavras, Konstantin, and Lukas Isermann. 2022. archiveRetriever: Retrieve Archived Web Pages from the ’Internet Archive’. https://CRAN.R-project.org/package=archiveRetriever. Gozzi, Nicolò, Michele Tizzani, Michele Starnini, Fabio Ciulla, Daniela Paolotti, André Panisson, and Nicola Perra. 2020. “Collective Response to Media Coverage of the COVID-19 Pandemic on Reddit and Wikipedia: Mixed-Methods Analysis.” Journal of Medical Internet Research 22 (10): e21597. Grimmer, Justin, and Brandon M Stewart. 2013. “Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.” Polit. Anal. 21 (3): 267–97. Haim, Mario, Andreas Graefe, and Hans-Bernd Brosius. 2018. “Burst of the Filter Bubble?” Digital Journalism 6 (3): 330–43. https://doi.org/10.1080/21670811.2017.1338145. Hale, Scott A., Grant Blank, and Victoria D. Alexander. 2017. “Live Versus Archive: Comparing a Web Archive to a Population of Web Pages.” In The Web as History: Using Web Archives to Understand the Past and the Present, edited by Niels Brügger and Ralph Schroeder. UCL Press. https://doi.org/10.2307/j.ctt1mtz55k. Hale, Scott A., Taha Yasseri, Josh Cowls, Eric T. Meyer, Ralph Schroeder, and Helen Margetts. 2014. “Mapping the UK Webspace: Fifteen Years of British Universities on the Web.” In Proceedings of the 2014 ACM Conference on Web Science - WebSci ’14, 62–70. Bloomington, Indiana, USA: ACM Press. https://doi.org/10.1145/2615569.2615691. Haßler, Jörg, Anna-Katharina Wurst, Marc Jungblut, and Katharina Schlosser. 2021. “Influence of the Pandemic Lockdown on Fridays for Future’s Hashtag Activism.” New Media &amp; Society, 14614448211026575. https://doi.org/https://doi.org/10.1177/14614448211026575. Hintze, P, and T. Lakes. 2009. “Geographically Referenced Data for Social Science.” Working {Paper} No. 125. German Council for Social; Economic Data (RatSWD). http://ssrn.com/abstract=1498449. Hipp, Lena, and Markus Konrad. 2021. “Has Covid-19 Increased Gender Inequalities in Professional Advancement? Cross-Country Evidence on Productivity Differences Between Male and Female Software Developers.” Journal of Family Research, September. https://doi.org/10.20377/jfr-697. Holman, Luke, Devi Stuart-Fox, and Cindy E. Hauser. 2018. “The Gender Gap in Science: How Long Until Women Are Equally Represented?” PLOS Biology 16 (4): e2004956. https://doi.org/10.1371/journal.pbio.2004956. Hosseinmardi, Homa, Sabrina Arredondo Mattson, Rahat Ibn Rafiq, Richard Han, Qin Lv, and Shivakant Mishr. 2015. “Prediction of Cyberbullying Incidents on the Instagram Social Network,” August. https://arxiv.org/abs/1508.06257. Hu, Yuheng, L Manikonda, and S Kambhampati. 2014. “What We Instagram: A First Analysis of Instagram Photo Content and User Types.” ICWSM. Huckins, Jeremy F, Alex W daSilva, Weichen Wang, Elin Hedlund, Courtney Rogers, Subigya K Nepal, Jialing Wu, et al. 2020. “Mental Health and Behavior of College Students During the Early Phases of the COVID-19 Pandemic: Longitudinal Smartphone and Ecological Momentary Assessment Study.” Journal of Medical Internet Research 22 (6): e20185. https://doi.org/10.2196/20185. Instagram. 2021. “Instagram Developer Documentation.” https://www.instagram.com/developer/. Internet Archive. 2016. Wayback CDX Server API - BETA. https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server#readme. ———. 2022. “About the Internet Archive.” 2022. https://archive.org/about/. Kearney, Michael. 2019. “Rtweet: Collecting and Analyzing Twitter Data.” Journal of Open Source Software 4 (42): 1829. https://doi.org/10.21105/joss.01829. Konrad, Markus. 2020. “Using Google Places Data to Analyze Changes in Mobility During the COVID-19 Pandemic.” https://datascience.blog.wzb.eu/2020/05/11/using-google-places-data-to-analyze-changes-in-mobility-during-the-covid-19-pandemic/. Lacognata, Anthony, and Jennifer Poole. 2021. “The Melodies of Politics: Assessing a Correlation Between Music Taste and Political Views with Spotify.” Journal of Student Research 10 (November). https://doi.org/10.47611/jsrhs.v10i3.1898. Larsson, Anders Olof. 2020. “Right-Wingers on the Rise Online: Insights from the 2018 Swedish Elections.” New Media &amp; Society 22 (12): 2108–27. Lima, Antonio, Luca Rossi, and Mirco Musolesi. 2014. “Coding Together at Scale: GitHub as a Collaborative Social Network.” In Eighth International AAAI Conference on Weblogs and Social Media. Littman, Justin, Daniel Chudnov, Daniel Kerchner, Christie Peterson, Yecheng Tan, Rachel Trent, Rajat Vij, and Laura Wrubel. 2018. “API-based Social Media Collecting as a Form of Web Archiving.” International Journal on Digital Libraries 19 (1): 21–38. https://doi.org/10.1007/s00799-016-0201-7. Liu, Liu, C. 2018. “Open Government Data: The German Government Is Moving.” Asia-Pacific Social Science and Modern Education Conference (SSME 2018). MacTaggart, Andrew. 2018. “Pop Music and Politics: Tracking Political Trends Through the Hot 100 Chart, 1959-2016 - ProQuest.” Harvard University. https://www.proquest.com/docview/2487180443?fromopenview=true&amp;pq-origsite=gscholar&amp;parentSessionId=PiWLVy66xd%2FXPi84svQkAEg0ze49z52W1qhA3%2BBQlVs%3D. Margolin, Drew B, Sasha Goodman, Brian Keegan, Yu-Ru Lin, and David Lazer. 2016. “Wiki-Worthy: Collective Judgment of Candidate Notability.” Information, Communication &amp; Society 19 (8): 1029–45. Marienfeld, Schieferdecker, F. 2013. “Metadata Aggregation at GovData.de: An Experience Report.” Proceedings of the 9th International Symposium on Open Collaboration. Matamoros-Fernández, Ariadna, and Johan Farkas. 2021. “Racism, Hate Speech, and Social Media: A Systematic Review and Critique.” Television &amp; New Media 22 (2): 205–24. https://doi.org/http://dx.doi.org/10.1177/1527476420982230. Mavragani, Tsagarakis, A. 2019. “Predicting Referendum Results in the Big Data Era.” J Big Data. Milligan, Ian, Nick Ruest, and Jimmy Lin. 2016. “Content Selection and Curation for Web Archiving: The Gatekeepers Vs. The Masses.” In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, 107–10. Newark New Jersey USA: ACM. https://doi.org/http://dx.doi.org/10.1145/2910896.2910913. Newman, Nic, Richard Fletcher, Anne Schulz, Simge Andi, Craig T Robertson, and Rasmus Kleis Nielsen. 2021. “Reuters Institute digital news report 2021.” Obadimu, Mead, A. 2019. “Identifying Toxicity Within Youtube Video Comment.” In International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation. Ostermann, K., Eppelsheimer, J., Gläser, N., Haller, P., and Oertel, M. 2022. “Geodata in Labor Market Research: Trends, Potentials, and Perspectives” 1 (115). “Overpass API.” 2022. https://wiki.openstreetmap.org/wiki/Overpass_API. “Overpass API/Applications.” 2022. https://wiki.openstreetmap.org/wiki/Overpass_API/Applications. Prado-Román C, Orden-Cruz C., Gómez-Martínez R. 2021. “Google Trends as a Predictor of Presidential Elections: The United States Versus Canada.” American Behavioral Scientist. Prates, Marcelo O R, Pedro H C Avelar, and Luis Lamb. 2018. “Assessing Gender Bias in Machine Translation – a Case Study with Google Translate,” September. https://arxiv.org/abs/1809.02208. Proferes, Nicholas, Naiyan Jones, Sarah Gilbert, Casey Fiesler, and Michael Zimmer. 2021. “Studying Reddit: A Systematic Overview of Disciplines, Approaches, Methods, and Ethics.” Social Media + Society 7 (2): 205630512110190. https://doi.org/10.1177/20563051211019004. Proksch, Sven-Oliver, Christopher Wratil, and Jens Wäckerle. 2019. “Testing the Validity of Automatic Speech Recognition for Political Text Analysis.” Polit. Anal. 27 (3): 339–59. Puschmann, Cornelius, and Mario Haim. 2021. “useNews.” https://doi.org/10.17605/OSF.IO/UZCA3. Quarati, Alfonso, and Monica De Martino. 2019. “Open Government Data Usage: A Brief Overview.” In IDEAS 2019, 23rd International Database Engineering &amp; Applications Symposium. unknown. Rauchfleisch, Adrian, Dario Siegen, and Daniel Vogler. 2021. “How COVID-19 Displaced Climate Change: Mediated Climate Change Activism and Issue Attention in the Swiss Media and Online Sphere.” Environmental Communication, November, 1–9. https://doi.org/10.1080/17524032.2021.1990978. Revilla, Melanie, and Mick P Couper. 2021. “Improving the Use of Voice Recording in a Smartphone Survey.” Soc. Sci. Comput. Rev. 39 (6): 1159–78. Rieder, Matamoros-Fernández, B. 2018. “From Ranking Algorithms to ‘Ranking Cultures’: Investigating the Modulation of Visibility in YouTube Search Results.” Convergence. Rivera, Ivan. 2022. “RedditExtractoR: Reddit Data Extraction Toolkit.” https://CRAN.R-project.org/package=RedditExtractoR. Roberts, Hal, Rahul Bhargava, Linas Valiukas, Dennis Jen, Momin M Malik, Cindy Bishop, Emily Ndulue, et al. 2021. “Media Cloud: Massive Open Source Collection of Global News on the Open Web.” arXiv Preprint arXiv:2104.03702. Royé, Dominic. 2018. “Accessing OpenStreetMap Data with R.” https://www.r-bloggers.com/2018/11/accessing-openstreetmap-data-with-r/. Rudnytskyi, Iegor. 2023. Openai: R Wrapper for OpenAI API. https://github.com/irudnyts/openai. Salem, Hamza, and Fabian Stephany. 2021. “Wikipedia: A Challenger’s Best Friend? Utilizing Information-Seeking Behaviour Patterns to Predict US Congressional Elections.” Information, Communication &amp; Society, 1–27. Schmøkel, Rasmus, and Michael Bossetta. 2021. “FBAdLibrarian and Pykognition: Open Science Tools for the Collection and Emotion Detection of Images in Facebook Political Ads with Computer Vision.” Journal of Information Technology &amp; Politics 0 (0): 1–11. https://doi.org/10.1080/19331681.2021.1928579. Schober, Michael F, Frederick G Conrad, Christopher Antoun, Patrick Ehlen, Stefanie Fail, Andrew L Hupp, Michael Johnston, Lucas Vickers, H Yanna Yan, and Chan Zhang. 2015. “Precision and Disclosure in Text and Voice Interviews on Smartphones.” PLoS One 10 (6): e0128337. Shen, Yiqin Alicia, Jason M. Webster, Yuichi Shoda, and Ione Fine. 2018. “Persistent Underrepresentation of Women’s Science in High Profile Journals.” https://doi.org/10.1101/275362. Silge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037. Song, Hemmi, Jon Chun, and Katherine Elkins. 2021. “Tracking the Emotion of Music Across the Covid-19 Pandemic.” In. https://doi.org/10.3109/10673229.2011.549769. Steinberg, S. J., and S. L. Steinberg. 2006. GIS: Geographic Information Systems for the Social Sciences: Investigating Space and Place. Thousand Oaks, London, New Delhi: SAGE Publications. Sun, Seungjong, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, and Jang Hyun Kim. 2024. “Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information.” https://arxiv.org/abs/2402.18144. Tankovska, H. n.d. “Topic: Instagram.” https://www.statista.com/topics/1882/instagram/. Wais, Kamil. 2016. “Gender Prediction Methods Based on First Names with genderizeR.” The R Journal 8 (1): 17. https://doi.org/10.32614/RJ-2016-002. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
